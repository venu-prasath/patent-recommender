{
    "patent_id": "US-2022067540-A1",
    "title": "Dynamic rule generator ",
    "assignee": "Elemental OpCo, LLC",
    "publication_date": "2022-03-03",
    "patent_link": "https://patents.google.com/patent/US20220067540A1/en",
    "inventors": [
        "David Ferrucci",
        "Aditya Kalyanpur",
        "Jennifer Chu-Carroll",
        "Thomas Breloff",
        "Or Biran",
        "David Buchanan"
    ],
    "classifications": [
        "G06N5/022",
        "G06N5/025",
        "G06N20/10",
        "G06N20/20",
        "G06N5/04",
        "G06N7/00"
    ],
    "abstract": "A dynamic reasoning system may include a symbolic reasoning engine that iteratively calls a dynamic rule generator to answer an input query. The symbolic reasoning engine may determine a primary goal and/or secondary goals to generate proofs for the answer. The symbolic reasoning engine may call a rules component to provide rules to prove a current input goal. The rules component may use a static rule knowledge base and/or the dynamic rule generator to retrieve and rank rules relevant to the current input goal. The dynamic rule generator may generate new rules that lead to the current input goal. The dynamic rule generator may include a statistical model that generates unstructured or structured probabilistic rules based on context related to the input query. The symbolic reasoning engine may return a list of rules with confidence for explaining the answer to the input goal.",
    "claims": "\n1. A computer-implemented method comprising:\nreceiving a reading content and an input question about the reading content;\ndetermining structured representations for the input question and a context, wherein the context includes facts from the reading content;\ndetermining, based at least in part on the input question, a goal to prove;\ndetermining, using a rule generating model, a list of rules that proves the goal, wherein the rule generating model generates one or more rules that leads to the input question;\ngenerating, based at least in part on the list of rules, a proof graph;\ndetermining, based at least in part on the proof graph, one or more proofs for an answer for the input question; and\npresenting, via a user interface on user device, the answer for user feedback.\n2. The computer-implemented method as recited in claim 1, wherein individual rules of the one or more rules are associated with confidence scores and further comprising:\ndetermining, based at least in part on the confidence scores, a confidence for the list of rules.\n3. The computer-implemented method as recited in claim 1, wherein generating the proof graph includes determining whether rule conditions match the facts in the context or match inference in a partial proof graph.\n4. The computer-implemented method as recited in claim 1, further comprising:\npresenting, via a user interface on user device, the one or more proofs for user feedback;\nreceiving a positive feedback for the one or more proofs;\ngenerating one or more inferred rules by replacing constants with variables in the one or more proofs; and\nstoring the one or more inferred rules with the positive feedback.\n5. A computer-implemented method comprising:\nreceiving an input question with a context about a text;\ndetermining, using the context as input for a rule generating model, a list of rules that proves the input question, wherein the rule generating model generates one or more rules that leads to the input question;\ndetermining, based at least in part on the list of rules, a proof graph;\ndetermining, based at least in part on the proof graph, a complete proof is found; and\nreturning the complete proof as an answer to the input question.\n6. The computer-implemented method as recited in claim 5, wherein individual rules of the one or more rules are associated with confidence scores and the list of rules is determined based at least in part on the confidence scores, and further comprising:\ndetermining, based at least in part on a sum of individual confidence scores associated with the list of rules, a confidence score for the answer, wherein determining the complete proof is found is based at least in part on the confidence score meeting a threshold.\n7. The computer-implemented method as recited in claim 5, further comprising:\npresenting, via a user interface on a user device, one or more inferred rules from the complete proof for user feedback.\n8. The computer-implemented method as recited in claim 7, further comprising:\nreceiving a positive feedback for the one or more inferred rules; and\nstoring the one or more inferred rules with the positive feedback.\n9. The computer-implemented method as recited in claim 8, further comprising:\nreceiving a second input question about the text; and\ndetermining, using the one or more inferred rules with the positive feedback as input for the rule generating model, a second list of rules that proves the second input question.\n10. The computer-implemented method as recited in claim 7, further comprising:\nreceiving a negative feedback for the one or more inferred rules; and\nstoring the one or more inferred rules with the negative feedback.\n11. The computer-implemented method as recited in claim 10, further comprising:\nreceiving a second input question about the text; and\ndetermining, using the one or more inferred rules with the negative feedback as input for the rule generating model, a second list of rules that proves the second input question.\n12. The computer-implemented method as recited in claim 8, further comprising:\nreceiving a second input question with a second context about a second text; and\ndetermining, using the second context and the one or more inferred rules with the positive feedback as input for the rule generating model, a second list of rules that proves the second input question.\n13. The computer-implemented method as recited in claim 12, further comprising:\ngenerating a second proof graph for the second text that includes the second list of rules with the second context leading to the second input question.\n14. A system comprising:\none or more processors; and\none or more computer-readable media storing instructions that, when executed by the one or more processors, cause the one or more processors to perform operations comprising:\nreceiving an input query with a context, wherein the context includes a list of facts;\ndetermining, using the context as input for a rule generating model, a list of rules that proves the input query, wherein the rule generating model generates one or more rules that leads to the input query;\ndetermining, based at least in part on confidence scores associated with the list of rules, a ranking for the list of rules;\ndetermining, based at least in part on the ranking for the list of rules, a ranked list of explanations; and\nreturning the ranked list of explanations for the input query.\n15. The system as recited in claim 14, wherein determining the ranked list of explanations includes generating a proof graph.\n16. The system as recited in claim 15, wherein generating the proof graph includes determining whether rule conditions match the list of facts in the context.\n17. The system as recited in claim 16, the operations further comprising:\ndetermining the proof graph is incomplete based at least in part on the rule conditions failing to match the list of facts;\ndetermining a hint based at least in part on a rule condition that failed to match the list of facts; and\npresenting, via a user interface on a user device, the hint for user feedback.\n18. The system as recited in claim 14, the operations further comprising:\npresenting, via a user interface on a user device, the ranked list of explanations for user feedback.\n19. The system as recited in claim 18, the operations further comprising:\nreceiving the user feedback for the ranked list of explanations.\n20. The system as recited in claim 19, the operations further comprising:\nreplacing constants from the ranked list of explanations with variables to generate one or more inferred rules; and\nstoring the one or more inferred rules with the user feedback."
}