{
    "patent_link": "https://patents.google.com/patent/US20090010507A1/en",
    "patent_id": "US20090010507A1",
    "title": "System and method for generating a 3d model of anatomical structure using a plurality of 2d images",
    "abstract": "A system and method are provided for generating a three dimensional (3D) model of an anatomical structure of a patient using a plurality of two dimensional (2D) images acquired using a camera. The method includes the operation of searching the plurality of 2D images to detect correspondence points of image features across at least two images. Camera motion parameters can be determined using the correspondence points for a sequence of at least two images taken at different locations by the camera moving within the internal anatomical structure. A further operation is computing dense stereo maps for 2D image pairs that are temporally adjacent. A consistent 3D model can be formed by fusing together multiple 2D images which are applied to a plurality of integrated 3D model segments. Then the 3D model of the patient's internal anatomical structure can be displayed to a user on a display device.",
    "inventors": [
        "Zheng Jason Geng"
    ],
    "assignee": "Individual",
    "classifications": [
        "G06T7/593",
        "G06T2207/10021",
        "G06T2207/10068",
        "G06T2207/30028"
    ],
    "claims": "\n1. A method for generating a three dimensional (3D) model of an internal anatomical structure of a patient using a plurality of two dimensional (2D) images acquired using a camera, comprising the steps of:\nsearching the plurality of 2D images to detect correspondence points of image features across at least two images;\ndetermining camera motion parameters using the correspondence points for a sequence of at least two 2D images taken at different locations by the camera moving within the internal anatomical structure;\ncomputing dense stereo maps for 2D image pairs that are temporally adjacent;\nforming a 3D model that is consistent by fusing together multiple 2D images which are applied to a plurality of integrated 3D model segments; and\ndisplaying the 3D model of the patient's anatomical structure to a user on a display device.\n2. The method of claim 1, wherein the step of searching the plurality of 2D images further comprises:\nsearching each 2D image for feature points; and\nsearching across subsequent frames to detect correspondence points between each 2D image and subsequent 2D images.\n3. The method of claim 1, further comprising the step of capturing the 2D images using a capsule camera configured to travel through the internal anatomical structure.\n4. The method of claim 1, wherein the step of estimating camera motion parameters comprises the step of representing a capsule camera using a pin-hole camera model to describe a projection of a 3D point P to an image coordinate p through a perspective camera and a 2D image feature point defined by p=(x, y, 1).\n5. The method of claim 1, wherein the step of estimating camera motion parameters comprises the steps of:\nselecting keyframes suited for analysis of structure and motion recovery data; and\nutilizing intrinsic parameters of a capsule camera to avoid problems relating to critical camera sequences.\n6. The method of claim 1, further comprising the step of selecting keyframes by evaluating a lower bound for the resulting estimation error of initial camera parameters and initial 3D feature points selected from the correspondence points.\n7. The method of claim 1, wherein the step of calculating dense stereo maps further comprises the steps of:\nselecting multiple image pairs with different base line distances;\nselecting multiple frame image pairs having minimized camera motion errors in order to improve accuracy of the 3D images; and\ncomputing dense stereo image maps between selected multiple image pairs.\n8. The method of claim 1, wherein the step of calculating the dense stereo maps comprises:\ncreating an approximate 3D surface representation of the dense stereo maps suitable for visualization; and\nutilizing a parametric surface model in order to achieve spatial coherence for a connected surface of a depth map.\n9. The method of claim 1, further comprising the step of providing 3D sizing of selected pathological structures to enable a physician to determine the size, degree and stage of a visible disease.\n10. The method of claim 1, wherein the 2D image pairs are taken at different times.\n11. A method for generating a 3D model from a plurality of 2D images, comprising the steps of:\ninitiating a 2D image salient feature search for a first image to identify correspondence points between the first image and subsequent 2D images;\ncalculating camera motion parameters from subsequent 2D images using correspondence points between the first 2D image and subsequent 2D images;\nperforming key frame selection procedures utilizing stochastic analysis to lower camera error parameters and enhance 3 D positions of feature points to thereby significantly increase the convergence probability of a bundle adjustment and computation of dense depth maps with increased accuracy;\nforming a 3D model that is consistent by fusing together multiple 2D images which are applied to a plurality of integrated 3D model segments; and\ngenerating texture fusion for textures applied to the 3D model utilizing the 2D image sequence and the computed dense depth map data in order to enhance realism of the 3D model.\n12. The method of claim 11, further comprising the step of determining 3D sizing of selected pathological structures to enable a physician to determine the size, degree and stage of a detected disease.\n13. The method of claim 11, further comprising the step of tagging selected pathological structures on the 3D model to enable a reviewing physician to quickly locate marked candidate area locations on the 3D model to expedite quantitative analysis of target pathological structures.\n14. The method of claim 11, further comprising the step of enhancing 3D visualization of 3D model with 3D fly-through virtual camera zoom-in capability to provide visualization and diagnosis.\n15. A method for generating a three dimensional (3D) model of a patient's internal anatomical structure by analyzing a plurality of 2D images acquired using a camera, comprising the steps of:\nsearching the plurality of 2D images to detect correspondence points of image features across at least two 2D images;\nestimating camera motion parameters using the correspondence points for a sequence of at least two images taken at different times and locations by the camera moving within the internal anatomical structure;\ndetermining 3D model points by triangulation using an average of two lines of sight from at least two 2D images;\ncomputing dense stereo maps between 2D image pairs that are temporally adjacent by fusing a matching measure from the image pair with multiple baselines from multiple 2D images into a single matching measure;\napplying a texture map that is fused together from a plurality of 2D images related to the 3D model point; and\ndisplaying the 3D model of the patient's internal anatomical structure to a user on a display device.\n16. The method of claim 15, wherein the step of computing dense stereo maps is performed using the Sum of Squared Difference (SSD) over a defined window to determine measures of image matching with an unambiguous minimum representing depth.\n17. The method of claim 15, wherein the step of calculating dense stereo maps further comprises the steps of:\nselecting multiple image pairs with different base line distances;\nselecting multiple frame image pairs having minimized camera motion errors in order to improve accuracy of the 3D images; and\ncomputing dense stereo image maps between selected multiple image pairs.\n18. The method of claim 15, wherein the step of estimating camera motion parameters comprises the step of selecting keyframes suited for analysis of structure and motion recovery data by evaluating a lower bound for the resulting estimation error of initial camera parameters and initial 3D feature points.\n19. The method of claim 15, further comprising the step of interpolating the dense stereo maps for depth in a spatial orientation using a parametric surface model.\n20. The method of claim 15, further comprising the step of integrating a plurality of 3D surfaces from an object captured from different directions with partial overlapping by using the Iterative Closest Point (ICP) method.",
    "status": "Abandoned",
    "citations_own": [
        "US5821943A",
        "US6072496A",
        "US6240312B1",
        "US6643385B1",
        "US20050107695A1",
        "US20050163356A1",
        "US20050187479A1",
        "US20050251017A1",
        "US20050271269A1",
        "US20060195014A1",
        "US7103211B1",
        "US20070091713A1",
        "US20070196007A1",
        "US20080058597A1",
        "US20080152206A1",
        "US20090216079A1"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US20090074265A1",
        "US20090208143A1",
        "US20090303247A1",
        "US20100277571A1",
        "US20100290697A1",
        "US20110025830A1",
        "US20110175998A1",
        "US20110228997A1",
        "US20120105599A1",
        "US20120127270A1",
        "DE102011076338A1",
        "US20120306847A1",
        "US20130141433A1",
        "US8463024B1",
        "US20130155199A1",
        "US20130215229A1",
        "US20130258059A1",
        "US20130293696A1",
        "US20130321583A1",
        "US20140010407A1",
        "US20140210817A1",
        "US20140285486A1",
        "US20140340489A1",
        "US20140363048A1",
        "US20150002699A1",
        "US8942917B2",
        "US9014421B2",
        "US9066075B2",
        "US20150193935A1",
        "US9087408B2",
        "US20150213607A1",
        "US20150243047A1",
        "US9123115B2",
        "US9148673B2",
        "US9208608B2",
        "US20150371396A1",
        "US9236024B2",
        "CN105308621A",
        "US9286715B2",
        "US20160104286A1",
        "US20160217558A1",
        "US9430813B2",
        "WO2016154571A1",
        "WO2016128965A3",
        "US20160307052A1",
        "US9483853B2",
        "US9558575B2",
        "US9619933B2",
        "TWI604417B",
        "WO2017197085A1",
        "CN107451983A",
        "US9846963B2",
        "CN108460724A",
        "US20180293751A1",
        "CN108734652A",
        "WO2018217663A1",
        "US20190021865A1",
        "US10217225B2",
        "US10242488B1",
        "US10292617B2",
        "CN109947886A",
        "US10460512B2",
        "US20190340805A1",
        "US10473593B1",
        "US10488371B1",
        "US10685433B2",
        "CN112261399A",
        "US10914191B2",
        "US10928362B2",
        "US10943320B2",
        "US10958843B2",
        "US11010630B2",
        "US11017540B2",
        "US11055909B2",
        "US11079285B2",
        "US11215711B2",
        "US11268881B2",
        "US20220295139A1",
        "US20220295040A1",
        "US11481964B2",
        "US20220406025A1",
        "WO2023282452A1",
        "EP4064206A4",
        "WO2023061000A1",
        "US11645819B2",
        "US11657578B2",
        "US11656449B2",
        "US11710309B2"
    ],
    "citedby_ftf": []
}