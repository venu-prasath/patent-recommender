{
    "patent_id": "US-11436429-B2",
    "title": "Artificial intelligence-based sequencing ",
    "assignee": "Illumina, Inc.",
    "publication_date": "2022-09-06",
    "patent_link": "https://patents.google.com/patent/US11436429B2/en",
    "inventors": [
        "Kishore JAGANATHAN",
        "Anindita DUTTA",
        "Dorna KASHEFHAGHIGHI",
        "John Randall GOBBEL",
        "Amirali KIA"
    ],
    "classifications": [
        "G16B40/20",
        "G06K9/6218",
        "G06F16/58",
        "G06F16/907",
        "G06F18/213",
        "G06F18/214",
        "G06F18/217",
        "G06F18/23",
        "G06F18/23211",
        "G06F18/24",
        "G06F18/2415",
        "G06F18/2431",
        "G06K9/6222",
        "G06K9/6232",
        "G06K9/6256",
        "G06K9/6262",
        "G06K9/6267",
        "G06K9/6277",
        "G06K9/628",
        "G06N3/04",
        "G06N3/044",
        "G06N3/045",
        "G06N3/08",
        "G06N3/084",
        "G06N7/005",
        "G06N7/01",
        "G06V10/454",
        "G06V10/751",
        "G06V10/763",
        "G06V10/764",
        "G06V10/7715",
        "G06V10/7784",
        "G06V10/82",
        "G06V10/993",
        "G16B30/20",
        "G16B40/00",
        "G06N5/046"
    ],
    "abstract": "The technology disclosed processes a first input through a first neural network and produces a first output. The first input comprises first image data derived from images of analytes and their surrounding background captured by a sequencing system for a sequencing run. The technology disclosed processes the first output through a post-processor and produces metadata about the analytes and their surrounding background. The technology disclosed processes a second input through a second neural network and produces a second output. The second input comprises third image data derived by modifying second image data based on the metadata. The second image data is derived from the images of the analytes and their surrounding background. The second output identifies base calls for one or more of the analytes at one or more sequencing cycles of the sequencing run.",
    "claims": "\n1. A computer-implemented method of end-to-end sequencing, including template generation and base calling, comprising:\naccessing first image data and second image data that contain pixels in an optical, pixel resolution,\nwherein the first image data comprises images of clusters and their surrounding background captured by a sequencing system for initial ones of sequencing cycles of a sequencing run, and\nwherein the second image data comprises images of the clusters and their surrounding background captured by the sequencing system for the sequencing cycles of the sequencing run;\nprocessing the first image data through a neural network-based template generator, and producing a cluster map that identifies cluster metadata,\nwherein the cluster metadata includes cluster centers, cluster shapes, cluster sizes, cluster background, and/or cluster boundaries, and\nwherein the neural network-based template generator is trained on a task of mapping the images of the clusters to the cluster metadata;\nencoding the cluster metadata in a template image in an upsampled, subpixel resolution,\nwherein subpixels of the template and the pixels of the images of the clusters represent a same image area;\nmodifying intensity values of the pixels of the second image data based on the template image, and producing an intensity modified version of the second image data with an intensity distribution that accounts for the cluster metadata; and\nprocessing the intensity modified version of the second image data through a neural network-based base caller, and producing base calls for one or more of the clusters at one or more sequencing cycles of the sequencing run, wherein the neural network-based base caller is trained on a task of mapping the images of the clusters to the base calls.\n2. The computer-implemented method of claim 1, further including:\nsupplementing the second image data with the template image; and\nprocessing the second image data, supplemented with the template image, through the neural network-based base caller, and producing base calls for one or more of the clusters at one or more sequencing cycles of the sequencing run.\n3. The computer-implemented method of claim 1, wherein each subpixel in the template image is identified as either background subpixel, cluster center subpixel, or cluster interior subpixel.\n4. The computer-implemented method of claim 3, wherein modifying intensity values of the pixels of the second image data comprises:\ncalculating an area weighting factor for one or more pixels in the second image data based on how many subpixels in the template image that correspond to a pixel in the images of the second image data contain parts of one or more of the clusters; and\nmodifying intensities of the pixels based on the area weighting factor.\n5. The computer-implemented method of claim 4, wherein modifying intensity values of the pixels of the second image data comprises:\nupsampling the images of clusters and their surrounding background to the upsampled, subpixel resolution to produce upsampled images, and assigning a background intensity to those subpixels in the upsampled images that correspond to background subpixels in the template image and assigning cluster intensities to those subpixels in the upsampled images that correspond to cluster center subpixels and cluster interior subpixels in the template image.\n6. The computer-implemented method of claim 5, wherein the background intensity has a zero value.\n7. The computer-implemented method of claim 6, wherein the cluster intensities are determined by interpolating intensities of the pixels in the optical, pixel resolution.\n8. The computer-implemented method of claim 7, wherein modifying intensity values of the pixels of the second image data comprises:\nupsampling the images of clusters and their surrounding background to the upsampled, subpixel resolution to produce upsampled images, and distributing an entire intensity of a pixel in the optical, pixel domain among only those constituent subpixels of the pixel in the upsampled images that correspond to the cluster center subpixels and the cluster interior subpixels in the template image.\n9. A computer-implemented base calling method, comprising: using a first neural network to determine a template image about clusters, wherein the template image identifies at least one property selected from the group consisting of: spatial distribution of the clusters, cluster shape, centers of the clusters and cluster boundary; and using a second neural network to base call the clusters based on the template image.\n10. The computer-implemented method of claim 9, wherein the template image comprises modified intensity values to identify at least one of the properties selected from the group consisting of: spatial distribution of the clusters, cluster shape, centers of the clusters and cluster boundary; and\nprocessing the modified intensity values through the second neural network to base call the clusters.\n11. The computer-implemented method of claim 10, further comprising:\nevaluating the template image in an upsampled subpixel domain for at least one particular cluster to identify a pixel that contains part of the at least one particular cluster and adjoining pixels to the pixel that also contain part of the at least one particular cluster;\ncalculating an area weighting factor for each pixel based on how many subpixels in each of the identified pixels contain parts of the at least one particular cluster; and\nmodifying a pixel intensity value of the identified pixel and the adjoining pixels for processing based on the area weighting factor for a respective pixel.\n12. The computer-implemented method of claim 11, wherein evaluating the template image further comprises:\nprocessing one or more initial image sets respectively generated at one or more initial sequencing cycles of a plurality of sequencing cycles through the first neural network to produce the template image to identify the centers, shapes, and boundaries of the clusters at the upsampled, subpixel resolution; wherein each image set comprises one or more images, each of the images depicting intensity emissions of the clusters and their surrounding background in a respective one of one or more imaging channels captured at the optical, pixel resolution.\n13. The computer-implemented method of claim 12, wherein evaluating the template image further comprises:\nevaluating the cluster shape and boundaries of the at least one particular cluster to identify at least one pixel that contains part of the at least one particular cluster and adjoining pixels to the pixel that also contain part of the at least one particular cluster; and wherein the method further comprises storing the area weighting factor in the template image; and\ngenerating a modified version of each of the images with pixels having modified pixel intensity values;\nprocessing modified versions of the images through the second neural network to generate an alternative representation of the modified versions; and\nbase calling the at least one particular cluster using the alternative representation.\n14. The computer-implemented method of claim 13, wherein the base calling further comprises:\naccessing one or more images at the optical, pixel resolution in each\nof a current image set generated at a current one of the plurality of sequencing cycles,\nof a one or more preceding image sets respectively generated at one or more of the plurality of sequencing cycles preceding the current one of the plurality of sequencing cycles, and\nof a one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the current one of the plurality of sequencing cycles;\nfor pixels in each of the images, modifying a pixel intensity value based on the area weighting factor in the template image for a respective pixel;\ngenerating a modified version of each of the images with pixels having modified pixel intensity values;\nfor the at least one particular cluster, extracting an image patch from each modified version such that each image patch\nhas an array of pixels, and\ncontains in its center pixel a center of the particular cluster identified in the template image;\nconvolving image patches extracted from modified versions of the images through a convolutional neural network of the second neural network to generate a convolved representation of the image patches;\nprocessing the convolved representation through an output layer to produce, for the center pixel, likelihoods of a base incorporated in the at least one particular cluster at the current one of the plurality of sequencing cycles being A, C, T, and G; and\nclassifying the base as A, C, T, or G based on the likelihoods.\n15. The computer-implemented method of claim 14, further comprising:\nprior to modifying the pixel intensity values, aligning each of the images captured at the optical, pixel resolution with the template image using cycle-specific and imaging channel-specific transformations.\n16. The computer-implemented method of claim 9, further comprising:\nevaluating the template image in an upsampled subpixel domain to identify subpixels that contain parts of any cluster; and\nassigning a background intensity to subpixels identified in the template image as not contributing to any cluster.\n17. The computer-implemented method of claim 16, wherein evaluating the template image in an upsampled subpixel domain further comprises:\ncalculating how many subpixels in at least one pixel contain parts of any cluster and calculating a per-subpixel area weighting factor for the subpixels in the at least one pixel.\n18. The computer-implemented method of claim 17, wherein the method comprises:\nprocessing one or more initial image sets respectively generated at one or more initial sequencing cycles of a plurality of sequencing cycles through the first neural network to produce the template image at the upsampled, subpixel resolution, wherein each image set comprises one or more images, each of the images depicting intensity emissions of the clusters and their surrounding background in a respective one of one or more imaging channels captured at the optical, pixel resolution and wherein the template image classifies subpixels into classes including cluster center, background, and cluster interior;\nupsampling each of the images captured at the optical, pixel resolution into a subpixel domain and assigning a background intensity to subpixels of each of the images identified in the template image as not contributing to any cluster;\nprocessing the upsampled images through the second neural network to generate an alternative representation of the upsampled images; and\nbase calling a plurality of the clusters using the alternative representation.\n19. The computer-implemented method of claim 18, wherein upsampling each of the images further comprises:\ndistributing intensity of a particular pixel among first subpixels of the particular pixel identified in the template image as contributing to any cluster by applying the per-subpixel area weighting factor and assigning a background intensity to second subpixels of the particular pixel identified in the template as not contributing to any cluster.\n20. The computer-implemented method of claim 19, wherein the prior to upsampling the method comprises:\naccessing one or more images at the optical, pixel resolution in each\nof a current image set generated at a current one of the plurality of sequencing cycles,\nof a one or more preceding image sets respectively generated at one or more of the plurality of sequencing cycles preceding the current one of the plurality of sequencing cycles, and\nof a one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the current one of the plurality of sequencing cycles; and after upsampling the method comprises:\nextracting an image patch from each upsampled image such that each image patch has an array of subpixels;\nconvolving image patches extracted from the upsampled images through the convolutional neural network of the second neural network to generate a convolved representation of the image patches;\nprocessing the convolved representation through an output layer to produce, for each subpixel in the array, likelihoods of a base incorporated at the current one of the plurality of sequencing cycles being A, C, T, and G;\nclassifying the base as A, C, T, or G based on the likelihoods; and\nbase calling each one of the plurality of the clusters based on a base classification assigned to a respective subpixel containing a center of a corresponding cluster.\n21. The computer-implemented method of claim 20, further comprising:\nprior to the upsampling, aligning each of the images captured at the optical, pixel resolution with the template image using cycle-specific and imaging channel-specific transformations.\n22. A sequencing system, comprising:\na receptacle coupled to a biosensor system, the biosensor system configured to comprise an array of light detectors, the biosensor system comprising a biosensor, and the biosensor comprising reaction sites configured to contain clusters;\nan illumination system configured to direct excitation light toward the biosensor and illuminate the clusters in the reaction sites, wherein at least some of the clusters provide emission signals when illuminated; and\na system controller coupled to the receptacle and comprising an analysis module, the analysis module configured to:\ngenerate a template image that reduces positional uncertainty by identifying at least one property selected from the group consisting of: spatial distribution of the clusters, cluster shape, centers of the clusters and cluster boundary;\nobtain image data from the light detectors at each of a plurality of sequencing cycles, wherein the image data is derived from the emission signals detected by the light detectors; and\nprocess the image data for each of the plurality of sequencing cycles based on the template image through a neural network and produce a base call for at least some of the clusters at each of the plurality of sequencing cycles.",
    "status": "Active",
    "citations_own": [
        "US3401258A",
        "US3594439A",
        "US20030062485A1",
        "US20060014151A1",
        "US20060040297A1",
        "US20060064248A1",
        "US20060269130A1",
        "JP2007199397A",
        "WO2008154317A1",
        "US20090081775A1",
        "US20100046830A1",
        "US20100111370A1",
        "US20100157086A1",
        "US20110065607A1",
        "US20110286628A1",
        "US20120015825A1",
        "US20120020537A1",
        "US8241573B2",
        "US8392126B2",
        "US8407012B2",
        "US20130079232A1",
        "US20130124100A1",
        "US20130188866A1",
        "US20130250407A1",
        "US8725425B2",
        "US20140152801A1",
        "WO2014142921A1",
        "US20150079596A1",
        "US20150117784A1",
        "WO2015084985A2",
        "US20150169824A1",
        "US20160042511A1",
        "US20160078272A1",
        "US20160110498A1",
        "US20160196479A1",
        "WO2016145516A1",
        "US20160357903A1",
        "US20160356715A1",
        "CA2894317A1",
        "WO2016201564A1",
        "US20160371431A1",
        "EP3130681A1",
        "US20170116520A1",
        "US20170169313A1",
        "US9708656B2",
        "US20170249744A1",
        "US20170249421A1",
        "WO2017184997A1",
        "US20170362634A1",
        "US20180075279A1",
        "US20180107927A1",
        "US20180114337A1",
        "US20180189613A1",
        "US20180195953A1",
        "US10023911B2",
        "US20180201992A1",
        "US20180211001A1",
        "US10068054B2",
        "EP3373238A1",
        "WO2018165099A1",
        "US20180305751A1",
        "US20180322327A1",
        "WO2018203084A1",
        "US20180330824A1",
        "US20180334712A1",
        "US20180334711A1",
        "US20180340234A1",
        "US10168438B2",
        "US20190034586A1",
        "WO2019027767A1",
        "WO2019028047A1",
        "US20190080450A1",
        "WO2019055856A1",
        "US10241075B2",
        "US20190107642A1",
        "US20190114544A1",
        "WO2019079202A1",
        "WO2019090251A2",
        "US20190156915A1",
        "US20190164010A1",
        "US20190170680A1",
        "WO2019136388A1",
        "WO2019136284A1",
        "US10354747B1",
        "WO2019140402A1",
        "WO2019147904A1",
        "US20190237163A1",
        "US20190272638A1",
        "US20190332118A1",
        "US20190392578A1",
        "US10527549B2",
        "WO2020014280A1",
        "US10540591B2",
        "US20200027002A1",
        "US20200054306A1",
        "US20200057838A1",
        "US10648027B2",
        "US20200176082A1",
        "US20200193597A1",
        "WO2020123552A1",
        "US10711299B2",
        "US10740883B2",
        "US20200302603A1",
        "US20200320294A1",
        "US20200388029A1",
        "US20210027462A1",
        "US20210056287A1",
        "US20210072391A1",
        "US20210089827A1",
        "US20210115490A1",
        "US11138496B2"
    ],
    "citations_ftf": [
        "CA2044616A1",
        "US6090592A",
        "US5641658A",
        "EP0975802B1",
        "AR021833A1",
        "CN101525660A",
        "AU2002227156A1",
        "AR031640A1",
        "US7057026B2",
        "US20040002090A1",
        "WO2004018497A2",
        "GB0321306D0",
        "EP2789383B1",
        "US7476503B2",
        "WO2006064199A1",
        "AU2006259565B2",
        "GB0514936D0",
        "GB0517097D0",
        "US7405281B2",
        "GB0522310D0",
        "EP2021503A1",
        "US20080242560A1",
        "WO2008115410A2",
        "US8594439B2",
        "US9023769B2",
        "US10619195B2",
        "EP2390810B1",
        "US9096899B2",
        "CA2833165A1",
        "WO2013096692A1",
        "KR20160103398A",
        "US9836839B2",
        "US10061972B2",
        "US10176408B2",
        "US11094058B2",
        "US10755810B2",
        "US10930372B2",
        "CN109416928A",
        "WO2018131898A2",
        "US10740880B2",
        "US10491239B1",
        "US10713794B1",
        "JP6915349B2",
        "US10943255B1",
        "US20200256856A1",
        "CN111527044A",
        "WO2019084559A1",
        "EP3640837A1",
        "KR20200043169A",
        "US10818386B2",
        "US10789462B2",
        "EP3939047A4",
        "US11210554B2",
        "US11783917B2",
        "CN110245685B",
        "CN112368567A",
        "US11593649B2",
        "WO2020243185A1",
        "US20230343416A1"
    ],
    "citedby_own": [
        "US20220197709A1",
        "US11782760B2"
    ],
    "citedby_ftf": [
        "US11615285B2",
        "CN108875455B",
        "US11468603B2",
        "US11561196B2",
        "AU2019206709B2",
        "JP6992590B2",
        "US11663478B2",
        "US11443181B2",
        "JP2020000197A",
        "US11170506B2",
        "US11569978B2",
        "US11652603B2",
        "US11783917B2",
        "US11275973B2",
        "WO2020231016A1",
        "US11593649B2",
        "US11257183B1",
        "US11100122B2",
        "US10877540B2",
        "US20210110243A1",
        "TWI738131B",
        "US11228683B2",
        "US11580401B2",
        "US11797827B2",
        "US11651210B2",
        "IL294629A",
        "US11636602B1",
        "JP2023515108A",
        "US11546568B1",
        "US11188778B1",
        "US11526964B2",
        "US11488021B2",
        "KR20220013231A",
        "US11200446B1",
        "US11800258B2",
        "US11361194B2",
        "US11468634B2",
        "US11694415B2",
        "CN112488392B",
        "EP4241200A1",
        "CN112668481A",
        "CN112862266B",
        "KR102444841B1",
        "CN112800244B",
        "WO2022197754A1",
        "CA3183578A1",
        "CN113222831A",
        "CA3212217A1",
        "CN112950632B",
        "WO2022229218A2",
        "US11693570B2",
        "US11251031B1",
        "US20230005253A1",
        "WO2023003758A1",
        "US11455487B1",
        "WO2023009758A1",
        "WO2023010069A1",
        "US20230093253A1",
        "WO2023049212A2",
        "WO2023115550A1",
        "KR20230116549A",
        "CN116204770B"
    ]
}