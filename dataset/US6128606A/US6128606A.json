{
    "patent_link": "https://patents.google.com/patent/US6128606A/en",
    "patent_id": "US6128606A",
    "title": "Module for constructing trainable modular network in which each module inputs and outputs data structured as a graph",
    "abstract": "A machine learning paradigm called Graph Transformer Networks extends the applicability of gradient-based learning algorithms to systems composed of modules that take graphs as inputs and produce graphs as output. Training is performed by computing gradients of a global objective function with respect to all the parameters in the system using a kind of back-propagation procedure. A complete check reading system based on these concept is described. The system uses convolutional neural network character recognizers, combined with global training techniques to provides record accuracy on business and personal checks.",
    "inventors": [
        "Yoshua Bengio",
        "Leon Bottou",
        "Yann Andre LeCun"
    ],
    "assignee": "AT&T Corp",
    "classifications": [
        "G06N3/084",
        "G06F30/18",
        "G06N20/00",
        "G06N3/105",
        "G06N7/01",
        "G06V30/153",
        "G06V30/18057",
        "G06F2111/12",
        "G06N3/044",
        "G06N3/045",
        "G06V30/10"
    ],
    "claims": "\n1. A computerized method of training a network of graph transformers comprising the steps of:\ngenerating a network of graph transformers, each graph transformer having an input receiving a first graph, having an output outputting a second graph, and having an internal function producing data in the second graph from data in the first graph;\nwherein said network of graph transformers includes:\na) at least one tunable graph transformer having at least one tunable parameter being used to produce data in the second graph of the at least one tunable graph transformer from data in the first graph of the at least one tunable graph transformer;\nb) the internal function of each graph transformers differentiable with respect to data in the first graph and\nc) the internal function of the at least one tunable graph transformer also differentiable with respect to the at least one tunable parameter; and\nback-propagating gradients through the network to determine a minimum of a global objective function of the network; wherein said back-propagated gradients are used to train said network of graph transformers.\n2. The method according to claim 1, wherein the back-propagation step determines the minimum of the global objective function by:\na) calculating a gradient of the global objective function by back-propagating gradients through the network;\nb) adjusting the at least one tunable parameter using the calculated gradient of the global objective function; and\nc) repeating steps a) and b) until reaching the minimum of the global objective function.\n3. The method according to claim 2, wherein the step b) of adjusting further comprises decrementing the at least one tunable parameter by an amount equal to the calculated gradient of the global objective function.\n4. The method according to claim 2, wherein the step b) of adjusting further comprises decrementing the at least one tunable parameter by an amount equal to the calculated gradient of the global objective function multiplied by a positive constant.\n5. The method according to claim 2, wherein the step b) of adjusting further comprises decrementing the at least one tunable parameter by an amount equal to the calculated gradient of the global objective function multiplied by a step size.\n6. The method according to claim 2, wherein the step b) of adjusting further comprises decrementing the at least one tunable parameter by an amount equal to the calculated gradient of the global objective function multiplied by a learning rate.\n7. The method according to claim 2, wherein the step b) of adjusting further comprises adjusting the at least one tunable parameter according to the gradient descent algorithm.\n8. The method according to claim 2, wherein the step b) of adjusting further comprises adjusting the at least one tunable parameter according to the conjugate gradient algorithm.\n9. The method according to claim 2, wherein the step b) of adjusting further comprises adjusting the at least one tunable parameter according to the Newton algorithm.\n10. The method according to claim 2, wherein the step b) of adjusting further comprises adjusting the at least one tunable parameter according to the adaptive step-size algorithm.\n11. The method according to claim 2, wherein the step b) of adjusting further comprises adjusting the at least one tunable parameter using a complex function of the calculated gradient of the global objective function.\n12. The method according to claim 2, wherein the step b) of adjusting further comprises adjusting the at least one tunable parameter using a complex function of the calculated gradient of the global objective function and secondary information.\n13. The method according to claim 12, wherein the secondary information includes the second derivative of the calculated gradient of the global objective function.\n14. The method according to claim 12, wherein the secondary information includes a previous update vector.\n15. The method according to claim 12, wherein the secondary information includes a plurality of constants.\n16. The method according to claim 2, wherein the steps a) and b) are repeated until the calculated gradient of the global objective function reaches an acceptable level.\n17. The method according to claim 2, wherein the steps a) and b) are repeated until the calculated gradient of the global objective function falls below a predetermined threshold.\n18. The method according to claim 2, wherein the steps a) and b) are repeated until the global objective function stops decreasing significantly.\n19. The method according to claim 2, wherein the steps a) and b) are repeated until the global objective function when measured on a different set of training examples stops decreasing.\n20. The method according to claim 2, wherein the steps a) and b) are repeated until validation is achieved.\n21. The method according to claim 2, wherein the steps a) and b) are repeated until achieving a stopping criterion.\n22. In a network composed of a plurality (n) of trainable modules, which communicate their states and gradients in the form of graphs, and in which the trainable modules are graph transformers that receive one or more graphs as inputs and outputs a graph, and each graph transformer of a subset of the graph transformers has one or more tunable parameters that are used to produce an output graph for said each graph transformer, a computerized method for training the network at the network level, comprising the steps of:\na) computing for an nth module in the network a first gradient of a global objective function with respect to any tunable parameters in the nth module;\nb) computing a second gradient of the global objective function with respect to an input of the nth module;\nc) using the second gradient calculated in step b) to calculate a gradient of the global objective function with respect to any tunable parameters in a next lower module in the network and a gradient of the global objective function with respect to an input of the next lower module;\nd) repeating step c) for each successive module in the network until reaching the first module, whereby a gradient of the global objective function for each module is calculated with respect to any tunable parameters of said each module;\ne) adjusting any tunable parameters in each module according to a gradient calculated for that module; and,\nf) using said adjusted tunable parameters to train said network at the network level.\n23. The method according to claim 22, further comprising the step of:\ng) repeating the steps a) through e) until reaching a minimum of the global objective function.\n24. The method according to claim 22, further comprising the step of:\ng) repeating the steps a) through e) until the gradients for all of the modules fall below a predetermined threshold.\n25. The method according to claim 22, further comprising the step of:\ng) repeating the steps a) through e) until all of the gradients for all of the modules reach an acceptable level.\n26. The method according to claim 22, wherein each module includes a function that converts data attached to the input graph to data attached to the output graph using any tunable parameter in said each module, and the function is differentiable with respect to any tunable parameters in said each module and the data attached to the input graph of said each module.\n27. The method according to claim 22, wherein the first gradient in the nth module is calculated using the following equations: ##EQU2## and ##EQU3## where F is a function used by the nth module to convert a graph input to the nth module to a graph output by the nth module, Xx-1 is the input to the nth module, and Xn is the output of the nth module, Wn is any tunable parameters in the nth module, ##EQU4## is the Jacobian of F with respect to W evaluated at the point (Wn, Xn-1), and ##EQU5## is the Jacobian of F with respect to X.\n28. The method according to claim 22, wherein the step e) of adjusting further comprises adjusting some of the tunable parameters in at least one module of the plurality of trainable module, but not all of the tunable parameters in said at least one module.\n29. A computerized trainable network for performing a particular function comprising:\na) means for creating a first graph transformer layer having a first input receiving data structured as a first graph, and a first output outputting data structured as a second graph, wherein the transformer layer uses differentiable functions to produce numerical information in the second graph from numerical information in the first graph and from at least one parameter;\nb) means for creating a second graph transformer layer being coupled to the first graph transformer later, having a second input receiving the second graph, and a second output outputting data structured as a third graph, wherein the second graph transformer layer uses differentiable functions to produce numerical information in the third graph from numerical information in the second graph and from at least one parameter;\nc) means for calculating a first gradient of the particular function with respect to the at least one parameter in the second graph transformer, calculating a second gradient of the particular function with respect to the at least one parameter in the first graph transformer, and modifying the at least one parameter in the first and second graph transformers according to the second and first gradients, respectively, until reaching a minimum value for the particular function; and\nd) means for outputting the minimum value which trains the network to perform the particular function.\n30. The network according to claim 29, wherein the first gradient is calculated using the following equations: ##EQU6## and ##EQU7## where F is a function used by the nth module to convert the second graph input to the second module to the third graph output by the second module, X1 is the data structured as the second graph, and X2 is the data structured as the third graph, W2 is the at least one parameter in the second module, ##EQU8## is the Jacobian of F with respect to W evaluated at the point ( W2, X1), and ##EQU9## is the Jacobian of F with respect to X.\n31. The network according to claim 29, wherein the minimum value of the particular function is determined using a gradient based technique.\n32. The network according to claim 29, wherein the gradient based technique includes a stochastic gradient descent.\n33. The network according to claim 29, wherein the minimum value of the particular function is determined when the particular function stops decreasing significantly.\n34. The network according to claim 29, wherein the minimum value of the particular function is determined when the particular function measured on a different set of training examples stops decreasing.\n35. The network according to claim 29, wherein the minimum value of the particular function is determined when validation is achieved.",
    "status": "Expired - Lifetime",
    "citations_own": [
        "US4713778A",
        "US4829450A",
        "US5067165A",
        "US5430744A"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US20020163518A1",
        "US20030174881A1",
        "US20030226111A1",
        "US20050259866A1",
        "US20050283450A1",
        "US6990628B1",
        "US6993396B1",
        "US20060173559A1",
        "US7155441B2",
        "US20070047802A1",
        "US20070282538A1",
        "US20080071802A1",
        "WO2008034075A2",
        "WO2008133951A2",
        "US20090210218A1",
        "WO2009100417A3",
        "US20110129153A1",
        "US20110218950A1",
        "US20120150836A1",
        "JP2012118649A",
        "US20120288181A1",
        "US20130335422A1",
        "JP2014157409A",
        "US8831339B2",
        "US20150242710A1",
        "US20160086078A1",
        "US20160117574A1",
        "US20160180214A1",
        "US20170177739A1",
        "US9754049B2",
        "US9787725B2",
        "US20180018535A1",
        "WO2019067831A1",
        "US20190272468A1",
        "WO2020041026A1",
        "CN111133458A",
        "US10679129B2",
        "US10824946B2",
        "AU2017355535B2",
        "US10891545B2",
        "US10970628B2",
        "JP2021103573A",
        "US11126190B2",
        "US11501164B2"
    ],
    "citedby_ftf": [
        "EP3262569A1",
        "CN107180023B",
        "CN112633169B"
    ]
}