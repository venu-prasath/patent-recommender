{
    "patent_id": "US-10216983-B2",
    "title": "Techniques for assessing group level cognitive states ",
    "assignee": "General Electric Company",
    "publication_date": "2019-02-26",
    "patent_link": "https://patents.google.com/patent/US10216983B2/en",
    "inventors": [
        "Peter Henry Tu",
        "Tao Gao",
        "Jilin Tu"
    ],
    "classifications": [
        "G06V10/62",
        "G06K9/00335",
        "G06N5/041",
        "A61B5/0013",
        "A61B5/0022",
        "A61B5/0077",
        "A61B5/165",
        "A61B5/742",
        "A61B5/746",
        "A61B5/747",
        "G06F18/2415",
        "G06K9/00281",
        "G06K9/00302",
        "G06K9/0055",
        "G06K9/00778",
        "G06K9/6277",
        "G06N20/00",
        "G06N3/04",
        "G06N3/049",
        "G06N5/046",
        "G06N99/005",
        "G06V10/82",
        "G06V20/40",
        "G06V20/53",
        "G06V40/10",
        "G06V40/171",
        "G06V40/174",
        "G06V40/20",
        "A61B2576/00",
        "G06F2218/16",
        "G06K9/0061",
        "G06K9/00718",
        "G06N3/044",
        "G06N3/0445",
        "G06T7/277",
        "G06V20/41",
        "G06V40/193"
    ],
    "abstract": "A security monitoring technique includes receiving data related to one or more individuals from one or more cameras in an environment. Based on the input data from the cameras, agent-based simulators are executed that each operate to generate a model of behavior of a respective individual, wherein an output of each model is symbolic sequences representative of internal experiences of the respective individual during simulation. Based on the symbolic sequences, a subsequent behavior for each of the respective individuals is predicted when the symbolic sequences match a query symbolic sequence for a query behavior.",
    "claims": "\n1. A method, comprising:\nreceiving data related to one or more individuals from one or more cameras in an environment;\nexecuting one or more agent-based simulators that each operate to generate a model of behavior of a respective individual, wherein an output of each model is symbolic sequences representative of internal experiences of the respective individual during simulation; and\npredicting a subsequent behavior for each of the respective individuals when the symbolic sequences match a query symbolic sequence for a query behavior;\nwherein each model uses particle filtering and each particle includes recurrent neural networks that iteratively estimates temporal evolution of the symbolic sequences based on the data.\n2. The method of claim 1, wherein particles that include similar symbolic sequences are allowed to transition to the next iteration to predict a next set of internal symbols of the symbolic sequences.\n3. The method of claim 1, wherein particles that do not include similar symbolic sequences are terminated.\n4. The method of claim 1, wherein the recurrent neural networks are used to predict the subsequent behavior based on the symbolic sequences.\n5. The method of claim 4, wherein the recurrent neural networks are seeded with random internal experience symbols initially.\n6. The method of claim 5, wherein the recurrent neural networks predict the subsequent behavior by sampling a next set of physical state symbols and comparing the next set of physical symbols to physical state symbols of the query symbolic sequence.\n7. The method of claim 1, wherein the symbolic sequences include stored graphics for character type, emotion, observed expressions, or some combination thereof.\n8. The method of claim 7, wherein the character type comprises sunshine, predator, stranger, depressed, or nervous, and the emotion comprises angry, frustrated, neutral, or happy.\n9. The method of claim 1, comprising performing an action when a certain behavior is predicted, wherein the action comprises sounding an alarm, calling emergency services, triggering an alert, sending a message, displaying an alert, or some combination thereof.\n10. The method of claim 1, wherein the one or more cameras comprise red, green, blue, depth (RGB+D) cameras that capture estimates of location and articulated body motion, and fixed cameras and pan tilt zoom (PTZ) cameras that capture facial imagery.\n11. One or more tangible, non-transitory computer-readable media storing computer instructions that, when executed by one or more processors, cause the one or more processors to:\nreceive data related to one or more individuals from one or more cameras in an environment;\nexecute one or more agent based simulators that each model behavior of a respective individual and each output symbolic sequences representative of internal experiences of the respective individual during simulation; and\npredict a subsequent behavior for each of the respective individuals when the symbolic sequences match a query symbolic sequence for a query behavior;\nwherein each model uses particle filtering and each particle includes recurrent neural networks that iteratively estimates temporal evolution of the symbolic sequences based on the data.\n12. The one or more computer-readable media of claim 11, wherein particles that include similar symbolic sequences are allowed to transition to the next iteration to predict a next set of internal symbols of the symbolic sequences.\n13. The one or more computer-readable media of claim 11, wherein the recurrent neural networks are used to predict the subsequent behavior based on the symbolic sequences.\n14. The one or more computer-readable media of claim 11, wherein the computer instructions, when executed by the processor, cause the one or more processors to perform an action when a certain behavior is predicted, wherein the action comprises sounding an alarm, calling emergency services, triggering an alert, sending a message, displaying an alert, or some combination thereof.\n15. A system, comprising:\none or more cameras that capture data related to a behavior of one or more individuals in an environment;\none or more computing devices comprising one or more processors that:\nreceive the data related to the behavior of one or more individuals from one or more cameras in an environment;\nexecute one or more agent based simulators that each model the behavior of a respective individual and each output symbolic sequences representative of internal experiences of the respective individual during simulation; and\npredict a subsequent behavior for each of the respective individuals when the symbolic sequences match a query symbolic sequence for a query behavior; and\na display coupled to the one or more computing devices and configured to display an indication representative of the subsequent behavior;\nwherein each model uses particle filtering and each particle includes recurrent neural networks that iteratively estimates temporal evolution of the symbolic sequences based on the data.\n16. The system of claim 15, wherein the one or more cameras comprise red, green, blue, depth (RGB+D) cameras that capture estimates of location and articulated body motion, and fixed cameras and pan tilt zoom (PTZ) cameras that capture facial imagery.\n17. The system of claim 15, wherein the one or more computing devices comprise a smartphone, a smartwatch, a tablet, a laptop computer, a desktop computer, a server in a cloud-based computing system, or some combination thereof.\n18. The system of claim 15, wherein the one or more processors perform an action when a certain subsequent behavior is predicted, the action comprising sounding an alarm, calling emergency services, triggering an alert, sending a message, displaying an alert, or some combination thereof.",
    "status": "Active",
    "citations_own": [
        "US20040057556A1",
        "US20060059113A1",
        "US20060153328A1",
        "WO2008064431A1",
        "US7418082B2",
        "US7450683B2",
        "US7486764B2",
        "US20110263946A1",
        "US20120250982A1",
        "US20120310591A1",
        "US8442839B2",
        "US20130173504A1",
        "CN103716324A",
        "US20140222738A1",
        "US20140355734A1",
        "US20150179291A1",
        "US20150282766A1",
        "US20160124908A1",
        "US20160142679A1",
        "US20170358154A1"
    ],
    "citations_ftf": [
        "US7433493B1",
        "US8195598B2",
        "JP5366047B2",
        "JP2011186521A",
        "JP5620147B2"
    ],
    "citedby_own": [
        "US11381651B2",
        "EP4004814A4"
    ],
    "citedby_ftf": [
        "US10445565B2",
        "US20180190377A1",
        "JP6615800B2",
        "US11151992B2",
        "US10963493B1",
        "US10929759B2",
        "US10475222B2",
        "US10832393B2",
        "CN110472726B",
        "US11514767B2",
        "CN110991375B",
        "US11497418B2",
        "US11587428B2",
        "US11373425B2",
        "US11763591B2",
        "US11595723B2",
        "US11553247B2",
        "US20220245388A1"
    ]
}