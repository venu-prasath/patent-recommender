{
    "patent_id": "US-11055616-B2",
    "title": "Architecture for an explainable neural network ",
    "assignee": "UMNAI Limited",
    "publication_date": "2021-07-06",
    "patent_link": "https://patents.google.com/patent/US11055616B2/en",
    "inventors": [
        "Angelo Dalli",
        "Mauro PIRRONE"
    ],
    "classifications": [
        "G06N3/08",
        "G06N3/084",
        "G06N3/042",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/063",
        "G06N5/003",
        "G06N5/01",
        "G06N5/022",
        "G06N3/044",
        "G06N3/088",
        "G06N5/02"
    ],
    "abstract": "An architecture for an explainable neural network may implement a number of layers to produce an output. The input layer may be processed by both a conditional network and a prediction network. The conditional network may include a conditional layer, an aggregation layer, and a switch output layer. The prediction network may include a feature generation and transformation layer, a fit layer, and a value output layer. The results of the switch output layer and value output layer may be combined to produce the final output layer. A number of different possible activation functions may be applied to the final output layer depending on the application. The explainable neural network may be implementable using both general purpose computing hardware and also application specific circuitry including optimized hardware only implementations. Various embodiments of XNNs are described that extend the functionality to different application areas and industries.",
    "claims": "\n1. A system for an artificial neural network that is implemented by a combination of hardware and software and that is interpretable and explainable, comprising:\nan input layer which receives an input and identifies one or more input features;\na conditional network, comprising:\na conditional layer configured to model the input features based on one or more partitions, wherein each of the one or more partitions comprises a rule;\nan aggregation layer configured to aggregate one or more rules into one or more aggregated partitions; and\na switch output layer configured to selectively pool the aggregated partitions from the aggregation layer with the one or more partitions from the conditional layer;\na prediction network, comprising:\na feature generation and transformation network comprising one or more transformation neurons configured to apply one or more transformations to the input features;\na fit layer configured to combine features which have been transformed by the feature generation and transformation network to identify one or more coefficients related to at least one of: one or more features and one or more partitions;\na value output layer configured to output a value related to at least one of: one or more features, one or more partitions, as applied to the one or more coefficients; and\nan output layer configured to generate an output which is interpretable and explainable by at least one of a machine program or a human;\nwherein each of the one or more rules is an if-then rule;\nwherein each of the one or more partitions forms at least one local model and a combination of local models forms a global model; and\nwherein one or more paths throughout the partitions are identifiable by an external process.\n2. The system of claim 1, wherein the system is further configured to apply an additional transformation within at least one of: the prediction network before the fit layer, at the output, and within the conditional network before the conditional layer.\n3. The system of claim 1, wherein the one or more paths include at least one of: one or more activation paths, a switch output selection path, an aggregation path, and a transformation path.\n4. The system of claim 3, further comprising a ranking layer configured to identify a score for each of the one or more activation paths wherein the activation path(s) comprise one of a hierarchical and a flat structure.\n5. The system of claim 3, wherein each partition is explainable and interpretable, such that each local model formed from the partitions is explainable and interpretable, and the global model formed from the combination of local models is explainable and interpretable, and wherein an explanation is formed with the output in a single feed forward step.\n6. The system of claim 1, wherein the feature generation and transformation layer is configured to apply at least one of a linear transformation and a non-linear transformation, wherein the transformation functions comprise one or more of polynomial expansions, rotations, dimensional scaling, dimensionless scaling, Fourier transforms, integer/real/complex/quaternion/octonion transforms, Walsh functions, state-space transforms, phase-space transforms, Haar and non-Haar wavelets, generalized L2 functions, fractal-based transforms, Hadamard transforms, Type 1 fuzzy logic, Type 2 fuzzy logic, modal transforms, probabilistic transforms, quantum/qubit transforms, knowledge graph networks, categorical encoding, difference analysis, normalization, standardization, scaling, multi-dimensional Bezier curves, recurrence relations, causal operators, gradient descent based transformations, and subsets of an explainable neural network.\n7. The system of claim 1, wherein the input is received on the conditional network and prediction network simultaneously.\n8. The system of claim 1, wherein the partitions are hierarchically structured.\n9. The system of claim 1, wherein the transformation layer is further configured to perform a plurality of transformations in a transformation pipeline.\n10. The system of claim 9, wherein the transformation pipeline is further configured to perform transformations that analyze one or more temporally ordered data sequences according to the value of one or more variables.\n11. The system of claim 1, further comprising at least one of a selection, ranking, split, and merge layer implemented at least one of before, in, and after the conditional network.\n12. The system of claim 1, wherein the partitions are non-overlapping.\n13. The system of claim 1, wherein the prediction network fits one or more local models, wherein each local model is linked to a specific partition.\n14. The system of claim 1, further comprising a loss function applied to minimize or completely eliminate an overlap between partitions and to minimize an error between a prediction and a labeled output.\n15. The system of claim 1, wherein each partition in the one or more partitions forms exactly one local model and a combination of more than one of the one or more partitions is a global model.\n16. The system of claim 1, wherein the value output layer is configured to present at least one of a predicted value and a classification label based on the input features, and wherein the value output layer further includes an activation function.\n17. The system of claim 1, wherein the system is further configured to form the partitions, based on the input, by comparing features from the input to a set of localization values and conditions, wherein the localization values and conditions are identified using an internal and/or external process.\n18. The system of claim 17, wherein the internal and/or external process is a gradient descent method.\n19. The system of claim 1, wherein human knowledge is embedded into the network.\n20. The system of claim 1, wherein the system is further configured to apply one or more gradient descent methods to implement human knowledge and machine-generated knowledge into the partitions.\n21. The system of claim 1, wherein the input data is an output from another neural network.\n22. The system of claim 1, wherein the output is machine readable and is read by a subsequent neural network.\n23. The system of claim 1, wherein one or more of the partitions overlap with one another, and wherein the system is configured to identify at least one of: a probability or ranking score for each partition, two or more partitions to merge, and one or more partitions to split.\n24. The system of claim 1, wherein one or more of the partitions overlap with one another, and wherein the system further comprises a ranking function configured to rank the partitions and select the highest ranking partition when more than one overlapping partition is identified.\n25. The system of claim 1, wherein the coefficients and/or partitions are formed based on at least one of human input, taxonomy information, and ontology information.\n26. The system of claim 1, wherein the transformation transforms the prediction output using to be structured as one of: (i) hierarchical tree or network, (ii) causal diagrams, (iii) directed and undirected graphs, (iv) multimedia structures, and (v) sets of hyperlinked graphs.\n27. A computer implemented method for providing an explainable neural network, comprising executing on a processor the steps of:\ninputting a set of data into an input layer;\npartitioning the input based on one or more input features identified in the input, creating one or more partitions;\naggregating one or more of the partitions;\napplying one or more transformation functions to the partitioned input features, providing a set of transformed features;\ncombining the transformed features and identifying one or more coefficients corresponding to the transformed features;\ncompiling one or more equations based on the coefficients and the transformed features;\ncompiling one or more rules based on the equation and the partitions;\napplying the equations and rules to the set of input features to obtain an output value;\noutputting the output value and generating an explanation of the output value, wherein the explanation accounts for at least one of the coefficients;\nwherein each of the one or more rules is an if-then rule;\nwherein each of the one or more partitions forms at least one local model and a combination of local models forms a global model; and\nwherein one or more paths throughout the partitions are identifiable by an external process.\n28. The computer implemented method of claim 27, further comprising convoluting the set of data prior to inputting the set of data into the input layer.\n29. The computer implemented method of claim 27, further comprising customizing one or more of the partitions based on human input.",
    "status": "Active",
    "citations_own": [
        "US4979126A",
        "WO1997014113A2",
        "US7480640B1",
        "US7542960B2",
        "US10210452B2",
        "US20190147369A1",
        "US20190156216A1",
        "US10311368B2",
        "US20190197357A1",
        "US20190370647A1",
        "US20200073788A1",
        "US20200152330A1",
        "US20200167677A1",
        "US20200279140A1",
        "US20200349466A1"
    ],
    "citations_ftf": [],
    "citedby_own": [],
    "citedby_ftf": [
        "EP3671660A1",
        "KR102512552B1",
        "US11755345B2",
        "CN113724110A",
        "CN114611842B",
        "CN114925190B"
    ]
}