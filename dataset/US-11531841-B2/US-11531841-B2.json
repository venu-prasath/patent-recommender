{
    "patent_id": "US-11531841-B2",
    "title": "Machine learning model training method and apparatus, server, and storage medium ",
    "assignee": "Tencent Technology (Shenzhen) Company Limited",
    "publication_date": "2022-12-20",
    "patent_link": "https://patents.google.com/patent/US11531841B2/en",
    "inventors": [
        "Wei Zhao",
        "Yabing FENG",
        "Yu Liao",
        "Junbin LAI",
        "Haixia CHAI",
        "Xuanliang PAN",
        "Lichun LIU"
    ],
    "classifications": [
        "G06N20/20",
        "G06F18/214",
        "G06K9/6257",
        "G06F18/2148",
        "G06F18/217",
        "G06F18/231",
        "G06F18/24323",
        "G06F18/254",
        "G06K9/6262",
        "G06K9/6292",
        "G06N20/00",
        "G06N20/10",
        "G06Q10/0639"
    ],
    "abstract": "A machine learning model training method includes: training a machine learning model using features of each sample in a training set based on an initial first weight and an initial second weight. In one iteration, the method includes determining a first sample set in which a target variable is incorrectly predicted, and a second sample set in which a target variable is correctly predicted, based on a predicted loss of each sample; and determining overall predicted loss of the first sample set based on a predicted loss and a first weight of each sample in the first sample set. The method also includes updating the first weight and a second weight of each sample in the first sample set based on the overall predicted loss; and inputting the updated second weight, the features, and the target variable of each sample to the machine learning model, and initiating a next iteration.",
    "claims": "\n1. A machine learning model training method, comprising:\ntraining, by a computing device, a machine learning model using features of each sample in a training set based on an initial first weight of each sample and an initial second weight of each sample;\nin one iteration of training the machine learning model,\ndetermining, by the computing device, a first sample set comprising a sample whose corresponding target variable is incorrectly predicted, and a second sample set comprising a sample whose corresponding target variable is correctly predicted, based on a predicted loss of each sample in the training set;\ndetermining, by the computing device, an overall predicted loss of the first sample set based on the predicted loss and a corresponding first weight of each sample in the first sample set;\nupdating, by the computing device, the first weight and a second weight of each sample in the first sample set based on the overall predicted loss of the first sample set; and\ninputting, by the computing device, the updated second weight of each sample in the training set, the features of each sample in the training set, and the target variable of each sample in the training set to the machine learning model, and initiating a next iteration of training the machine learning model.\n2. The method according to claim 1, wherein the training a machine learning model using features of each sample in a training set based on an initial first weight and an initial second weight of each sample comprises:\ninitializing the first weight and the second weight of each sample in the training set to obtain the initial first weight of each sample and the initial second weight of each sample;\ninputting, to the machine learning model, the second weight of each sample in the training set, the features of each sample in the training set, and the target variable of each sample in the training set; and\nallocating a thread to samples having a same feature in the machine learning model, and training the machine learning model using parallel threads.\n3. The method according to claim 2, wherein the initializing the first weight and the second weight of each sample in the training set comprises:\nuniformly allocating the initial first weight to each sample in the training set, and uniformly allocating the initial second weight to each sample in the training set based on a quantity of samples in the training set, wherein the initial second weight is different from the initial first weight.\n4. The method according to claim 1, further comprising:\nafter training the machine learning model in one iteration,\ndetermining, according to a gradient direction of a loss function of the machine learning model, a compensation function that causes the predicted loss to converge based on the gradient direction; and\nsuperimposing, on the machine learning model, the compensation function to compensate for the predicted loss.\n5. The method according to claim 1, further comprising:\nbased on a difference between a predicted value of the target variable and an actual value of the target variable of a sample in the first sample set, determining that the predicted loss of the sample in the first sample set is an output value of a loss function that uses the difference as a dependent variable.\n6. The method according to claim 1, wherein the determining a first sample set comprising a sample whose corresponding target variable is incorrectly predicted, and a second sample set comprising a sample whose corresponding target variable is correctly predicted, based on a predicted loss of each sample in the training set comprises:\nin the training set, determining that a set of samples whose predicted losses exceed a loss threshold is the first sample set, and determining that a set of samples whose predicted losses do not exceed the loss threshold is the second sample set.\n7. The method according to claim 1, wherein the updating the first weight and a second weight of each sample in the first sample set based on the overall predicted loss of the first sample set comprises:\nconstructing a weight update factor by using a product of the overall predicted loss of the first sample set and the first weight; and\ndecreasing the first weight of each sample in the second sample set, and increasing the second weight of each sample in the first sample set based on the weight update factor.\n8. The method according to claim 7, further comprising:\nperforming normalization processing on the first weight of each sample in the training set to obtain a normalization processing result, and updating the first weight of each sample in the training set based on the normalization processing result.\n9. The method according to claim 1, further comprising:\ndetermining a fusion coefficient of a classifier comprised in the machine learning model, by minimizing a quadratic sum of predicted losses of samples in the first sample set; and\ncombining classifiers to form the trained machine learning model, based on fusion coefficients of the classifiers.\n10. The method according to claim 1, further comprising:\niteratively updating the first sample set and the second sample set, and iteratively updating the first weight and the second weight of the first sample set; and\ntraining the machine learning model based on the updated first sample set and the updated second weight, until a quantity of iterations is satisfied, or the overall predicted loss of the first sample set is less than a pre-determined value.\n11. A machine learning model training apparatus, comprising:\na memory; and\none or more processors configured to:\ntrain a machine learning model using features of each sample in a training set based on an initial first weight of each sample and an initial second weight of each sample;\nin one iteration of training the machine learning model,\ndetermine a first sample set comprising a sample whose corresponding target variable is incorrectly predicted, and a second sample set comprising a sample whose corresponding target variable is correctly predicted, based on a predicted loss of each sample in the training set;\ndetermine an overall predicted loss of the first sample set based on the predicted loss and a corresponding first weight of each sample in the first sample set;\nupdate the first weight and a second weight of each sample in the first sample set based on the overall predicted loss of the first sample set; and\ninput the updated second weight of each sample in the training set, the features of each sample in the training set, and the target variable of each sample in the training set to the machine learning model, and initiate a next iteration of training the machine learning model.\n12. The apparatus according to claim 11, wherein the one or more processors are further configured to:\ninitialize the first weight and the second weight of each sample in the training set to obtain the initial first weight of each sample and the initial second weight of each sample;\ninput the second weight of each sample in the training set, the features of each sample in the training set, and the target variable of each sample in the training set to the machine learning model; and\nallocate a thread to samples having a same feature in the machine learning model, and train the machine learning model using parallel threads.\n13. The apparatus according to claim 12, wherein the one or more processors are further configured to:\nuniformly allocate the initial first weight to each sample in the training set, and uniformly allocate the initial second weight to each sample in the training set based on a quantity of samples in the training set, wherein the initial second weight is different from the initial first weight.\n14. The apparatus according to claim 11, wherein the one or more processors are further configured to:\nin one iteration when the machine learning model is trained,\ndetermine, according to a gradient direction of a loss function of the machine learning model, a compensation function that causes the predicted loss to converge based on the gradient direction; and\nsuperimpose, on the machine learning model, the compensation function to compensate for the predicted loss.\n15. The apparatus according to claim 11, wherein the one or more processors are further configured to:\nbased on a difference between a predicted value of the target variable and an actual value of the target variable of a sample in the first sample set, determine that a predicted loss of the sample is an output value of a loss function that uses the difference as a dependent variable.\n16. The apparatus according to claim 11, wherein the one or more processors are further configured to:\nin the training set, determine that a set of samples whose predicted losses exceed a loss threshold is the first sample set, and determine that a set of samples whose predicted losses do not exceed the loss threshold is the second sample set.\n17. The apparatus according to claim 11, wherein the one or more processors are further configured to:\nconstruct a weight update factor by using a product of the overall predicted loss of the first sample set and the first weight; and\ndecrease the first weight of each sample in the second sample set, and increase the second weight of each sample in the first sample set based on the weight update factor.\n18. The apparatus according to claim 11, wherein the one or more processors are further configured to:\ndetermine a fusion coefficient of a classifier comprised in the machine learning model, by minimizing a quadratic sum of predicted losses of the samples in the first sample set; and\ncombine classifiers to form the trained machine learning model, based on fusion coefficients of the classifiers.\n19. The apparatus according to claim 11, wherein the one or more processors are further configured to:\nupdate the first sample set and the second sample set, and update the first weight the second weight of the first sample set,\ntrain the machine learning model based on the updated first sample set and the updated second weight, until a quantity of iterations is satisfied, or the overall predicted loss of the first sample set is less than a pre-determined value.\n20. A non-transitory storage medium, storing an executable program, when being executed by a processor, the executable program causes the processor to perform:\ntraining a machine learning model using features of each sample in a training set based on an initial first weight of each sample and an initial second weight of each sample;\nin one iteration of training the machine learning model,\ndetermining a first sample set comprising a sample whose corresponding target variable is incorrectly predicted, and a second sample set comprising a sample whose corresponding target variable is correctly predicted, based on a predicted loss of each sample in the training set;\ndetermining an overall predicted loss of the first sample set based on the predicted loss and a corresponding first weight of each sample in the first sample set;\nupdating the first weight and a second weight of each sample in the first sample set based on the overall predicted loss of the first sample set; and\ninputting the updated second weight of each sample in the training set, the features of each sample in the training set, and the target variable of each sample in the training set to the machine learning model, and initiating a next iteration of training the machine learning model.",
    "status": "Active",
    "citations_own": [
        "CN101571998A",
        "EP2164025A1",
        "CN102637143A",
        "US20130132315A1",
        "US8909564B1",
        "CN104346221A",
        "US20150094983A1",
        "CN104573013A",
        "US9053391B2",
        "US9141622B1",
        "CN105320957A",
        "CN105844300A",
        "CN106548210A",
        "US11200514B1"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US20230031156A1"
    ],
    "citedby_ftf": [
        "US10127240B2",
        "US11416751B2",
        "GB201705189D0",
        "WO2018187948A1",
        "CN107124365B",
        "CN107169513B",
        "CN107766929B",
        "CN108320026B",
        "CN109034175B",
        "EP3627806A4",
        "CN107507613B",
        "CN107563201B",
        "CN110021373A",
        "KR102563752B1",
        "CN109754105A",
        "CN108021940B",
        "CN109903071A",
        "CN108038508A",
        "CN109977977B",
        "CN108108821B",
        "CN108257593B",
        "CN108199951A",
        "CN109919317A",
        "CN108256052B",
        "CN108446170B",
        "CN108446817B",
        "CN108647373A",
        "CN108538389B",
        "CN108846340B",
        "CN110728289B",
        "CN109272332B",
        "US11151165B2",
        "CN111357014B",
        "US20210192390A1",
        "CN111046891A",
        "CN109472296A",
        "US10576380B1",
        "TW202018727A",
        "CN109901881B",
        "US10839318B2",
        "CN111353001B",
        "CN110046642B",
        "CN110046259A",
        "JP7276757B2",
        "CN110084271B",
        "CN110033098A",
        "CN111985651A",
        "US10956597B2",
        "CN110263294B",
        "CN110222339B",
        "CN110276113A",
        "CN110348581B",
        "CN110322342B",
        "CN111417124A",
        "CN112149833A",
        "CN110490632A",
        "CN110309203B",
        "JP7114528B2",
        "CN110505144A",
        "CN110458725A",
        "CN110851321B",
        "CN111045716B",
        "CN111126628B",
        "CN110879921B",
        "CN110995382A",
        "CN110942144B",
        "CN111050266B",
        "CN111092769A",
        "CN110912627B",
        "CN111198938A",
        "CN111178623B",
        "CN111178443B",
        "CN111159169B",
        "CN111275288A",
        "US20210248503A1",
        "CN111311000B",
        "CN111291867A",
        "CN111275133B",
        "CN111476403A",
        "CN111428783B",
        "CN111460966A",
        "EP3893057A1",
        "CN111489037B",
        "US11586917B2",
        "CN111553542B",
        "CN111612072A",
        "CN111680973B",
        "CN111639463B",
        "CN111693938A",
        "CN111709089B",
        "CN111985681A",
        "CN112748941A",
        "CN112818344A",
        "CN111950644A",
        "CN112052900A",
        "CN112153636A",
        "US11720962B2",
        "CN112989906A",
        "CN112700131B",
        "CN113139475A",
        "CN112819085B",
        "CN113159275A",
        "CN112966968B",
        "CN112735535B",
        "CN113469241B",
        "CN113343391A",
        "CN113435653B",
        "CN113671919B",
        "CN113904801A",
        "US20230094635A1",
        "CN114513612B",
        "CN114092162B",
        "CN114565252B",
        "CN114916913B",
        "CN114743081B",
        "CN114611634B",
        "CN114842837B",
        "CN115358367B",
        "CN115618962B",
        "CN115701866B",
        "CN116092683B",
        "CN116470618A"
    ]
}