{
    "patent_link": "https://patents.google.com/patent/US20050088969A1/en",
    "patent_id": "US20050088969A1",
    "title": "Port congestion notification in a switch",
    "abstract": "A congestion notification mechanism provides a congestion status for all destinations in a switch at each ingress port. Data is stored in a memory subsystem queue associated with the destination port at the ingress side of the crossbar. A cell credit manager tracks the amount of data in this memory subsystem for each destination. If the count for any destination exceeds a threshold, the credit manager sends an XOFF signal to the XOFF masks. A lookup table in the XOFF masks maintains the status for every switch destination based on the XOFF signals. An XON history register receives the XOFF signals to allow queuing procedures that do not allow a status change to XON during certain states. Flow control signals directly from the memory subsystem are allowed to flow to each XOFF mask, where they are combined with the lookup table status to provide a congestion status for every destination.",
    "inventors": [
        "Scott Carlsen",
        "Anthony Tornetta",
        "Steven Schmidt"
    ],
    "assignee": "McData Services Corp",
    "classifications": [
        "H04L49/3009",
        "H04L49/30",
        "H04L2012/5683"
    ],
    "claims": "\n1. A method for congestion notification within a switch comprising:\na) maintaining a plurality of lookup tables having multiple entries, each entry containing a congestion status for a different destination in the switch;\nb) sending a congestion update to the plurality of lookup tables, the congestion update containing a destination identifier and an updated congestion status; and\nc) updating the entry in the lookup table corresponding to the destination identifier using the updated congestion status.\n2. The method of claim 1, wherein each lookup table contains an entry for all available destinations in the switch.\n3. The method of claim 2, wherein a separate lookup table is maintained at each ingress to the switch.\n4. The method of claim 1, wherein the switch is a Fibre Channel switch.\n5. The method of claim 1, further comprising:\nd) maintaining an indicator of an amount of data within a buffer for each destination; and\ne) triggering the sending of the congestion update when the indicator passes a threshold value.\n6. The method of claim 5, wherein a credit module maintains the indicators and sends the congestion updates.\n7. The method of claim 6, wherein the credit module uses a single indicator for each destination to track data entering the switch from a plurality of ingress ports.\n8. The method of claim 7, wherein\ni) data from each ingress ports passes through a fabric module before entering the buffer,\nii) each fabric module submits a first credit event to the credit module for each grouping of data submitted to the buffer, and\niii) the credit module uses the first credit event to alter the indicator so as to reflect additional data entering the buffer.\n9. The method of claim 8, wherein\niv) the buffer informs the fabric module each time a grouping of data leaves the buffer,\nv) the fabric module responds to such information from the buffer by submitting a second credit event to the credit module, and\nvi) the credit module uses the second credit event to alter the indicator so as to reflect data leaving the buffer.\n10. The method of claim 9, wherein the first credit event is a decrement event decreasing a value of the indicator, the second credit event is an increment event increasing the value the indicator.\n11. The method of claim 9, wherein a plurality of fabric modules submit first and second credit events to the credit module, which stores the credit events in a plurality of FIFOs.\n12. The method of claim 11, wherein the credit events are retrieved from the FIFOs and applied to the indicator.\n13. The method of claim 8, wherein each lookup table responds to a switch destination address by returning the congestion status for the destination associated with the switch destination address.\n14. The method of claim 13, wherein the congestion status returned by the lookup table is combined with a congestion indicator generated by the fabric module to return a final congestion status for the switch destination address.\n15. The method of claim 14, wherein a first fabric module shares the congestion indicator with a second fabric module within the switch, with the second fabric module submitting the congestion indicator to at least one additional lookup table.\n16. The method of claim 7, wherein each congestion update from the credit module is sent to the lookup tables used by the plurality of ingress ports for which the credit module maintains the indicators.\n17. The method of claim 16, wherein the credit module is a master credit module, further comprising a plurality of slave credit modules each serving a different subset of ingress ports on the switch.\n18. The method of claim 17, wherein each slave credit module receives information on the data entering the buffer from its own subset of ingress ports and forwards that information to the master credit module.\n19. The method of claim 18, wherein the master credit module uses the information received from the slave credit modules to maintain the indicators, and furthermore wherein the master credit module directs the slave credit modules to submit congestion updates to their subset of served ports.\n20. The method of claim 5, wherein different threshold values are maintained for different destinations, and further wherein the grouping of data is a fixed-sized data cell.\n21. The method of claim 1, wherein each lookup table responds to a switch destination address by returning the congestion status for the destination associated with the switch destination address.\n22. The method of claim 21, wherein the congestion status returned by the lookup table is combined with a congestion indicator to return a final congestion status.\n23. A method for congestion notification within a switch comprising:\na) maintaining at each ingress port a lookup table having multiple entries, each entry containing a congestion status for a different destination in the switch, each lookup table containing entries for all available destinations in the switch, each lookup table returning the congestion status in response to a status query for a particular destination;\nb) maintaining at a first module an indicator of an amount of data submitted for each destination; and\nc) when the indicator passes a threshold value, sending a congestion update from the first module to a first lookup table, the congestion update containing a destination identifier and an updated congestion status; and\nd) updating the entry in the first lookup table corresponding to the destination identifier using the updated congestion status.\n24. The method of claim 23, wherein the first module services a plurality of ports and their associated lookup tables, with all data passing through the serviced ports being reflected in the indicators of the first module.\n25. The method of claim 24, wherein data from each serviced port passes through a separate second module, each second module submitting credit events to the first module reflecting data being submitted to and exiting a memory subsystem.\n26. The method of claim 25, wherein cell credit events are stored by the first module in FIFOs to be later applied to the indicators for each destination.\n27. The method of claim 25, wherein the congestion status returned by the lookup table is combined with a congestion signal generated by the second module to return a final congestion status.\n28. The method of claim 27, wherein the congestion signal is in response to an XOFF/XON signal from the memory subsystem.\n29. The method of claim 23, wherein different threshold values are maintained for different destinations.\n30. A method for sharing congestion information in a switch comprising:\na) interfacing with an ingress memory subsystem for a crossbar component through a plurality of fabric interfaces, the crossbar component handling data in predefined units;\nb) associating a set of fabric interfaces to a credit module;\nc) transmitting a first data event from one of the fabric interfaces to the credit module when a unit of data for a destination is submitted to the ingress memory subsystem, the first data event identifying the destination;\nd) transmitting a second data event from one of the fabric interfaces to the credit module when the ingress memory subsystem informs the fabric interface that a unit of data has been submitted to the crossbar from the ingress memory subsystem;\ne) using the first and second data events at the credit module to track a congestion status for the destinations in the switch.\n31. The method of claim 30, further comprising:\nf) sending a congestion event from the credit module to a plurality of ingress ports to indicate a change in the congestion status for a destination;\n32. The method of claim 31, further comprising:\ng) upon receiving a flow control signal from the ingress memory subsystem, sending a congestion signal from one of the fabric interfaces to one of the ingress ports.\n33. The method of claim 32, further comprising:\nh) sending the congestion signal from the one of the fabric interfaces to a second fabric interface, and then sending the congestion signal from the second fabric interface to a second ingress port.\n34. A method for distributing information regarding port congestion on a switch having a switch fabric and a plurality of I/O boards, each board having a plurality of ports, the method comprising\na) submitting incoming data on a first I/O board to the switch fabric via a single ingress memory subsystem;\nb) organizing the ingress memory subsystem so as to establish a separate queue for each destination on the switch;\nc) monitoring an amount of data in each queue in the ingress memory subsystem;\nd) submitting a congestion event to each port on the first I/O board when the amount of data in a first queue passes a threshold value; and\ne) maintaining at each port a destination lookup table containing a congestion value for each destination on the switch based upon the congestion events.\n35. The method of claim 34, wherein each I/O board has a plurality of protocol devices servicing a plurality of ports, and further wherein a credit module on each protocol device performs the monitoring step based on the amount of data in each queue that originated from ports on its protocol device, wherein the credit module submits the congestion event to each port on its protocol device.\n36. The method of claim 34, wherein each I/O board has a plurality of protocol devices each servicing a plurality of ports, and further wherein slave credit modules on at least some of the protocol devices submit information to a master credit module concerning the data entering each queue that originated from ports on its protocol device, wherein the master credit module instructs the slave credit modules to submit the congestion event to each port that it services.\n37. The method of claim 36, wherein\ni) all data passes through a fabric interface before being submitted to the ingress memory subsystem,\nii) multiple fabric interfaces exist on each protocol device,\niii) the fabric interfaces receive congestion signals from the ingress memory subsystem, and\niv) the fabric interfaces submit a fabric congestion signal to at least one port after receiving congestion signals from the ingress memory subsystem.\n38. The method of claim 37, wherein the fabric interfaces indicate to each other when fabric congestion signals are created.\n39. The method of claim 37, wherein the fabric interfaces track the data entering and leaving the ingress memory subsystem and all the fabric interfaces on a single protocol device report this information to a single credit module.\n40. A data communication switch having a plurality of destinations comprising:\na) a crossbar component; and\nb) a plurality of I/O boards, each I/O board having\ni) a memory subsystem for queuing data for submission to the crossbar component,\nii) a credit component for tracking an amount of data within the memory subsystem for each destination, and\niii) a plurality of protocol devices, each protocol devices having\n(1) a plurality of ports,\n(2) a port congestion indicator at each port, the port congestion indicator having an indication of a congestion status for each destination in the switch, and\n(3) a congestion communication link connecting the credit component with each of the port congestion indicators.\n41. The switch of claim 40, wherein each destination in the switch has a switch destination address and further wherein the credit component contains a decrement FIFO containing switch destination addresses and an increment FIFO containing switch destination addresses.\n42. The switch of claim 41, wherein a first switch destination address for a first destination is added to the decrement FIFO when a unit of data for the first destination is submitted to the memory subsystem and further wherein the first switch destination address for the first destination is added to the increment FIFO when the unit of data for the first destination exits the memory subsystem.\n43. The switch of claim 42, wherein each port communicates to the memory subsystem through a fabric interface module, and further wherein the fabric interface modules submit the switch destination addresses to the FIFOs.\n44. The switch of claim 43, wherein the fabric interface modules receive flow control signals from the memory subsystem.\n45. The switch of claim 44, wherein each fabric interface module send congestion signals to an associated port congestion indicator upon receipt of the flow control signals.\n46. The switch of claim 45, wherein each fabric interface module sends a congestion signal to the other fabric interface modules on its I/O board upon receipt of the flow control signals.\n47. The switch of claim 40, wherein the credit component submits a congestion event to at least one of the port congestion indicators when an amount of data in the memory subsystem for a first destination crosses a threshold.\n48. The switch of claim 47, wherein the credit component is a master credit component, and further comprising a plurality of slave credit components, wherein when the master credit component submits the congestion event to the at least one port congestion indicators, the master credit component also submits an instruction to the slave credit components to submit the congestion event to other port congestion indicators.\n49. The switch of claim 40, wherein each port communicates to the memory subsystem through a fabric interface module, and further wherein the fabric interface modules communicate to the credit components events related to data entering and leaving the memory subsystem.\n50. The switch of claim 40, wherein the port congestion indicator is an XOFF mask lookup table.\n51. A data communication switch having a plurality of destinations comprising:\na) a crossbar component; and\nb) at least one I/O board having\ni) an output queuing means for queuing data for submission to the crossbar component,\nii) a plurality of ports,\niii) a congestion indicator means at each port for indicating a congestion status for each destination in the switch,\niv) a congestion signaling means for signaling a need to update the congestion indicator means with a new congestion status for at least one port.\n52. The switch of claim 51, further comprising a means for sharing the congestion signaling means with all ports on an I/O board.\n53. The switch of claim 51, wherein the destinations include the ports and at least one microprocessor.",
    "status": "Abandoned",
    "citations_own": [
        "US4710868A",
        "US5455820A",
        "US5533201A",
        "US5751969A",
        "US5781549A",
        "US5844887A",
        "US5974467A",
        "US5983260A",
        "US5999527A",
        "US6067286A",
        "US6160813A",
        "US6335992B1",
        "US6370145B1",
        "US6421348B1",
        "US20020156918A1",
        "US20020176363A1",
        "US20030016686A1",
        "US20030026267A1",
        "US20030202474A1",
        "US6643256B1",
        "US20040017771A1",
        "US20040024906A1",
        "US20040081096A1",
        "US6937607B2",
        "US6967924B1",
        "US6992980B2"
    ],
    "citations_ftf": [
        "FR2625392B1",
        "JP3269273B2",
        "GB9509484D0",
        "US6442172B1",
        "JPH1032585A",
        "JP3156623B2",
        "KR100247022B1",
        "US6094435A",
        "US6091707A",
        "US6078959A",
        "JP3001502B2",
        "IL125271A0",
        "US6473827B2",
        "US7120117B1",
        "US6952401B1",
        "US6625121B1",
        "US6904043B1",
        "WO2001067672A2",
        "US7046632B2",
        "TW477133B",
        "US6987732B2",
        "US7042842B2",
        "US7260104B2",
        "US6804245B2",
        "US6606322B2",
        "US7423967B2",
        "US20060098660A1"
    ],
    "citedby_own": [
        "US20050015518A1",
        "US20050013258A1",
        "US20050013318A1",
        "US20050018650A1",
        "US20050018701A1",
        "US20050018663A1",
        "US20050018603A1",
        "US20050018676A1",
        "US20050018649A1",
        "US20050018671A1",
        "US20050018675A1",
        "US20050018621A1",
        "US20050018680A1",
        "US20050018606A1",
        "US20050018674A1",
        "US20050018672A1",
        "US20050025193A1",
        "US20050030978A1",
        "US20050030954A1",
        "US20050030893A1",
        "US20050044267A1",
        "US20050135251A1",
        "US20050174942A1",
        "US20050174936A1",
        "US20060013135A1",
        "US20060023705A1",
        "US20060072473A1",
        "US20060072616A1",
        "US20060072580A1",
        "US20060087963A1",
        "US20060092932A1",
        "US20060153186A1",
        "US7184466B1",
        "US20070047535A1",
        "US20070064605A1",
        "US20070081527A1",
        "US20070153816A1",
        "US20070268825A1",
        "US20080168161A1",
        "US20080168302A1",
        "US20080253289A1",
        "US20080270638A1",
        "US20080316942A1",
        "US20090041057A1",
        "US20090043880A1",
        "US7577133B1",
        "US7593330B1",
        "US20090252167A1",
        "US20100008375A1",
        "US7684401B2",
        "US7729288B1",
        "US7876711B2",
        "US7894348B2",
        "US7930377B2",
        "US8711697B1",
        "US9088497B1",
        "WO2016105419A1",
        "WO2016109104A1",
        "CN106301967A",
        "US9621484B2",
        "US9832143B2",
        "US9838330B2",
        "US9838338B2",
        "US20190190982A1",
        "US20190190853A1",
        "US10848426B2",
        "US11088966B2",
        "US11134021B2",
        "US11165720B2",
        "US11750504B2"
    ],
    "citedby_ftf": [
        "US7260104B2",
        "US7277448B1",
        "US20050089054A1",
        "US7539143B2",
        "KR100612442B1",
        "US20050281282A1",
        "US7916743B2",
        "US7136954B2",
        "US20060174050A1",
        "US7493426B2",
        "US7548513B2",
        "WO2007138250A2",
        "US8068429B2",
        "US8135025B2",
        "US8644140B2",
        "US8699491B2",
        "WO2013022428A1",
        "WO2013022427A1",
        "US9013997B2",
        "US9582440B2",
        "US8989011B2",
        "CA2819539C",
        "US9641465B1",
        "US9548960B2",
        "US10601713B1",
        "CN104679667B",
        "US9372500B2",
        "US9325641B2",
        "US9584429B2",
        "US9946819B2",
        "US10229230B2",
        "US9342388B1",
        "US10951549B2",
        "US11558316B2"
    ]
}