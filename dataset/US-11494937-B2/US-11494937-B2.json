{
    "patent_id": "US-11494937-B2",
    "title": "Multi-task multi-sensor fusion for three-dimensional object detection ",
    "assignee": "Uatc, Llc",
    "publication_date": "2022-11-08",
    "patent_link": "https://patents.google.com/patent/US11494937B2/en",
    "inventors": [
        "Raquel Urtasun",
        "Bin Yang",
        "Ming Liang"
    ],
    "classifications": [
        "G06N3/084",
        "G01S17/86",
        "G01S17/89",
        "G01S7/4802",
        "G05D1/0088",
        "G05D1/0238",
        "G06F18/253",
        "G06N20/00",
        "G06N20/20",
        "G06N3/045",
        "G06N3/082",
        "G06T11/60",
        "G06T7/246",
        "G06T7/55",
        "G06T7/75",
        "G06V10/806",
        "G06V10/82",
        "G06V20/58",
        "G06V30/19173",
        "G06V30/2504",
        "G01S17/931",
        "G05D2201/0213",
        "G06N20/10",
        "G06N3/044",
        "G06N5/01",
        "G06N5/04",
        "G06N7/01",
        "G06T2207/10024",
        "G06T2207/10028",
        "G06T2207/20081",
        "G06T2207/20084",
        "G06T2207/20221",
        "G06T2207/30252"
    ],
    "abstract": "Provided are systems and methods that perform multi-task and/or multi-sensor fusion for three-dimensional object detection in furtherance of, for example, autonomous vehicle perception and control. In particular, according to one aspect of the present disclosure, example systems and methods described herein exploit simultaneous training of a machine-learned model ensemble relative to multiple related tasks to learn to perform more accurate multi-sensor 3D object detection. For example, the present disclosure provides an end-to-end learnable architecture with multiple machine-learned models that interoperate to reason about 2D and/or 3D object detection as well as one or more auxiliary tasks. According to another aspect of the present disclosure, example systems and methods described herein can perform multi-sensor fusion (e.g., fusing features derived from image data, light detection and ranging (LIDAR) data, and/or other sensor modalities) at both the point-wise and region of interest (ROI)-wise level, resulting in fully fused feature representations.",
    "claims": "\n1. A computing system configured to perform multi-task multi-sensor fusion for detecting objects, the computing system comprising:\none or more processors; and\none or more non-transitory computer-readable media that collectively store:\na machine-learned light detection and ranging (LIDAR) backbone model configured to receive a bird's eye view (BEV) representation of a LIDAR point cloud generated for an environment surrounding an autonomous vehicle and to process the BEV representation of the LIDAR point cloud to generate a LIDAR feature map;\na machine-learned image backbone model configured to receive an image of the environment surrounding the autonomous vehicle and to process the image to generate an image feature map;\na machine-learned refinement model configured to receive respective region of interest (ROI) feature crops from each of the LIDAR feature map and the image feature map, to perform ROI-wise fusion to fuse respective pairs of ROI feature crops to generate fused ROI feature crops, and to generate one or more object detections based on the fused ROI feature crops, wherein each of the one or more object detections indicates a location of a detected object within the environment surrounding the autonomous vehicle; and\na machine-learned depth completion model configured to receive the image feature map generated by the machine-learned image backbone model and to produce a depth completion map;\nwherein the machine-learned LIDAR backbone model, the machine-learned image backbone model, the machine-learned refinement model, and the machine-learned depth completion model have all been jointly trained end-to-end based on a total loss function that evaluates training object detections output by the machine-learned refinement model and training depth completion maps output by the machine-learned depth completion model during training.\n2. The computer system of claim 1, wherein one or more non-transitory computer-readable media collectively store instructions that, when executed by the one or more processors, cause the computer system to perform operations, the operations comprising:\ndetermining a motion plan for the autonomous vehicle based at least in part on the one or more object detections, wherein the motion plan describes a path of motion for the autonomous vehicle through the environment surrounding the autonomous vehicle; and\ncontrolling the autonomous vehicle according to the motion plan.\n3. The computer system of claim 1, wherein the machine-learned LIDAR backbone model comprises one or more point-wise fusion layers that are configured to:\nreceive image feature data from one or more intermediate layers of the machine-learned image backbone model; and\nperform point-wise fusion to fuse the image feature data with one or more intermediate LIDAR feature maps generated by one or more intermediate layers of the machine-learned LIDAR backbone model.\n4. The computer system of claim 3, wherein at least one of the one or more point-wise fusion layers is configured to use depth points included in the depth completion map to establish correspondence between the image feature data and the one or more intermediate LIDAR feature maps.\n5. The computer system of claim 1, wherein:\nthe one or more object detections comprise one or more two-dimensional object detections and one or more three-dimensional object detections; and\nthe total loss function on which each of the machine-learned LIDAR backbone model, the machine-learned image backbone model, the machine-learned refinement model, and the machine-learned depth completion model has been jointly trained evaluates two-dimensional training object detections and three-dimensional training object detections output by the machine-learned refinement model during training.\n6. The computer system of claim 1, wherein the one or more non-transitory computer-readable media collectively store instructions that, when executed by the one or more processors, cause the computer system to perform operations, the operations comprising:\nprojecting the LIDAR point cloud to the image to generate a sparse depth image;\nconcatenating the sparse depth image with the image; and\ninputting the image concatenated with the sparse depth image into the machine-learned image backbone model.\n7. The computer system of claim 1, wherein:\nthe one or more non-transitory computer-readable media collectively store a machine-learned mapping model configured to receive the LIDAR point cloud and to process the LIDAR point cloud to generate a ground geometry prediction that describes locations of a ground of the environment within the LIDAR point cloud; and\nthe machine-learned mapping model, the machine-learned LIDAR backbone model, the machine-learned image backbone model, the machine-learned refinement model, and the machine-learned depth completion model have all been jointly trained end-to-end based on the total loss function.\n8. The computer system of claim 1, wherein the one or more object detections comprise one or more three-dimensional object detections.\n9. An autonomous vehicle, comprising:\nat least one camera configured to capture an image of an environment surrounding the autonomous vehicle;\na light detection and ranging (LIDAR) system configured to generate a LIDAR point cloud for the environment surrounding the autonomous vehicle; and\na computing system comprising:\none or more processors; and\none or more non-transitory computer-readable media that collectively store:\na machine-learned mapping model configured to receive a light detection and ranging (LIDAR) point cloud and to process the LIDAR point cloud to generate a ground geometry prediction that describes locations of a ground of the environment within the LIDAR point cloud;\na machine-learned LIDAR backbone model configured to receive a bird's eye view (BEV) representation of the LIDAR point cloud and to process the BEV representation of the LIDAR point cloud to generate a LIDAR feature map, the BEV representation of the LIDAR point cloud generated based at least in part on the ground geometry prediction output by the machine-learned mapping model;\na machine-learned image backbone model configured to receive the image and to process the image to generate an image feature map;\na machine-learned refinement model configured to receive respective region of interest (ROI) feature crops from each of the LIDAR feature map and the image feature map, to perform ROI-wise fusion to fuse respective pairs of ROI feature crops to generate fused ROI feature crops, and to generate one or more object detections based on the fused ROI feature crops, wherein each of the one or more object detections indicates a location of a detected object within the environment surrounding the autonomous vehicle; and\nwherein the machine-learned LIDAR backbone model, the machine-learned image backbone model, the machine-learned refinement model, and the machine-learned mapping model have all been jointly trained end-to-end based on a total loss function that evaluates the one or more object detections.\n10. The autonomous vehicle of claim 9, wherein the one or more non-transitory computer-readable media collectively store instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising:\ndetermining a motion plan for the autonomous vehicle based at least in part on the one or more object detections, wherein the motion plan describes a path of motion for the autonomous vehicle through the environment surrounding the autonomous vehicle; and\ncontrolling the autonomous vehicle according to the motion plan.\n11. The autonomous vehicle of claim 9, wherein:\nthe one or more object detections comprise one or more three-dimensional object detections;\nthe ground geometry prediction provides a ground height value for each voxel of a raw BEV representation of the LIDAR point cloud; and\nthe one or more non-transitory computer-readable media collectively store instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising:\nsubtracting, from a height axis value of each point in the LIDAR point cloud, the respective ground height value associated with the respective voxel in which such point resides to produce the BEV representation of the LIDAR point cloud; and\nadding to a height axis value of each of the one or more three-dimensional object detections, the respective ground height values associated with the respective voxel in which such three-dimensional object detection resides.\n12. The autonomous vehicle of claim 9, wherein the machine-learned LIDAR backbone model comprises one or more point-wise fusion layers that are configured to:\nreceive image feature data from one or more intermediate layers of the machine-learned image backbone model; and\nperform point-wise fusion to fuse the image feature data with one or more intermediate LIDAR feature maps generated by one or more intermediate layers of the machine-learned LIDAR backbone model.\n13. The autonomous vehicle of claim 9, wherein:\nthe one or more non-transitory computer-readable media collectively store a machine-learned depth completion model configured to receive the image feature map generated by the machine-learned image backbone model and to produce a depth completion map; and\nthe machine-learned mapping model, the machine-learned LIDAR backbone model, the machine-learned image backbone model, the machine-learned refinement model, and the machine-learned depth completion model have all been jointly trained end-to-end based on the total loss function.\n14. An object detection system with both point-wise and region of interest (ROI)-wise feature fusion, the object detection system comprising:\none or more processors; and\none or more non-transitory computer-readable media that collectively store:\na machine-learned image backbone model configured to receive an image of an environment surrounding an autonomous vehicle and to process the image to generate an image feature map;\na machine-learned light detection and ranging (LIDAR) backbone model configured to receive a bird's eye view (BEV) representation of a LIDAR point cloud generated for the environment surrounding the autonomous vehicle and to process the BEV representation of the LIDAR point cloud to generate a LIDAR feature map, wherein the machine-learned LIDAR backbone model comprises one or more point-wise fusion layers that are configured to:\nreceive image feature data from one or more intermediate layers of the machine-learned image backbone model; and\nperform point-wise fusion to fuse the image feature data with one or more intermediate LIDAR feature maps generated by one or more intermediate layers of the machine-learned LIDAR backbone model;\na machine-learned refinement model configured to receive respective ROI feature crops from each of the LIDAR feature map and the image feature map, to perform ROI-wise fusion to fuse respective pairs of ROI feature crops to generate fused ROI feature crops, and to generate one or more three-dimensional object detections based on the fused ROI feature crops, wherein each of the one or more three-dimensional object detections indicates a location of a detected object within the environment surrounding the autonomous vehicle.\n15. The object detection system of claim 14, wherein the machine-learned image backbone model, the machine-learned LIDAR backbone model, and the machine-learned refinement model have all been jointly trained end-to-end based on a loss function that evaluates the one or more three-dimensional object detections.\n16. The object detection system of claim 14, wherein:\nthe one or more non-transitory computer-readable media collectively store a machine-learned mapping model configured to receive the LIDAR point cloud and to process the LIDAR point cloud to generate a ground geometry prediction that describes locations of a ground of the environment within the LIDAR point cloud; and\nthe machine-learned mapping model, the machine-learned LIDAR backbone model, the machine-learned image backbone model, and the machine-learned refinement model have all been jointly trained end-to-end based on a total loss function.\n17. The object detection system of claim 14, wherein:\nthe one or more non-transitory computer-readable media collectively store a machine-learned depth completion model configured to receive the image feature map generated by the machine-learned image backbone model and to produce a depth completion map; and\nthe machine-learned LIDAR backbone model, the machine-learned image backbone model, the machine-learned refinement model, and the machine-learned depth completion model have all been jointly trained end-to-end based on a total loss function.\n18. The object detection system of claim 17, wherein at least one of the one or more point-wise fusion layers is configured to use depth points included in the depth completion map to establish correspondence between the image feature data and the one or more intermediate LIDAR feature maps.\n19. The object detection system of claim 14, wherein to fuse respective pairs of ROI feature crops together, the machine-learned refinement model is configured to:\nassign at least one of ROI feature crops to one of two orientation anchors of 0 and 90 degrees; and\nwhen a ROI feature crop is rotated, re-parameterize a regression target of relative offsets with respect to the orientation anchor.\n20. The object detection system of claim 14, wherein the object detection system is installed within an autonomous vehicle.",
    "status": "Active",
    "citations_own": [
        "US20180141562A1",
        "US20190243372A1",
        "US20190272446A1"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US20210319261A1"
    ],
    "citedby_ftf": [
        "US10678244B2",
        "US10671349B2",
        "US11409692B2",
        "US11157441B2",
        "US11561791B2",
        "CN111133447A",
        "US10997433B2",
        "US11537139B2",
        "US11215999B2",
        "US11636333B2",
        "US11562231B2",
        "US11155259B2",
        "US11196678B2",
        "US10824913B1",
        "US11537811B2",
        "IL270540A",
        "US11610117B2",
        "CN113454636A",
        "WO2020140049A1",
        "US11170299B2",
        "EP3749976A4",
        "US10817777B2",
        "US10997461B2",
        "US11567514B2",
        "US10956755B2",
        "CN113811886A",
        "US11713978B2",
        "KR20210027894A",
        "US11726492B2",
        "US11586931B2",
        "US11733353B2",
        "US11120280B2",
        "TWI759651B",
        "US11195033B2",
        "US11704579B2",
        "US11328517B2",
        "CN111666852A",
        "ES2882834A1",
        "US20210405638A1",
        "US20210406679A1",
        "CN111626419A",
        "US20220026917A1",
        "DE102020119743A1",
        "CN111860425B",
        "CN112053374A",
        "CN112001914A",
        "AU2021337678A1",
        "US11623661B2",
        "CN112183485A",
        "US11657522B2",
        "US11775615B2",
        "US11430218B2",
        "CN112861653A",
        "CN113052187B",
        "CN113011357B",
        "CN113159151B",
        "CN113160330B",
        "CN113205495B",
        "US20220366176A1",
        "CN113378693B",
        "JP2022188643A",
        "CN113408456A",
        "CN113850284B",
        "CN113393457B",
        "DE102021208192A1",
        "CN113569803A",
        "KR20230031036A",
        "US20230084623A1",
        "US20230091924A1",
        "CN114475650B",
        "CN114119671B",
        "CN114295139A",
        "US20230294687A1",
        "WO2023158642A1",
        "CN116964599A",
        "CN114842287B",
        "US11403860B1",
        "CN114463825B",
        "CN115171006B",
        "CN114819109B",
        "CN114840327B",
        "CN115661395B",
        "CN116503418B",
        "CN116778292A"
    ]
}