{
    "patent_id": "US-10848621-B1",
    "title": "Learning based metric determination for service sessions ",
    "assignee": "United Services Automobile Association (Usaa)",
    "publication_date": "2020-11-24",
    "patent_link": "https://patents.google.com/patent/US10848621B1/en",
    "inventors": [
        "Vijay Jayapalan",
        "Gregory Yarbrough",
        "Bipin Chadha",
        "John McChesney TenEyck, Jr.",
        "Eric J. Smith"
    ],
    "classifications": [
        "G06Q30/01",
        "G06N20/00",
        "G06N3/044",
        "G06N3/0445",
        "G06N3/045",
        "G06N3/088",
        "G06N5/04",
        "G06N5/041",
        "H04M3/523",
        "G06N3/006",
        "H04M2203/301",
        "H04M2203/401",
        "H04M2203/403",
        "H04M2203/408",
        "H04M3/42221",
        "H04M3/5175"
    ],
    "abstract": "Techniques are described for generating metric(s) that predict survey score(s) for a service session. Model(s) may be trained, through supervised or unsupervised machine learning, using training data from previous service sessions between service representative(s) and individual(s). Training data may include, for previous service session(s), a session record (e.g., audio record) of the session and a set of survey scores provided by the serviced individual to rate the session on one or more criteria (e.g., survey questions). The model(s) may be trained to output, based on an input session record, metric(s) that each correspond to a survey score that would have been provided by the individual had they completed the survey. The model may be a concatenated model that is a combination of a language model output from a language classifier recurrent neural network, and an acoustic model output from an acoustic feature layer convolutional neural network.",
    "claims": "\n1. A computer-implemented method, comprising:\nreceiving session data representing one or more audio recordings;\ngenerating transcription data by transcribing at least a portion of the one or more audio recordings;\ngenerating spectrogram data by analyzing the acoustic properties of at least a portion of the one or more audio recordings;\ngenerating, based at least partly on the transcription data, a computer-processable language model; and\ngenerating, based at least partly on the spectrogram data, a computer-processable acoustic model.\n2. The computer-implemented method of claim 1, wherein the one or more audio recordings comprise one or more audio recordings representing a session record of communications between a service representative (SR) and an individual during a service session.\n3. The computer-implemented method of claim 1, wherein generating the computer-processable language model comprises transmitting the transcription data to a recurrent neural network.\n4. The computer-implemented method of claim 3, wherein the recurrent neural network comprises a language classifier recurrent neural network that outputs the language model.\n5. The computer-implemented method of claim 1, wherein generating the computer-processable acoustic model comprises transmitting the spectrogram data to a convolutional neural network.\n6. The computer-implemented method of claim 5, wherein the convolutional neural network comprises a language classifier convolutional neural network that outputs the acoustic model, the language classifier convolutional neural network comprising one or more layers, each layer of the one or more layers configured to model a particular acoustic feature or set of acoustic features.\n7. The computer-implemented method of claim 1, further comprising combining the language model and the acoustic model to generate a concatenated model.\n8. The computer-implemented method of claim 7, further comprising applying the concatenated model to second session data representing a second one or more audio recordings to generate predicted survey results.\n9. The computer-implemented method of claim 1, wherein the session data further represents one or more surveys, each of the one or more surveys corresponding to an audio recording of the one or more audio recordings.\n10. One or more non-transitory computer-readable media storing instructions which, when executed by at least one processor, cause the at least one processor to perform operations comprising:\nreceiving session data representing one or more audio recordings;\ngenerating transcription data by transcribing at least a portion of the one or more audio recordings;\ngenerating spectrogram data by analyzing the acoustic properties of at least a portion of the one or more audio recordings;\ngenerating, based at least partly on the transcription data, a computer-processable language model; and\ngenerating, based at least partly on the spectrogram data, a computer-processable acoustic model.\n11. The computer-implemented method of claim 10, wherein the one or more audio recordings comprise one or more audio recordings representing a session record of communications between a service representative (SR) and an individual during a service session.\n12. The computer-implemented method of claim 10, wherein generating the computer-processable language model comprises transmitting the transcription data to a recurrent neural network.\n13. The computer-implemented method of claim 12, wherein the recurrent neural network comprises a language classifier recurrent neural network that outputs the language model.\n14. The computer-implemented method of claim 10, wherein generating the computer-processable acoustic model comprises transmitting the spectrogram data to a convolutional neural network.\n15. The computer-implemented method of claim 14, wherein the convolutional neural network comprises a language classifier convolutional neural network that outputs the acoustic model, the language classifier convolutional neural network comprising one or more layers, each layer of the one or more layers configured to model a particular acoustic feature or set of acoustic features.\n16. The computer-implemented method of claim 10, the operations further comprising combining the language model and the acoustic model to generate a concatenated model.\n17. The computer-implemented method of claim 16, the operations further comprising applying the concatenated model to second session data representing a second one or more audio recordings to generate predicted survey results.\n18. The computer-implemented method of claim 10, wherein the session data further represents one or more surveys, each of the one or more surveys corresponding to an audio recording of the one or more audio recordings.\n19. A system, comprising:\nat least one processor; and\na memory communicatively coupled to the at least one processor, the memory storing instructions which, when executed by the at least one processor, cause the at least one processor to perform operations comprising:\nreceiving session data representing one or more audio recordings;\ngenerating transcription data by transcribing at least a portion of the one or more audio recordings;\ngenerating spectrogram data by analyzing the acoustic properties of at least a portion of the one or more audio recordings;\ngenerating, based at least partly on the transcription data, a computer-processable language model; and\ngenerating, based at least partly on the spectrogram data, a computer-processable acoustic model.\n20. The system of claim 19, the operations further comprising:\ncombining the language model and the acoustic model to generate a concatenated model; and\napplying the concatenated model to second session data representing a second one or more audio recordings to generate predicted survey results.",
    "status": "Active",
    "citations_own": [
        "US20060262922A1",
        "US20080167952A1",
        "US20090089135A1",
        "US20100138282A1",
        "US20100332287A1",
        "US20110137696A1",
        "US20110251871A1",
        "US20120047000A1",
        "US20120101865A1",
        "US8615074B1",
        "US20140143018A1",
        "US20140249873A1",
        "US8855292B1",
        "US20140316862A1",
        "US20150134404A1",
        "US20150170295A1",
        "US20150178371A1",
        "US20150189088A1",
        "US20160078142A1",
        "US20160086125A1",
        "US20160189558A1",
        "US20160352900A1",
        "US20170024640A1",
        "US9728185B2",
        "US20170256254A1",
        "US20170316438A1",
        "US9881615B2",
        "US20180061439A1",
        "US10019438B2",
        "US20180225602A1",
        "US20180270354A1",
        "US20180358005A1"
    ],
    "citations_ftf": [
        "US6801520B2",
        "US20120297395A1",
        "US20160180381A1",
        "US20180330278A1"
    ],
    "citedby_own": [],
    "citedby_ftf": [
        "CN106887225B",
        "US11544719B1",
        "US11323564B2",
        "US10715665B1",
        "US11604979B2",
        "US11075862B2",
        "CN111507455B",
        "US20210005207A1",
        "US10656923B1",
        "US11328205B2",
        "US20210117882A1",
        "US20210136220A1",
        "US11736615B2",
        "US10735530B1",
        "US11367089B2",
        "US11768945B2",
        "WO2021220285A1",
        "CN113836275B",
        "US11677875B2",
        "US20230111621A1",
        "US11615469B1",
        "US11736616B1"
    ]
}