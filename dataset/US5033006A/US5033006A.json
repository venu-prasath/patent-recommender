{
    "patent_link": "https://patents.google.com/patent/US5033006A/en",
    "patent_id": "US5033006A",
    "title": "Self-extending neural-network",
    "abstract": "A self-extending shape neural-network is capable of a self-extending operation in accordance with the studying results. The self-extending shape neural-network has initially minimum number of the intermediate layers and the number of the nodes (units) within each layer by the self-extension of the network construction so as to shorten the studying time and the discriminating time. This studying may be effected efficiently by the studying being directed towards the focus when the studying is not focused.",
    "inventors": [
        "Yasushi Ishizuka",
        "Fumio Togawa",
        "Toru Ueda"
    ],
    "assignee": "Sharp Corp",
    "classifications": [
        "G06N3/082",
        "G06F18/24137"
    ],
    "claims": "\n1. A self-extending neural-network having a multilayer neural-network composed of at least an input layer, an intermediate layer, and an output layer, which, during a studying operation, obtains inputted studying data and outputs output data according to a value of a coupling weight between nodes in the multilayer neural-network, comprising:\nstudy progress judging portion means for judging whether or not the studying operation is progressing in accordance with the output data and the value of the coupling weight between the nodes and for outputting an extending instruction signal when the studying operation has been judged not to be progressing; and\nself-extending portion means, responsive to said extending instruction signal, for providing a new node in accordance with said extending instruction signal from said study progress judging portion means;\nsaid self-extending portion means setting a condition of a coupling between the nodes and said new node and an initial value of a coupling weight between the nodes and said new node so as to self-extend construction of the self-extending neural-network, thereby continuing the studying operation when the construction of the self-extending neural-network has been self-extended by said self-extending portion means.\n2. The self-extending shape neural-network as claimed in claim 1, wherein the self-extending neural-network is a multilayer neural-network composed of an input layer, an intermediate layer and an output layer, and said new node is provided within said intermediate layer when said self-extending portion means self-extends the construction of the self-extending neural-network.\n3. The self-extending shape neural-network as claimed in claim 2, wherein the self-extending neural-network is a multilayer neural-network composed of an input layer, an intermediate layer and an output layer; and\nsaid self-extending portion means self-extending the construction of the self-extending neural-network by adding a new intermediate layer to said intermediate layer, said new intermediate layer being composed of said new node.\n4. The self-extending shape neural-network as claimed in claim 2, wherein the self-extending neural-network is a multilayer neural-network composed of an input layer, an intermediate layer and an output layer; and\nsaid self-extending portion means self-extending the construction of the self-extending neural-network by adding a new output layer as an upper layer of said output layer, said output layer being provided as a new intermediate layer, thereby adding to said intermediate layer.\n5. The self-extending shape neural-network as claimed in claim 1, wherein the self-extending neural-network is a multilayer neural-network composed of an input layer, an intermediate layer and an output layer; and\nsaid self-extending portion means self-extending the construction of the self-extending neural-network by adding a new intermediate layer to said intermediate layer, said new intermediate layer being composed of said new node.\n6. The self-extending shape neural-network as claimed in claim 5, wherein the self-extending neural-network is a multilayer neural-network composed of an input layer, an intermediate layer and an output layer; and\nsaid self-extending portion means self-extending the construction of the self-extending neural-network by adding a new output layer as an upper layer of said output layer, said output layer being provided as a new intermediate layer, thereby adding to said intermediate layer.\n7. The self-extending shape neural-network as claimed in claim 1, wherein the self-extending neural-network is a multilayer neural-network composed of an input layer, an intermediate layer and an output layer; and\nsaid self-extending portion means self-extending the construction of the self-extending neural-network by adding a new output layer as an upper layer of said output layer, said output layer being provided as a new intermediate layer, thereby adding to said intermediate layer.\n8. A self-extending neural-network comprising:\ninput portion means for receiving a studying sample;\ncalculating portion means, operatively connected to said input portion means, for calculating an output value by performing a study operation using a given algorithm upon said studying sample and a value of a coupling weight among respective nodes in said calculating portion means;\noutput portion means, operatively connected to said calculating portion means, for externally outputting said output value;\nstudy progress judging portion means, operatively connected to said output portion means, for determining whether said study operation is progressing in accordance with said output value and said value of said coupling weight between said nodes; and\nsaid study progress judging portion means outputting an extend instruction signal when said study operation is not progressing;\nself-extending portion means, responsive to said extend instruction signal, for providing a new node;\nsaid self-extending portion means establishing a condition of coupling between said nodes and said new node and establishing an initial value for a coupling weight between said nodes and said new node, thereby enabling further study operations after self-extension.\n9. The self-extending neural-network as claimed in claim 8 further comprising:\nfocusing judging portion means, operatively connected to said output portion means, for determining whether said study operation is focused to a result; and\nsaid focusing judging portion means outputting a focus signal when said study operation is not focused;\nstudy control means, operatively connected to said focus judging portion means and said calculating portion means, for controlling said calculating portion means to renew said value of said coupling weight between said nodes and recalculate said output value when said focus signal is received.\n10. The self-extending shape neural-network as claimed in claim 9, wherein the self-extending neural-network is a multilayer neural-network composed of an input layer, an intermediate layer, and an output layer; and\nsaid new node is provided within said intermediate layer when said self-extending portion means self-extends construction of the self-extending neural-network.\n11. The self-extending shape neural-network as claimed in claim 9, wherein the self-extending neural-network is a multilayer neural-network composed of an input layer, an intermediate layer, and an output layer; and\nsaid self-extending portion means self-extending construction of the self-extending neural-network by adding a new intermediate layer to said intermediate layer, said new intermediate layer, being composed of said new node.\n12. The self-extending shape neural-network as claimed in claim 9, wherein the self-extending neural-network is a multilayer neural-network composed of an input layer, an intermediate layer, and an output layer; and\nsaid self-extending portion means self-extending construction of the self-extending neural-network by adding a new output layer as an upper layer of said output layer, said output layer upstream of said upper layer being provided as a new intermediate layer, thereby adding an additional intermediate layer to said intermediate layer.\n13. The self-extending shape neural-network as claimed in claim 8, wherein the self-extending neural-network is a multilayer neural-network composed of an input layer, an intermediate layer, and an output layer; and\nsaid new node is provided within said intermediate layer when said self-extending portion means self-extends construction of the self-extending neural-network.\n14. The self-extending shape neural-network as claimed in claim 8, wherein the self-extending neural-network is a multilayer neural-network composed of an input layer, an intermediate layer, and an output layer; and\nsaid self-extending portion means self-extending construction of the self-extending neural-network by adding a new intermediate layer to said intermediate layer, said new intermediate layer being composed of said new node.\n15. The self-extending shape neural-network as claimed in claim 8, wherein the self-extending neural-network is a multilayer neural-network composed of an input layer, an intermediate layer, and an output layer; and\nsaid self-extending portion means self-extending construction of the self-extending neural-network by adding a new output layer as an upper layer of said output layer, said output layer upstream of said upper layer being provided as a new intermediate layer, thereby adding an additional intermediate layer to said intermediate layer.\n16. A method for self-extending a neural-network comprising the steps of:\n(a) receiving a studying sample\n(b) calculating an output value by performing a study operation using a given algorithm upon the studying sample and a value of a coupling weight among respect nodes;\n(c) externally outputting the output value;\n(d) determining whether the study operation is progressing in accordance with the output value and the value of the coupling weight between the nodes;\n(e) outputting an extend instruction signal when said step (d) has determined that the study operation is not progressing;\n(f) providing a new node in response to the extend instruction signal;\n(g) establishing a condition of coupling between the nodes and the new node; and\n(h) establishing an initial value for a coupling weight between the nodes and the new node, thereby enabling further study operations after self-extension.\n17. The method as claimed in claim 16 further comprising the steps of:\n(i) determining whether the study operation is focused to a result;\n(j) outputting a focus signal when said step (i) determines that the study operation is not focused;\n(k) renewing the value of the coupling weight between the nodes when the focus signal is produced; and\n(l) recalculating the output value with the renewed value when the focus signal is produced.\n18. The method as claimed in claim 16, wherein said step (f) provides the new node within an intermediate layer, the intermediate layer performing the calculations of said step (b).",
    "status": "Expired - Lifetime",
    "citations_own": [
        "US4803736A",
        "US4874963A",
        "US4912655A",
        "US4914708A",
        "US4933872A"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "WO1992017849A1",
        "US5195169A",
        "US5214747A",
        "US5239594A",
        "US5239619A",
        "US5265224A",
        "US5293458A",
        "US5308915A",
        "US5313559A",
        "US5317675A",
        "EP0599347A2",
        "US5357597A",
        "US5371809A",
        "US5408588A",
        "US5428559A",
        "US5430829A",
        "US5438629A",
        "US5438645A",
        "US5452399A",
        "US5479575A",
        "DE4430628A1",
        "US5522015A",
        "US5533169A",
        "US5546503A",
        "US5586223A",
        "US5592589A",
        "US5649067A",
        "US5684929A",
        "US5689622A",
        "US5720003A",
        "US5719955A",
        "US5742740A",
        "US5796923A",
        "US5852817A",
        "US6052679A",
        "EP1283496A2",
        "US20030200189A1",
        "US20030200191A1",
        "US20040019574A1",
        "US6745169B1",
        "US20080281767A1",
        "EP2472443A3",
        "US20130212052A1",
        "EP3026600A3",
        "CN108985386A",
        "US10634081B2",
        "US10741184B2",
        "US10832138B2",
        "US10853727B2"
    ],
    "citedby_ftf": [
        "JP2533942B2",
        "GB8929146D0",
        "JP6784162B2",
        "JP7107797B2",
        "EP3518153A1",
        "KR102219904B1",
        "JP6735862B1"
    ]
}