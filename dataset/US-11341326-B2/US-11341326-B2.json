{
    "patent_id": "US-11341326-B2",
    "title": "Compression method and platform of pre-training language model based on knowledge distillation ",
    "assignee": "Zhejiang Lab",
    "publication_date": "2022-05-24",
    "patent_link": "https://patents.google.com/patent/US11341326B2/en",
    "inventors": [
        "Hongsheng Wang",
        "Haijun Shan",
        "Fei Yang"
    ],
    "classifications": [
        "G06F40/211",
        "G06F40/20",
        "G06F18/24",
        "G06F40/30",
        "G06N20/00",
        "G06N20/20",
        "G06N3/045",
        "G06N3/08",
        "G06N5/02",
        "G06N5/022",
        "G06N7/005"
    ],
    "abstract": "Provided is a method and a platform for compressing a pre-training language model based on knowledge distillation. According to the method, a universal knowledge distillation strategy of feature migration is firstly designed, and in the process of knowledge distillation from the teacher model to the student model, the feature mapping of each layer of the student model is approaching the teacher's features, focusing on the ability of small samples to express features in the intermediate layer of the teacher model, and guiding the student model by using these features; then, a knowledge distillation method based on self-attention cross is constructed; finally, a linear transfer strategy based on Bernoulli probability distribution is designed to gradually complete the knowledge transfer of feature mapping and self-attention distribution from teachers to students.",
    "claims": "\n1. A method for compressing a pre-training language model based on knowledge distillation, wherein the method compresses a BERT model, which comprises a feature mapping knowledge distillation module, a self-attention cross knowledge distillation module and a linear learning module based on Bernoulli probability distribution; wherein, an original BERT model is a teacher model and a compressed BERT model is a student model; the feature mapping knowledge distillation module is based on a knowledge distillation strategy based on feature transfer; in a process of knowledge distillation from the teacher model to the student model, feature mapping of each layer of the student model is approaching feature mapping of the teacher model, and the student model focuses on intermediate layer features of the teacher model and uses the intermediate layer features to guide the student model; the self-attention cross knowledge distillation module realizes deep mutual learning between the teacher model and the student model by cross-connecting self-attention units of the teacher model and the student model by way of convex combination cross-connection on a network self-attention layer; the linear learning module based on Bernoulli probability distribution gradually completes feature mapping from the teacher model to the student model and knowledge transfer from self-attentive distribution;\nthe self-attention cross knowledge distillation module comprises the following three stages:\na first stage: the self-attention unit of a teacher network is input into a student network, and a transfer objective function is minimized; specifically, the self-attention unit of the teacher network is regarded as a basic truth value, the student network is input at the self-attention unit of the network, and the student network receives a correct supervision signal to train a subsequent layer, avoiding a phenomenon of an excessive estimation error and propagation thereof;\na second stage: the self-attention unit of the student network is input into the teacher network and the transfer objective function is minimized; because the estimation error propagates layer by layer on the student network, there is a difference between an input of the student network and an input of the teacher network on a same layer; the self-attention unit of the student network is input to the teacher network, which makes the student network imitate an output behavior of the teacher network on the premise of a same input;\na third stage: migration objective functions of the first stage and the second stage on the self-attention unit of the network are convexly combined to realize the distillation strategy of cross migration;\nthe linear learning module based on Bernoulli probability distribution is configured to set different linear migration probabilities for driving the feature mapping knowledge distillation module and the self-attention cross knowledge distillation module, and comprises the following two steps:\nstep 1, the feature mapping knowledge distillation module and the self-attention cross knowledge distillation module both adopt the migration probability of Bernoulli probability distribution, that is, assuming that an ith module is currently migrated, a random variable X is sampled through a Bernoulli distribution, and X is 0 or 1; the random variable being 1 represents that transfer learning is performed for a current module, and the random variable not being 1 represents that transfer learning is not performed;\nstep 2: although setting a constant migration probability p in step 1 can meet the needs of a compression model, a linear learning-driven migration probability is helpful to gradually migrate an encoder module in the model; this step designs a linear learning-driven migration probability plinear to dynamically adjust the migration probability p in step 1, namely\n\np linear=min(1,k*i+b)\nwhere, plinear represents the migration probability of the current migration module, migration of the ith module corresponds to an ith step of a current training, and b represents an initial migration probability without training; k is a dynamic value greater than 0, and when the training is increased to 1000 steps, 5000 steps, 10000 steps and 30000 steps, plinear is gradually increased to 0.25, 0.5, 0.75 and 1.00 in turn.\n2. The method for compressing a pre-training language model based on knowledge distillation according to claim 1, wherein interlayer normalization is added in the feature mapping knowledge distillation module to stabilize interlayer training loss; when training the student network, two statistical differences of a mean and a variance in feature map transformation is minimized.\n3. The method for compressing a pre-training language model based on knowledge distillation according to claim 1, wherein the migration objective function of the self-attention cross knowledge distillation module is to minimize a relative entropy between attention distribution of the student model and the teacher model.\n4. The method for compressing a pre-training language model based on knowledge distillation according to claim 1, wherein the initial transition probability b ranges from 0.1 to 0.3.\n5. A platform for the method for compressing a pre-training language model based on knowledge distillation according to claim 1, wherein the platform comprises the following components:\na data loading component which is configured to obtain a multi-task oriented BERT model and a training sample thereof; wherein the training samples is a labeled text sample satisfying supervision of a learning task;\na compression component which configured to compress a multitask-oriented large-scale language model, comprising a teacher model fine-tuning module, a teacher-student model distillation module and a student model fine-tuning module; wherein, the teacher model fine-tuning module is responsible for loading the BERT model, inputting the training sample into the BERT model containing downstream tasks for fine-tuning and outputting a teacher model; by utilizing the teacher model obtained by the teacher model fine-tuning module, the teacher-student model distillation module gradually completes the feature mapping from teachers to students and knowledge distillation of self-attention distribution and updates weight parameters of each unit module of a student network through the feature mapping knowledge distillation module, the self-attention cross knowledge distillation module and the linear learning module based on Bernoulli probability distribution; the student model fine-tuning module recombines all encoder unit modules of the student network into a complete encoder, and uses a feature layer and an output layer of the teacher network to fine-tune a downstream task scene, and outputs the fine-tuned student model as a final compression model; and\na reasoning component which is configured for reasoning the downstream tasks of natural language processing on a data set of an actual scene by utilizing the compression model output by the compression component.\n6. The platform according to claim 5, wherein the compression component outputs the compression model to a designated container for users to download, and presents comparison information of model sizes before and after compression; the reasoning component uses the compression model to reason the downstream tasks of natural language processing, and presents the comparative information of reasoning speed before and after compression.",
    "status": "Active",
    "citations_own": [
        "US20180365564A1",
        "CN110097178A",
        "CN110232203A",
        "US10575788B2",
        "CN110880036A",
        "US20200104642A1",
        "CN111062489A",
        "CN111461226A",
        "CN111767711A",
        "US20210224660A1",
        "US20210335002A1",
        "US20210383238A1",
        "US11210467B1"
    ],
    "citations_ftf": [
        "CN111767110B"
    ],
    "citedby_own": [
        "US20230120965A1"
    ],
    "citedby_ftf": [
        "CN111767711B",
        "JP7283836B2",
        "CN112418291A",
        "JP2023519770A",
        "CN112232511B",
        "CN112613273B",
        "JP7283835B2",
        "CN112241455B",
        "CN112613559B",
        "CN112365385B",
        "CN113159168B",
        "CN113177415A",
        "US20220350968A1",
        "US11763082B2",
        "CN113592007B",
        "CN113849641B",
        "CN113887610A",
        "CN114461871B",
        "CN114004315A",
        "CN114708467B",
        "CN114580571B",
        "CN115309849A",
        "CN115019183B",
        "CN115457006B",
        "CN115272981A",
        "CN115423540B",
        "CN116110022B",
        "CN115797976B",
        "CN116340779A",
        "CN116415005B",
        "CN116542321B",
        "CN116776744B"
    ]
}