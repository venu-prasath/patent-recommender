{
    "patent_link": "https://patents.google.com/patent/US5033087A/en",
    "patent_id": "US5033087A",
    "title": "Method and apparatus for the automatic determination of phonological rules as for a continuous speech recognition system",
    "abstract": "A continuous speech recognition system includes an automatic phonological rules generator which determines variations in the pronunciation of phonemes based on the context in which they occur. This phonological rules generator associates sequences of labels derived from vocalizations of a training text with respective phonemes inferred from the training text. These sequences are then annotated with their pheneme context from the training text and clustered into groups representing similar pronunciations of each phoneme. A decision tree is generated using the context information of the sequences to predict the clusters to which the sequences belong. The training data is processed by the decision tree to divide the sequences into leaf-groups representing similar pronunciations of each phoneme. The sequences in each leaf-group are clustered into sub-groups representing respectively different pronunciations of their corresponding phoneme in a give context. A Markov model is generated for each sub-group. The various Markov models of a leaf-group are combined into a single compound model by assigning common initial and final states to each model. The compound Markov models are used by a speech recognition system to analyze an unknown sequence of labels given its context.",
    "inventors": [
        "Lalit R. Bahl",
        "Peter F. Brown",
        "Peter V. deSouza",
        "Robert L. Mercer"
    ],
    "assignee": "International Business Machines Corp",
    "classifications": [],
    "claims": "\n1. A method for automatically separating vocalizations of language components into a plurality of groups representing pronunciations of the language components in respectively different contexts, said method comprising the steps of:\nA) processing a training text and vocalizations representing the training text to obtain a plurality of samples representing the language components of said vocalizations;\nB) selecting, from among the plurality of samples, a set of samples representing respective instances of a selected language component in the vocalizations;\nC) annotating each of said selected samples with a context indicator, representing at least one language component in a contextual relationship with the selected sample, to produce annotated samples;\nD) separating the selected samples into respectively different leaf groups based on the respective context indicators of said annotated samples, each of said leaf groups representing a pronunciation of said selected language component in a respectively different context.\n2. The method of automatically separating vocalizations of language components set forth in claim 1 wherein:\nStep C) further includes the steps of:\nC1) grouping said annotated selected samples into a plurality of clusters, each cluster representing a respectively different pronunciation of said selected language component; and\nC2) further annotating each selected sample with an indicator of the cluster to which it belongs.\n3. The method for automatically separating vocalizations of language components set forth in claim 2 wherein:\nsaid language components are phonemes;\nsaid samples are sequences of fenemes having distinct types, each of said sequences of fenemes corresponding to a respective one of said phonemes; and\nstep C1) includes the steps of:\nC1a) assigning each sequence of fenemes to a respectively different prototype cluster;\nC1b) calculating respective expected frequency values for each type of feneme in each of the prototype clusters;\nC1c) statistically comparing the expected frequency values for the respective types of fenemes in each prototype cluster to the expected frequency values for the respective types of fenemes in all other prototype clusters to generate a plurality of statistical difference values, one for each pair of prototype clusters;\nC1d) combining pairs of prototype clusters that exhibit a statistical difference value which is less than a threshold value to generate new prototype clusters;\nC1e) repeating steps C1b through C1d until no pair of prototype clusters exhibits a statistical difference value which is less than the threshold value; wherein each cluster corresponds to a respectively different one of the clusters of step C1.\n4. The method for automatically separating vocalizations of language components set forth in claim 3 wherein the step C1c includes the steps of:\ngenerating a plurality of probabilistic models, each model representing a Markov model of the pronunciation of the phoneme represented by a respectively different one of said prototype clusters;\ngenerating a plurality of histograms, each histogram representing the relative frequency of occurrence of each feneme in a respectively different one of said prototype clusters;\ncalculating, for each pair of prototype clusters, a log-likelihood ratio that the histograms for each prototype cluster in said pair match expected frequencies of the respective types of fenemes in a single probabilistic model representing a combination of the respective probabilistic models for said pair of prototype clusters, wherein a sign inverted version of said log-likelihood ratio is the statistical difference value for said pair of clusters.\n5. The method for automatically separating vocalizations of language components set forth in claim 1, further comprising the step of generating, for each leaf group of said decision tree, a probabilistic model representing the pronunciation of he language component represented by the samples of said leaf group.\n6. The method for automatically separating vocalizations of language components set forth in claim 5, wherein each of the samples is classified as to type and said probabilistic model has the form of a Markov model representing respective relative frequencies of occurrence of the samples of each type in the leaf group.\n7. The method for automatically separating vocalizations of language components set forth in claim 5 wherein the step of generating a probabilistic model for each leaf group of the decision tree includes the steps of:\ngrouping the samples of the leaf group into a plurality of clusters, each cluster representing a respectively different pronunciation of the language component represented by the samples of the leaf group;\ngenerating, from said plurality of clusters, a respective plurality of statistical models each of said statistical models having the form of a Markov model; and\naugmenting said plurality of statistical models by adding a common initial state and a common final state to each model to generate said probabilistic model.\n8. In an automatic speech recognition system, a method for associating vocalizations of a continuously spoken sequence of words with respective language components, comprising the steps of:\ngenerating a sampled data signal representing the vocalizations of said continuously spoken sequence of words;\nA) associating a first language component with a first set of samples of said sampled data signal;\nB) associating a second set of samples of said sampled data signal with a second language component;\nC) accessing a decision means with the first language component as a context indicator to define a probabilistic model to be used to relate the second set of samples to the second language component, the probabilistic model defined by said decision means representing a distinct pronunciation of a selected language component in terms of a context provided for the selected language component;\nD) calculating, from said defined probabilistic model, a likelihood that the second language component corresponds to the second set of samples;\nE) repeating steps A) through D) for a plurality of second language components; and\nF) associating the one of the second language component and the plurality of second language components having the greatest likelihood with said second set of samples.\n9. The method set forth in claim 8 wherein:\nstep B) includes the step of calculating the likelihood that the second language component corresponds to the second set of samples, as defined by a context-independent probabilistic model representing the pronunciation of said second language component; and\nstep C) includes the step of accessing a binary decision tree with said context indicator to define said probabilistic model.\n10. The method set forth in claim 8 wherein:\nsaid probabilistic model has the form of a Markov model composed of a plurality of Markov sub-models having common initial and final states.\n11. In a speech recognition system, a method for dividing a set of sample values representing respective vocalizations into first and second groups of sample values using an automatically generated test question, the method comprising the steps of:\nA) annotating each of the sample values in said set with an indicator of a first attribute of said sample values;\nB) further annotating each of the sample values in said set with an indicator of a second attribute of said sample values, said set of further annotated samples having a predetermined entropy value measured with respect to said second attribute indicator; and\nC) generating the test question, in terms of the first attribute indicator of said further annotated samples, wherein, the test question is applied to divide the further annotated sample values into said first and second groups, wherein said first and second groups of samples have a combined entropy value, measured with respect to said second attribute indicator, that is less than said predetermined entropy value.\n12. The method set forth in claim 11 wherein step C) includes the steps of:\ngrouping the annotated sample values in said set into a plurality of clusters according to the second attribute of said sample values; and\nfurther annotating each of said annotated samples with an indicator of the cluster to which it belongs as said second attribute indicator.\n13. The method set forth in claim 12, further including the steps of:\ngenerating a first further test question in terms of the first attribute indicator of the sample values in said second group which transfers sample values from said second group to said first group to form revised first and second groups of sample values having a reduced combined entropy value measured with respect to said second attribute indicator; and\ngenerating a second further test question in terms of the first attribute indicator of said sample values in said first group which transfers sample values from said first group to said second group to form further revised first and second groups of sample values having a further reduced combined entropy value measured with respect to said second attribute indicator.\n14. The method set forth in claim 13, wherein:\nsaid sample values are respective sequences of fenemes representing sequential vocalizations of phonemes;\nsaid first attribute of said sample values relates to the phonemes occurring proximate in time with the respective phonemes corresponding to said sample values; and\nsaid second attribute of said sample values relates to the vocalizations represented by said sequences of fenemes.\n15. The method set forth in claim 12, wherein each samples value in said set of sample values includes a sequence of subsample values having distinct types and the step of grouping the annotated sample values in said set into clusters includes the steps of:\nassigning each sample value to a respectively different prototype cluster;\ncalculating respective expected frequency values for each type of subsample in each of the prototype clusters;\nstatistically comparing the expected frequency values of each subsample type in each prototype cluster to the expected frequencies of each subsample type in all other prototype clusters to generate a plurality of statistical difference values, one for each pair of prototype clusters;\ncombining pairs of prototype clusters that exhibit a statistical difference value which is less than a threshold value to generate new prototype clusters.\n16. The method set forth in claim 15 wherein the step of statistically comparing the expected frequency of each subsample type in each prototype cluster to the expected frequencies of all other subsample types in all other prototypes clusters includes the steps of:\ngenerating a plurality of probabilistic models, each model representing a Markov model of the sequences of subsamples represented by a respectively different one of said prototype clusters;\ngenerating a plurality of histograms, each histogram representing the relative frequency of occurrence of each type of subsample in a respectively different one of said prototype clusters;\ncalculating, for each pair of prototype clusters, a log-likelihood ratio that the histograms for each prototype cluster in said pair match the expected frequencies of subsamples in a single probabilistic model representing a combination of the respective probabilistic models for said pair of prototype clusters, wherein a sign inverted version of said log-likelihood ratio is the statistical difference value for said pair of clusters.\n17. In a voice recognition system, a method for grouping a plurality of vocalizations, represented by respective sequences of samples having respective sample values, into clusters comprising the steps of:\nA) assigning each sequence of samples to a respectively different prototype cluster;\nB) calculating an expected frequency value for each sample value in each prototype cluster;\nC) statistically comparing the expected frequency value of each sample value of each prototype cluster to the expected frequencies of all other sample values in all other prototype clusters to generate a plurality of statistical difference values, one for each pair of clusters;\nD) combining pairs of prototype clusters that exhibit a statistical difference value which is less than a threshold value to generate new prototype clusters.\n18. The method of groping a plurality of sequences of samples set forth in claim 17 further including the step of repeating steps B) through D) until no prototype clusters can be combined.\n19. The method of grouping a plurality of vocalizations set forth in claim 18 wherein the step of statistically comparing the expected frequency of each sample value of each prototype cluster to the expected frequencies of all other sample value of all other prototype clusters includes the steps of:\ngenerating a plurality of probabilistic models, each model representing a Markov model of the sequences of samples represented by a respectively different one of said prototype clusters;\ngenerating a plurality of histograms, each histogram representing the relative frequency of occurrence of each sample value in a respectively different one of said prototype clusters;\ncalculating, for each pair of prototype clusters, a log-likelihood ratio that the histograms for each prototype cluster in said pair match the expected frequency values of sample values of a single probabilistic model representing a combination of the respective probabilistic models for said pair of prototype clusters, wherein a sign inverted version of said log-likelihood ratio is the statistical difference value for said pair of clusters.\n20. Apparatus for automatically separating vocalizations of language components into a plurality of groups representing pronunciations of the language components in respectively different contexts, comprising:\nsampling means for converting a training text and vocalizations of the training test into a plurality of samples representing the language components of said vocalizations;\nmeans for selecting, from among the plurality of samples, a set of samples representing respective instances of a selected language component in the vocalizations;\nprocessing means including means for annotating each of said selected samples with a context indicator representing at least one language component in a contextual relationship with the selected sample, to produce annotated samples;\nmeans for separating the selected samples into respectively different leaf groups based on the respective context indicators of said annotated samples, each of said leaf groups representing a pronunciation of said selected language component in a respectively different context.\n21. The appratus set forth in claim 20 wherein:\nthe processing means further includes:\nmeans for grouping said annotated selected samples into a plurality of clusters, each cluster representing a respectively different pronunciation of said selected language component; and\nmeans for further annotating each selected sample with an indicator of the cluster to which it belongs.\n22. The apparatus set forth in claim 21 wherein:\nsaid language components are phonemes;\nsaid samples are sequences of fenemes having distinct types, each of said sequences of fenemes corresponding to a respective one of said phonemes; and\nthe means for grouping said annotated selected samples includes:\nmeans for assigning each sequence of fenemes to a respectively different prototype cluster;\nmeans for calculating respective expected frequency values for each type of feneme in each of the prototype clusters;\nstatistical comparison means for statistically comparing the expected frequency values for the respective types of fenemes in each prototype cluster to the expected frequency values for the respective types of fenemes in all other prototype clusters to generate a plurality of statistical difference values, one for each pair of prototype clusters; and\nmeans for combining pairs of prototype clusters that exhibit a statistical difference value which is less than a threshold value to generate new prototype clusters.\n23. The apparatus of claim 22 wherein the statistical comparison means includes:\nmeans for generating a plurality of probabilistic models, each model representing a Markov model of the pronunciation of the phoneme represented by a respectively different one of said prototype clusters;\nmeans for generating a plurality of histograms, each histogram representing the relative frequency of occurrence of each feneme in a respectively different one of said prototype clusters;\nmeans for calculating, for each pair of prototype clusters, a log-likelihood ratio that the histograms for each prototype cluster in said pair match expected frequencies of the respective types of fenemes in a single probabilistic model representing a combination of the respective probabilistic models for said pair of prototype clusters, wherein a sign inverted version of said log-likelihood ratio is the statistical difference value for said pair of clusters.\n24. The apparatus of claim 20 further comprising means for generating a probabilistic model for each leaf group of the decision tree, including:\nmeans for grouping the samples of the leaf group into a plurality of clusters, each cluster representing a respectively different pronunciation of the language component represented by the samples of the leaf group;\nmeans for generating, from said plurality of clusters, a respective plurality of statistical models each of said statistical models having the form of a Markov model; and\nmeans for augmenting said plurality of statistical models by adding a common initial state and a common final state to each model to generate said probabilistic model.",
    "status": "Expired - Fee Related",
    "citations_own": [
        "US4091237A",
        "US4181821A",
        "US4307446A",
        "US4319085A",
        "US4466060A",
        "US4535473A",
        "US4759068A",
        "US4827521A",
        "US4833712A",
        "US4837831A",
        "US4852173A"
    ],
    "citations_ftf": [
        "EP0238697B1",
        "DE3681023D1"
    ],
    "citedby_own": [
        "US5202926A",
        "US5230037A",
        "US5233681A",
        "US5329608A",
        "US5345537A",
        "WO1994020952A1",
        "US5355311A",
        "WO1995002879A1",
        "US5452397A",
        "US5465318A",
        "US5477451A",
        "US5497447A",
        "US5510981A",
        "US5524169A",
        "US5526465A",
        "US5553284A",
        "US5581655A",
        "US5613036A",
        "US5649023A",
        "US5657424A",
        "EP0789902A1",
        "US5664059A",
        "US5680509A",
        "US5682501A",
        "US5710916A",
        "US5715469A",
        "US5719996A",
        "US5737490A",
        "US5765133A",
        "US5774848A",
        "US5787394A",
        "US5787395A",
        "US5794197A",
        "US5794192A",
        "US5799279A",
        "US5799277A",
        "US5799276A",
        "US5802251A",
        "US5832430A",
        "US5865626A",
        "US5875426A",
        "US5970453A",
        "US5983180A",
        "US6023673A",
        "US6026397A",
        "US6058365A",
        "US6064959A",
        "US6134528A",
        "US6141641A",
        "US6151575A",
        "US6163768A",
        "US6167377A",
        "US6185530B1",
        "US6192337B1",
        "US6195635B1",
        "US6224636B1",
        "US6236964B1",
        "US6269335B1",
        "US6311157B1",
        "US6408270B1",
        "US20020152246A1",
        "US20020198713A1",
        "US6529865B1",
        "US6535886B1",
        "US6601027B1",
        "US20030200191A1",
        "US20030200075A1",
        "US20030220793A1",
        "US20040019574A1",
        "US20040034524A1",
        "US6711541B1",
        "US6718305B1",
        "US6721697B1",
        "US20040215449A1",
        "WO2004097673A1",
        "US6839670B1",
        "US20050049870A1",
        "US6928448B1",
        "US20050285933A1",
        "US20060136195A1",
        "US20070150277A1",
        "WO2007120777A2",
        "US20090155751A1",
        "US20090191521A1",
        "US20090208913A1",
        "US20100100379A1",
        "US20100145707A1",
        "US8140336B2",
        "US20130166283A1",
        "US8494850B2",
        "US8583432B1",
        "US8892446B2",
        "US8977584B2",
        "US20150302852A1",
        "US20150371633A1",
        "US9262612B2",
        "US9300784B2",
        "US9330720B2",
        "US9338493B2",
        "US9355651B2",
        "US9368114B2",
        "US9430463B2",
        "US9483461B2",
        "US9495129B2",
        "US9502031B2",
        "US9535906B2",
        "US9576574B2",
        "US9582608B2",
        "US9620104B2",
        "US9620105B2",
        "US9626955B2",
        "US9633660B2",
        "US9633004B2",
        "US9633674B2",
        "US9646609B2",
        "US9646614B2",
        "US9668121B2",
        "US9697822B1",
        "US9697820B2",
        "US9711141B2",
        "US9715875B2",
        "US9721566B2",
        "US9734193B2",
        "US9760559B2",
        "US9785630B2",
        "US9798393B2",
        "US9818400B2",
        "US9842101B2",
        "US9842105B2",
        "US9858922B2",
        "US9858925B2",
        "US9865280B2",
        "US9886953B2",
        "US9886432B2",
        "US9899019B2",
        "US9922642B2",
        "US9934775B2",
        "US9953088B2",
        "US9959870B2",
        "US9966065B2",
        "US9966068B2",
        "US9971774B2",
        "US9972304B2",
        "US10049663B2",
        "US10049668B2",
        "US10057736B2",
        "US10067938B2",
        "US10074360B2",
        "US10079014B2",
        "US10078631B2",
        "US10083688B2",
        "US10089072B2",
        "US10101822B2",
        "US10127220B2",
        "US10127911B2",
        "US10134385B2",
        "US10170123B2",
        "US10176167B2",
        "US10186254B2",
        "US10185542B2",
        "US10192552B2",
        "US10199051B2",
        "US10204619B2",
        "US20190066656A1",
        "US10223066B2",
        "US10223934B2",
        "US10241752B2",
        "US10241644B2",
        "US10249300B2",
        "US10255907B2",
        "US10269345B2",
        "US10276170B2",
        "US10283110B2",
        "US10289433B2",
        "US10297253B2",
        "US10318871B2",
        "US10354011B2",
        "US10366158B2",
        "US10446141B2",
        "US10446143B2",
        "US10490187B2",
        "US10496753B2",
        "US10509862B2",
        "US10521466B2",
        "US10529357B2",
        "US10552013B2",
        "US10553209B2",
        "US10567477B2",
        "US10568032B2",
        "US10593346B2",
        "US10592095B2",
        "US10659851B2",
        "US10671428B2",
        "US10679605B2",
        "US10691473B2",
        "US10705794B2",
        "US10706373B2",
        "US10733993B2",
        "US10747498B2",
        "US10762293B2",
        "US10791216B2",
        "US10789041B2",
        "US10791176B2",
        "US10810274B2",
        "US10951859B2",
        "US10957310B1",
        "US11010550B2",
        "US11025565B2",
        "US11295730B1",
        "US11587559B2",
        "US11599332B1"
    ],
    "citedby_ftf": [
        "US5388183A",
        "JPH05257492A",
        "EP0602296A1",
        "NL9301119A",
        "US5729656A",
        "JP2982689B2",
        "US5758024A",
        "US7599840B2",
        "US8895546B2",
        "CN102246226B",
        "US8719023B2"
    ]
}