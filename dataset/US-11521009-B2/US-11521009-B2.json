{
    "patent_id": "US-11521009-B2",
    "title": "Automatically generating training data for a lidar using simulated vehicles in virtual space ",
    "assignee": "Luminar, Llc",
    "publication_date": "2022-12-06",
    "patent_link": "https://patents.google.com/patent/US11521009B2/en",
    "inventors": [
        "Miguel Alexander Peake",
        "Benjamin Englard"
    ],
    "classifications": [
        "G06F13/28",
        "G06K9/6256",
        "G01C21/28",
        "G01S17/931",
        "G01S7/4808",
        "G01S7/497",
        "G05D1/0088",
        "G05D1/0231",
        "G05D1/0257",
        "G05D1/0278",
        "G06F18/214",
        "G06F18/2148",
        "G06F18/2411",
        "G06F18/2413",
        "G06K9/6257",
        "G06K9/6269",
        "G06N3/006",
        "G06N5/04",
        "G06T11/00",
        "G06T7/246",
        "G06T7/50",
        "G06V10/764",
        "G06V10/774",
        "G06V20/56",
        "G08G1/0962",
        "G01S17/87",
        "G05D2201/0213",
        "G06F2213/28",
        "G06F9/451",
        "G06N20/00",
        "G06N3/045",
        "G06N3/088",
        "G06T11/60",
        "G06T2207/10016",
        "G06T2207/10024",
        "G06T2207/10028",
        "G06T2207/20081",
        "G06T2207/20084",
        "G06T2207/30241",
        "G06T2207/30256",
        "G06V20/588"
    ],
    "abstract": "Automated training dataset generators that generate feature training datasets for use in real-world autonomous driving applications based on virtual environments are disclosed herein. The feature training datasets may be associated with training a machine learning model to control real-world autonomous vehicles. In some embodiments, an occupancy grid generator is used to generate an occupancy grid indicative of an environment of an autonomous vehicle from an imaging scene that depicts the environment. The occupancy grid is used to control the vehicle as the vehicle moves through the environment. In further embodiments, a sensor parameter optimizer may determine parameter settings for use by real-world sensors in autonomous driving applications. The sensor parameter optimizer may determine, based on operation of the autonomous vehicle, an optimal parameter setting of the parameter setting where the optimal parameter setting may be applied to a real-world sensor associated with real-world autonomous driving applications.",
    "claims": "\n1. A non-transitory computer-readable medium storing thereon instructions executable by one or more processors to implement an automated training dataset generator that generates feature training datasets for use in real-world autonomous driving applications based on virtual environments, the automated training dataset generator comprising:\nan imaging engine configured to generate a plurality of imaging scenes defining a virtual environment, the plurality of imaging scenes including a plurality of photo-realistic scenes and a plurality of corresponding depth-map-realistic scenes;\na physics component configured to generate environment-object data defining how objects or surfaces interact with each other in the virtual environment;\nan autonomous vehicle simulator configured to control an autonomous vehicle within the virtual environment based on one or both of (i) the plurality of photo-realistic scenes and (ii) the plurality of depth-map-realistic scenes; and\na dataset component configured to generate one or more virtual feature training datasets based on at least one of (i) the plurality of photo-realistic scenes, (ii) the plurality of depth-map-realistic scenes, or (iii) the environment-object data,\nwherein the dataset component is further configured to generate at least one real-world training dataset, the real-world training dataset based on real-world data, the real-world training dataset normalized with respect to the one or more virtual feature training datasets,\nwherein the real-world data includes real-world environment-object data, the real-world environment-object data captured by one or more sensors associated with a real-world vehicle, and\nwherein the virtual feature training dataset and the real-world training dataset as normalized is associated with training a machine learning model to control an autonomous vehicle in a real-world autonomous driving application.\n2. The non-transitory computer-readable medium of claim 1, wherein the feature training dataset is associated with training the machine learning model to detect or track one or more objects within the virtual environment or within a real-world environment.\n3. The non-transitory computer-readable medium of claim 1, wherein the feature training dataset is associated with training the machine learning model to detect or track one or more vehicle lanes within the virtual environment or within a real-world environment.\n4. The non-transitory computer-readable medium of claim 1, wherein the feature training dataset is associated with training the machine learning model to detect or track one or more road-free spaces within the virtual environment or within a real-world environment.\n5. The non-transitory computer-readable medium of claim 1, wherein the feature training dataset is associated with training the machine learning model to predict, for an object within the virtual environment or within a real-world environment, one of future object behavior, object intent, or future object trajectory.\n6. The non-transitory computer-readable medium of claim 1, wherein the feature training dataset is associated with training the machine learning model to estimate a depth based on one or more virtual cameras within the virtual environment.\n7. The non-transitory computer-readable medium of claim 1, wherein the plurality of photo-realistic scenes comprises two-dimensional (2D) images that simulate real-world scenes captured by a 2D camera.\n8. The non-transitory computer-readable medium of claim 7, wherein the 2D images comprise 2D pixel data.\n9. The non-transitory computer-readable medium of claim 1, wherein one or more color pixels of each of the depth-map-realistic scenes are associated with one or more corresponding depths within the depth-map-realistic scene.\n10. The non-transitory computer-readable medium of claim 1, wherein one or more color pixels of each of the depth-map-realistic scenes are associated with one or more corresponding simulated reflectivity values associated with one or more objects or surfaces within the virtual environment.\n11. The non-transitory computer-readable medium of claim 1, wherein the environment-object data defines how a first object or surface interacts with a second object or surface within the virtual environment.\n12. The non-transitory computer-readable medium of claim 11, wherein the first object or surface is the autonomous vehicle and the second object or surface is a virtual pothole.\n13. The non-transitory computer-readable medium of claim 1, wherein the environment-object data relates to motion of a vehicle within the virtual environment.\n14. The non-transitory computer-readable medium of claim 13, wherein the motion of the vehicle is defined by one or more of: a position of the vehicle, a velocity of the vehicle, an acceleration of the vehicle, or a trajectory of the vehicle.\n15. The non-transitory computer-readable medium of claim 1, wherein the autonomous vehicle simulator is further configured to control the autonomous vehicle within the virtual environment based on the environment-object data.\n16. The non-transitory computer-readable medium of claim 1, wherein the plurality of imaging scenes corresponds to a plurality of frames comprising a video.\n17. The non-transitory computer-readable medium of claim 16, wherein the video is rendered at a select number of frames per second.\n18. The non-transitory computer-readable medium of claim 17, wherein the select number of frames per second is 30 frames per second.\n19. The non-transitory computer-readable medium of claim 16, wherein the video defines the autonomous vehicle moving along a standard route within the virtual environment, the standard route being a predefined route.\n20. The non-transitory computer-readable medium of claim 19, wherein the standard route within the virtual environment defines a ground truth route.\n21. The non-transitory computer-readable medium of claim 16, wherein the video defines the autonomous vehicle moving along an undetermined route within the virtual environment.\n22. The non-transitory computer-readable medium of claim 1, wherein the real-world data includes a real-world photo-realistic scene, the real-world photo-realistic scene captured by a two-dimensional (2D) camera.\n23. The non-transitory computer-readable medium of claim 1, wherein the real-world data includes a real-world depth-map realistic scene, the real-world depth-map realistic scene captured by a three-dimensional (3D) sensor.\n24. The non-transitory computer-readable medium of claim 23, wherein the three-dimensional (3D) sensor is a lidar-based sensor.\n25. The non-transitory computer-readable medium of claim 1, wherein the one or more sensors include any one or more of: accelerometers, gyroscopes, motion sensors, or GPS devices.\n26. The non-transitory computer-readable medium of claim 1, wherein the dataset component is further configured to generate at least one real-world training dataset, the real-world training dataset based on real-world data and comprising one or more virtual objects superimposed onto the real-world data, wherein the real-world training dataset is associated with training the machine learning model to control the autonomous vehicle in the real-world autonomous driving application.\n27. The non-transitory computer-readable medium of claim 1, wherein the virtual environment includes one or more objects or surfaces, and wherein each object or surface is associated with a descriptor.\n28. The non-transitory computer-readable medium of claim 27, wherein the descriptor of each object or surface includes any one or more of the following: a unique identifier (ID) of the object or surface in the virtual environment, a category of the object or surface as defined within the virtual environment, a position value of the object or surface within the virtual environment, an orientation of the object or surface within the virtual environment, a velocity of the object or surface within the virtual environment, a reflectivity of the object or surface, or a status of the object or surface within the virtual environment.\n29. The non-transitory computer-readable medium of claim 27, wherein the descriptor of each object or surface includes any one or more of the following: an object class of an object or surface in the virtual environment or a future trajectory of an object or surface in the virtual environment.\n30. The non-transitory computer-readable medium of claim 1, wherein the automated training dataset generator further comprises a waypoint vehicle simulator configured to control one or more waypoint vehicles, wherein each waypoint vehicle follows a predetermined route within the virtual environment.\n31. The non-transitory computer-readable medium of claim 30, wherein the one or more waypoint vehicles interact with the autonomous vehicle within the virtual environment.\n32. The non-transitory computer-readable medium of claim 30, wherein the one or more waypoint vehicles implement one or more driving strategies.\n33. The non-transitory computer-readable medium of claim 30, wherein the one or more driving strategies include any of: a conservative driving strategy, an aggressive driving strategy, or a normal driving strategy.\n34. The non-transitory computer-readable medium of claim 30, wherein the machine learning model is trained with reinforcement learning, the reinforcement learning based on vehicle operation data captured when the autonomous vehicle interacts with the one or more waypoint vehicles.\n35. The non-transitory computer-readable medium of claim 1, wherein the automated training dataset generator further comprises a sensor simulator configured to generate simulated sensor data within the virtual environment.\n36. The non-transitory computer-readable medium of claim 35, wherein the sensor simulator positions one or more virtual sensors in the virtual environment, and wherein the one or more virtual sensors are configured to generate the simulated sensor data.\n37. The non-transitory computer-readable medium of claim 35, wherein the sensor simulator generates the sensor data via ray casting.\n38. The non-transitory computer-readable medium of claim 35, wherein the sensor simulator generates simulated lidar data or simulated radar data.\n39. The non-transitory computer-readable medium of claim 35, wherein the sensor simulator generates the sensor data based on the depth-map-realistic scenes.\n40. The non-transitory computer-readable medium of claim 35, wherein the sensor simulates the sensor data using a graphic shader.\n41. The non-transitory computer-readable medium of claim 35, wherein a particular object or surface is associated with a reflectivity value within the virtual environment, and wherein the sensor simulator generates at least a portion of the sensor data based on the reflectivity value.\n42. The non-transitory computer-readable medium of claim 41, wherein the reflectivity value is derived from a color of the particular object or surface.\n43. The non-transitory computer-readable medium of claim 41, wherein the reflectivity value is derived from a normal angle to a position of a virtual sensor.\n44. The non-transitory computer-readable medium of claim 35, wherein the simulated sensor data is accessed via direct memory access (DMA).\n45. The non-transitory computer-readable medium of claim 1, wherein the automated training dataset generator further comprises a geo-spatial component configured to generate the virtual environment based on geo-spatial data, the geo-spatial data defining one or more positions of simulated objects or surfaces within the virtual environment.\n46. The non-transitory computer-readable medium of claim 45, wherein the one or more simulated objects or surfaces include one or more of: a virtual road or street, a virtual building, a virtual tree or landscaping object, a virtual traffic sign, a virtual traffic light, a simulated pedestrian, or a simulated vehicle.\n47. The non-transitory computer-readable medium of claim 45, wherein the geo-spatial data includes predefined images.\n48. The non-transitory computer-readable medium of claim 47, wherein the predefined images are sourced from a remote server.\n49. The non-transitory computer-readable medium of claim 45, wherein the geo-spatial data includes real-world lidar based data.\n50. The non-transitory computer-readable medium of claim 45, wherein the geo-spatial data includes geo-spatial metadata, the geo-spatial metadata exposing detail parameters for generating the one or more simulated objects or surfaces within the virtual environment.\n51. The non-transitory computer-readable medium of claim 50, wherein the detail parameters include a number of lanes for a road and a width for the road.\n52. The non-transitory computer-readable medium of claim 50, wherein the detail parameters include elevation data for a particular simulated object or surface within the virtual environment.\n53. The non-transitory computer-readable medium of claim 45, wherein the geo-spatial component updates the virtual environment via simultaneous localization and mapping (SLAM).\n54. The non-transitory computer-readable medium of claim 45, wherein the geo-spatial component updates the virtual environment via photogrammetry.\n55. The non-transitory computer-readable medium of claim 1, wherein the autonomous vehicle simulator is further configured to apply one or more driving strategies.\n56. The non-transitory computer-readable medium of claim 55, wherein the one or more driving strategies are configurable driving strategies, the configurable driving strategies including parameters that when altered update vehicle operation of the autonomous vehicle within the virtual environment.\n57. The non-transitory computer-readable medium of claim 1, further comprising a scenario simulator configured to generate one or more simulated environment scenarios, wherein each of the one or more simulated environment scenarios corresponds to a generation of variations of a particular object, surface, or situation within the virtual environment.\n58. The non-transitory computer-readable medium of claim 57, wherein the particular object, surface, or situation is any one of: a road intersection, a stop sign, or a traffic light.\n59. The non-transitory computer-readable medium of claim 57, wherein the particular object, surface, or situation is any one of: a pedestrian's activity within the virtual environment or a railroad arm's behavior within the virtual environment.\n60. The non-transitory computer-readable medium of claim 57, wherein the one or more simulated environment scenarios are generated via one or more generative machine learning models.\n61. The non-transitory computer-readable medium of claim 60, wherein the one or more generative machine learning models are one or more generative adversarial networks (GANs).\n62. The non-transitory computer-readable medium of claim 57, wherein the one or more simulated environment scenarios are generated via procedural generation.\n63. The non-transitory computer-readable medium of claim 1, wherein at least a portion of the virtual environment is generated via a plurality of generative machine learning models.\n64. The non-transitory computer-readable medium of claim 63, wherein at least one of the plurality of generative machine learning is a generative adversarial network (GAN).\n65. The non-transitory computer-readable medium of claim 63, wherein at least a portion of the virtual environment is generated based on data collected by real-world sensors associated with real-world vehicles.\n66. The non-transitory computer-readable medium of claim 1, wherein the automated training dataset generator further comprises a configuration manager, the configuration manager accepting a predefined configuration, the predefined configuration defining configuration information for one or more objects or surfaces within the virtual environment.\n67. The non-transitory computer-readable medium of claim 66, wherein the predefined configuration is a configuration file.\n68. The non-transitory computer-readable medium of claim 66, wherein the predefined configuration file includes a JavaScript object notation (JSON) format.\n69. The non-transitory computer-readable medium of claim 66, wherein the configuration information includes spawn positions for the one or more objects or surfaces within the virtual environment.\n70. The non-transitory computer-readable medium of claim 66, wherein the configuration information includes one or more of the following: a weight of a particular object or surface, a number of sensors associated with a virtual vehicle, or a location of sensors placed on the virtual vehicle.\n71. The non-transitory computer-readable medium of claim 1, wherein one or more outputs of the machine learning model are compared to one or more ground truth values.\n72. The non-transitory computer-readable medium of claim 71, wherein the one or more ground truth values each include a vehicle action and a corresponding safety parameter, the safety parameter defining (i) a safety-related outcome, or (ii) a degree of safety, that is associated with the vehicle action.\n73. The non-transitory computer-readable medium of claim 71, wherein the machine learning model is updated to choose vehicle actions that maximize a degree of safety across a plurality of ground truth values.\n74. The non-transitory computer-readable medium of claim 73, wherein the machine learning model is updated to choose vehicle actions that vary the degree of safety across a plurality of ground truth values.\n75. An automated training dataset generation method for generating feature training datasets for use in real-world autonomous driving applications based on virtual environments, the automated training dataset generation method comprising:\ngenerating a plurality of imaging scenes defining a virtual environment, the plurality of imaging scenes including a plurality of photo-realistic scenes and a plurality of corresponding depth-map-realistic scenes;\ngenerating environment-object data defining how objects or surfaces interact with each other in the virtual environment;\ncontrolling an autonomous vehicle within the virtual environment based on one or both of (i) the plurality of photo-realistic scenes and (ii) the plurality of depth-map-realistic scenes; and\ngenerating one or more virtual feature training datasets based on at least one of (i) the plurality of photo-realistic scenes, (ii) the plurality of depth-map-realistic scenes, or (iii) the environment-object data; and\ngenerating at least one real-world training dataset, the real-world training dataset based on real-world data, the real-world training dataset normalized with respect to the one or more virtual feature training datasets,\nwherein the real-world data includes real-world environment-object data, the real-world environment-object data captured by one or more sensors associated with a real-world vehicle, and\nwherein the virtual feature training dataset and the real-world training dataset as normalized is associated with training a machine learning model to control an autonomous vehicle in a real-world autonomous driving application.",
    "status": "Active",
    "citations_own": [
        "US20170369073A1",
        "US20180074493A1",
        "US20180275658A1",
        "US20190034794A1",
        "US20190147250A1",
        "US20190243371A1",
        "US20190303759A1",
        "US20190325241A1",
        "US20200089506A1"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US20220135035A1",
        "US11654550B1"
    ],
    "citedby_ftf": [
        "WO2017118907A1",
        "US20190155291A1",
        "US10678244B2",
        "US10671349B2",
        "US11409692B2",
        "US11157441B2",
        "JP7133926B2",
        "US11561791B2",
        "US10877152B2",
        "US10884422B2",
        "US11126199B2",
        "US11215999B2",
        "US10768629B2",
        "US11636333B2",
        "US11562231B2",
        "US11508049B2",
        "EP3856596A4",
        "US10754030B2",
        "US11196678B2",
        "US10861176B2",
        "US11593539B2",
        "US11537811B2",
        "US11610117B2",
        "CN109858369A",
        "US11656620B2",
        "US11332007B2",
        "US10997461B2",
        "US10829114B2",
        "US11567514B2",
        "US10860878B2",
        "US10956755B2",
        "US11440471B2",
        "US11016496B2",
        "EP3736740A1",
        "DE102019209535A1",
        "US11453404B2",
        "US11727169B2",
        "US11126891B2",
        "US20210097731A1",
        "EP3822913A1",
        "EP3832525A1",
        "US11276179B2",
        "CN111144015A",
        "US11216669B1",
        "WO2021150497A1",
        "US11170568B2",
        "US11670190B1",
        "JP2021131652A",
        "US11347968B2",
        "US20210286924A1",
        "US20210286923A1",
        "US11493625B2",
        "US11314495B2",
        "KR20210132496A",
        "US11443138B2",
        "CN111581887B",
        "CN111812674A",
        "GB2596080A",
        "US20200324794A1",
        "EP3951673A1",
        "US20220055640A1",
        "JP2022044383A",
        "US11270164B1",
        "CN112347851B",
        "CN112269385B",
        "KR20220060404A",
        "US11348036B1",
        "DE102020215535A1",
        "US20220187463A1",
        "US11731652B2",
        "US20220242441A1",
        "WO2022171607A1",
        "WO2022197736A1",
        "US20220317300A1",
        "US11743334B2",
        "US20220315049A1",
        "DE102021117086A1",
        "CN113436293B",
        "WO2023009926A1",
        "WO2023010540A1",
        "US20230052039A1",
        "US11504622B1",
        "US11697069B1",
        "CN115272994B",
        "KR20230045826A",
        "US20230135398A1",
        "US20230147874A1",
        "US20230169322A1",
        "US11790604B2",
        "US20230205951A1",
        "US20230229826A1",
        "CN114162146B",
        "DE102023104342A1",
        "US11574002B1"
    ]
}