{
    "patent_id": "US-11048619-B2",
    "title": "AI software testing system and method ",
    "assignee": "Appdiff, Inc.",
    "publication_date": "2021-06-29",
    "patent_link": "https://patents.google.com/patent/US11048619B2/en",
    "inventors": [
        "Jason Joseph Arbon",
        "Justin Mingjay Liu",
        "Christopher Randall Navrides"
    ],
    "classifications": [
        "G06F11/3684",
        "G06F11/3664",
        "G06F11/3688",
        "G06F11/3692",
        "G06N20/00",
        "G06N3/006",
        "G06F3/0484"
    ],
    "abstract": "A system for performing software testing uses machine learning to extract features from a user interface of an app, classify screen types and screen elements of the user interface, and implement flows of test sequences to test the app. Training is performed to train the system to learn common application states of an application graph and to navigate through an application. In some implementations, the training includes Q-learning to learn how to navigate to a selected screen state. In some implementations, there is reuse of classifiers cross-application and cross platform.",
    "claims": "\n1. An artificial intelligence software testing system to test software applications having a sequence of screens with each screen having a set of screen elements, comprising:\na set of intelligent machine learning bots trained to:\ncrawl through a software application;\nidentify screen types and screen elements of the screens using a set of trained classifiers; and\napply test cases to the software application;\nwherein the software application has an associated logical state graph associated with potential user interactions with a graphical user interface and the set of intelligent machine learning bots are trained to determine application states and sequences of states associated with the logical state graph.\n2. The system of claim 1, wherein applying test cases comprises:\nidentify test cases based on the identified screen types and screen elements;\napply the identified test cases to the software application; and\nreport test results for the software application.\n3. The system of claim 1, wherein the set of intelligent machine learning bots are trained to:\nidentify test cases based on the identified screen types, screen elements, and associated application states and sequences of states of the logical state graph.\n4. The system of claim 1, wherein the set of classifiers are trained to:\nanalyze a visual appearance of screens using at least one classifier trained to analyze a visual appearance of graphical user interfaces;\nidentify screen types and screen elements based at least in part on a visual appearance.\n5. The system of claim 1, wherein the set of classifiers are trained to:\ndetermine a screen type based at least in part on a visual appearance of the screen based on an image classification.\n6. The system of claim 1, wherein the set of classifiers are trained to analyze elements, screens, and flows of applications.\n7. The system of claim 1, wherein the set of intelligent bots are further trained to report performance of the software application.\n8. The system of claim 1, wherein the system is configured to test software apps for different platforms by applying a conversion table to adjust the testing for differences in software application appearance and formatting on different platforms, devices, screen sizes, and screen densities.\n9. The system of claim 1, wherein the set of classifiers are trained to analyze images on the screens to identify displayable user interface elements of the software application.\n10. The system of claim 1, further comprising generating training data for the system based on an initial set of labelled training data for a set of training apps and providing ongoing training data based on feedback from ongoing testing of software apps.\n11. The system of claim 1, wherein the system is trained to:\nidentify a set of interactable user interface screen elements; and\ndetermine navigational paths between at least two different screen types.\n12. An artificial intelligence software testing system to test software applications having a sequence of screens with each screen having a set of screen elements, comprising:\na set of intelligent machine learning bots trained to:\ncrawl through a software application;\nidentify screen types and screen elements of the screens using a set of trained classifiers; and\napply test cases to the software application;\nwherein the set of classifiers are trained to recognize screens and screen elements common to a class of software applications having common screen states in a nodal state graph.\n13. The system of claim 12, wherein the nodal graph includes at least one of search screen node, a shopping cart screen node, a sign-in screen node, a sign-out screen node, a product screen node and a checkout screen node.\n14. An artificial intelligence software testing system to test software applications having a sequence of screens with each screen having a set of screen elements, comprising:\na set of intelligent machine learning bots trained to:\ncrawl through a software application;\nidentify screen types and screen elements of the screens using a set of trained classifiers; and\napply test cases to the software application;\nwherein the software application is represented by an abstract node graph and the set of intelligent machine learning bots includes a set of trained machine learning engine each trained for a subgraph of the node graph.\n15. A computer-implemented method to test software applications having a sequence of screens with each screen having a set of screen elements, using a set of intelligent machine learning bots trained to perform a method comprising:\ncrawling through a software application;\nidentifying screen types and screen elements of the screens using a set of trained classifiers; and\napplying test cases to the software application;\nwherein applying test cases includes identifying test cases based on the identified screen types and screen elements, applying the identified test cases to the software application, and reporting test results for the software application.\n16. The method of claim 15, wherein the method comprises the set of classifiers:\nanalyzing a visual appearance of screen using at least one classifier trained to analyze a visual appearance of graphical user interfaces;\nidentifying screen types and screen elements based at least in part on a visual appearance.\n17. The method of claim 15, comprising:\ndetermining a screen type based at least in part on a visual appearance of the screen based on an image classification.\n18. The method of claim 15, wherein the set of classifiers are trained to analyze elements, screens, and flows of applications.\n19. The method of claim 15, wherein the set of intelligent bots are further trained to report performance of the software application.\n20. The method of claim 15, further comprising testing software apps for different platforms by applying a conversion table to adjust the testing for differences in software application appearance and formatting on different platforms, devices, screen sizes, and screen densities.\n21. The method of claim 15, wherein the set of classifiers are trained to analyze images on the screens to identify displayable user interface elements of the software application.\n22. The method of claim 15, further comprising generating training data for the system based on an initial set of labelled training data for a set of training apps and providing ongoing training data based on feedback from ongoing testing of software apps.\n23. The method of claim 15, comprising training the system to:\nidentify a set of interactable user interface screen elements of an app; and\ndetermine navigational paths between at least two different screen types.\n24. A computer-implemented method to test software applications having a sequence of screens with each screen having a set of screen elements, using a set of intelligent machine learning bots trained to perform a method comprising:\ncrawling through a software application;\nidentifying screen types and screen elements of the screens using a set of trained classifiers; and\napplying test cases to the software application;\nwherein the software application has an associated logical state graph associated with potential user interactions with a graphical user interface and the method further comprises determining application states and sequences of states associated with the logical state graph.\n25. The method of claim 24, wherein the method further comprises:\nidentifying test cases based on the identified screen types, screen elements, and associated application states and sequences of states of the logical state graph.\n26. A computer-implemented method to test software applications having a sequence of screens with each screen having a set of screen elements, using a set of intelligent machine learning bots trained to perform a method comprising:\ncrawling through a software application;\nidentifying screen types and screen elements of the screens using a set of trained classifiers, including recognizing screens and screen elements common to a class of software applications having common screen states in a nodal state graph; and\napplying test cases to the software application.\n27. The method of claim 26, wherein the nodal graph includes at least one of search screen node, a shopping cart screen node, a sign-in screen node, a sign-out screen node, a product screen node and a checkout screen node.\n28. A computer-implemented method to test software applications having a sequence of screens with each screen having a set of screen elements, using a set of intelligent machine learning bots trained to perform a method comprising:\ncrawling through a software application;\nidentifying screen types and screen elements of the screens using a set of trained classifiers; and\napplying test cases to the software application;\nwherein the method includes representing the software application by an abstract node graph and the set of intelligent machine learning bots includes a set of trained machine learning engines each trained for a subgraph of the node graph.",
    "status": "Active",
    "citations_own": [
        "US20090217302A1",
        "US20150339213A1",
        "US20150363304A1",
        "US20170212829A1",
        "US20190129701A1",
        "US10509717B1"
    ],
    "citations_ftf": [],
    "citedby_own": [],
    "citedby_ftf": [
        "US10810502B2",
        "US10802953B2",
        "US10657035B2",
        "US11641406B2",
        "US11030086B2",
        "JP7211229B2",
        "US11409640B2",
        "US11392469B2",
        "US20220229764A1",
        "US11263117B2",
        "US11366645B2",
        "US11086486B2",
        "US11726752B2",
        "US11442749B2",
        "US11379092B2",
        "US11086707B2",
        "US10783064B1",
        "US11409546B2",
        "US11386356B2",
        "US11010286B1",
        "US10846106B1",
        "US11803415B2",
        "US11496293B2",
        "US11010679B1",
        "US20210349430A1",
        "KR20210142271A",
        "US10824549B1",
        "US11115502B1",
        "US20220100639A1",
        "CN112084117B",
        "US20220108079A1",
        "US10963731B1",
        "US11782733B2",
        "US11544177B2",
        "US20220236857A1",
        "US20220334959A1",
        "US20220382424A1",
        "US11573888B2",
        "DE102021132827A1"
    ]
}