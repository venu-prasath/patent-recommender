{
    "patent_link": "https://patents.google.com/patent/US20020099730A1/en",
    "patent_id": "US20020099730A1",
    "title": "Automatic text classification system",
    "abstract": "An automatic text classification system is provided which extracts words and word sequences from a text or texts to be analyzed. The extracted words and word sequences are compared with training data comprising words and word sequences together with a measure of probability with respect to the plurality of qualities. Each plurality of qualities may be represented by an axis whose two end points correspond to mutually exclusive characteristics. Based on the comparison, the texts to be analyzed are then classified in terms of the plurality of qualities. In addition, a fuzzy logic retrieval system and a system for generating the training data are provided.",
    "inventors": [
        "Daniel Brown",
        "Benjamin Janes",
        "Murray Steele",
        "Richard Cooper"
    ],
    "assignee": "APR SMARTLOGIK Ltd",
    "classifications": [],
    "claims": "\n1. Processing apparatus for generating classification data for text, the processing apparatus comprising:\nidentifying means for identifying semantic content bearing lexical units in data representing the text to be classified;\nsequence determining means for determining sequences of the identified lexical units; and\nclassification data determining means for determining classification data as a score for the text to be classified with respect to each of a plurality of qualities by comparing the determined sequences of the identified lexical units with stored sequences of lexical units for training texts having scores associated therewith for a plurality of qualities.\n2. Processing apparatus according to claim 1, wherein including storage means for storing the stored sequences of lexical units as at least one sequence of lexical units starting from each consecutive semantic content bearing lexical unit in data representing training text, and said sequence determining means is adapted to determine at least one sequence of lexical units starting from each consecutive semantic content bearing lexical unit in the text to be classified, and said classification data determining means is adapted to determine the scores by comparing said at least one sequence starting from each consecutive semantic content bearing lexical unit in data representing the text to be classified with said at least one stored sequence starting from each consecutive semantic content bearing lexical unit in data representing the training text.\n3. Processing apparatus according to claim 2, wherein said at least one sequence of lexical units comprises a sequence of previous lexical units.\n4. Processing apparatus according to claim 2, wherein said at least one sequence of lexical units comprise sequences of 1 to n lexical units, where n is an integer greater than 1.\n5. Processing apparatus according to claim 2, wherein said sequence determining means is adapted to determine sequences of lexical units in which the first lexical unit in said at least one sequence is not a common lexical unit or a modifying lexical unit that modifies the meaning of a subsequent lexical unit, and subsequent lexical units in said at least one sequence can be a modifying lexical unit.\n6. Processing apparatus according to claim 2, wherein said sequence determining means is adapted to determine said at least one sequence of lexical units starting at the beginning of each sentence in the text to be classified so that said at least one sequence of lexical units does not include lexical units from another sentence and sequences of lexical units starting with lexical units at the beginning of sentences can include identifiers in the sequence to identify that there is no word in a position in the sequence.\n7. Processing apparatus according to claim 1, wherein said at least one sequence of lexical units further includes a single semantic content bearing lexical unit.\n8. Processing apparatus according to claim 1, wherein said identifying means is adapted to identify semantic content bearing lexical units by rejecting common words, and to stem words to provide the semantic content bearing lexical units as word stems.\n9. Processing apparatus according to claim 1, including storage means storing scores for training texts and sequence scores for sequences of lexical units indicating the occurrence of the sequences in the training texts, wherein said sequence determining means is adapted to determine sequence scores for sequences of lexical units in the text to be classified, and said classification data determining means is adapted to compare the sequence scores for the training text and for the text to be classified to determine the scores for the text to be classified.\n10. Processing apparatus according to claim 9, wherein said storage means stores the sequence scores associated with scores for the training texts.\n11. Processing apparatus according to claim 10, wherein said storage means stores the sequence scores for groups of scores for the training texts, and said classification data determining means is adapted to determine a group score for each group by comparing the sequence scores for the training text and for the text to be classified, and to determine the scores for the text to be classified from the group scores.\n12. Processing apparatus according to claim 11, wherein the groups of scores comprise a mid range group of mid range scores and at least one other group of scores above and below the mid range group.\n13. Processing apparatus according to claim 1, wherein said classification data determining means is adapted to determine the scores for the text to be classified by attaching more weight to the comparison of longer sequences of lexical units than to shorter sequences of lexical units.\n14. A method of generating classification data for text, the method comprising:\nidentifying semantic content bearing lexical units in data representing the text to be classified;\ndetermining sequences of the identified lexical units; and\ndetermining means for determining classification data as a score for the text to be classified with respect to each of a plurality of qualities by comparing the determined sequences of the identified lexical units with stored sequences of lexical units for training texts having scores associated therewith for a plurality of qualities.\n15. A method according to claim 14, wherein the stored sequences of lexical units are stored as at least one sequence of lexical units starting from each consecutive semantic content bearing lexical unit in data representing training text, at least one sequence of lexical units is determined starting from each consecutive semantic content bearing lexical unit in the text to be classified, and the scores are determined by comparing said at least one sequence starting from each consecutive semantic content bearing lexical unit in data representing the text to be classified with said at least one stored sequence starting from each consecutive semantic content bearing lexical unit in data representing the training text.\n16. A method according to claim 15, wherein said at least one sequence of lexical units comprises a sequence of previous lexical units.\n17. A method according to claim 15, wherein said at least one sequence of lexical units comprise sequences of 1 to n lexical units, where n is an integer greater than 1.\n18. A method according to claim 15, wherein sequences of lexical units are determined in which the first lexical unit in said at least one sequence is not a common lexical unit or a modifying lexical unit that modifies the meaning of a subsequent lexical unit, and subsequent lexical units in said at least one sequence can be a modifying lexical unit.\n19. A method according to claim 15, wherein said at least one sequence of lexical units is determined starting at the beginning of each sentence in the text to be classified so that said at least one sequence of lexical units does not include lexical units from another sentence and sequences of lexical units starting with lexical units at the beginning of sentences can include identifiers in the sequence to identify that there is no word in a position in the sequence.\n20. A method according to claim 14, wherein said at least one sequence of lexical units further includes a single semantic content bearing lexical unit.\n21. A method according to claim 14, wherein semantic content bearing lexical units are identified by rejecting common words, and words are stemmed to provide the semantic content bearing lexical units as word stems.\n22. A method according to claim 14, including storing scores for training texts and sequence scores for sequences of lexical units indicating the occurrence of the sequences in the training texts, wherein sequence scores for sequences of lexical units in the text to be classified are determined, and the sequence scores for the training text are compared to the sequenced scores for the text to be classified to determine the scores for the text to be classified.\n23. A method according to claim 22, wherein the sequence scores associated with scores for the training texts are stored.\n24. A method according to claim 23, wherein the sequence scores for groups of scores for the training texts are stored, a group score is determined for each group by comparing the sequence scores for the training text and for the text to be classified, and the scores for the text to be classified are determined from the group scores.\n25. A method according to claim 24, wherein the groups of scores comprise a mid range group of mid range scores and at least one other group of scores above and below the mid range group.\n26. A method according to claim 14, wherein the scores for the text to be classified are determined by attaching more weight to the comparison of longer sequences of lexical units than to shorter sequences of lexical units.\n27. Processing apparatus for generating classification data for text, the processing apparatus comprising:\nprogram memory storing processor readable program code; and\na processor for reading and executing the program code;\nwherein the program code comprises code to control the processor to:\nidentify semantic content bearing lexical units in data representing the text to be classified;\ndetermine sequences of the identified lexical units; and\ndetermine means for determining classification data as a score for the text to be classified with respect to each of a plurality of qualities by comparing the determined sequences of the identified lexical units with stored sequences of lexical units for training texts having scores associated therewith for a plurality of qualities.\n28. Processing apparatus according to claim 27, including storage storing the stored sequences of lexical units as at least one sequence of lexical units starting from each consecutive semantic content bearing lexical unit in data representing training text, wherein the program code comprises code to control the processor to determine at least one sequence of lexical units starting from each consecutive semantic content bearing lexical unit in the text to be classified, and to determine the scores by comparing said at least one sequence starting from each consecutive semantic content bearing lexical unit in data representing the text to be classified with said at least one stored sequence starting from each consecutive semantic content bearing lexical unit in data representing the training text.\n29. Processing apparatus according to claim 28, wherein said at least one sequence of lexical units comprises a sequence of previous lexical units.\n30. Processing apparatus according to claim 28, wherein said at least one sequence of lexical units comprise sequences of 1 to n lexical units, where n is an integer greater than 1.\n31. Processing apparatus according to claim 28, wherein the program code comprises code to control the processor to determine sequences of lexical units in which the first lexical unit in said at least one sequence is not a common lexical unit or a modifying lexical unit that modifies the meaning of a subsequent lexical unit, and subsequent lexical units in said at least one sequence can be a modifying lexical unit.\n32. Processing apparatus according to claim 28, wherein the program code comprises code to control the processor to determine said at least one sequence of lexical units staring at the beginning of each sentence in the text to be classified so that said at least one sequence of lexical units does not include lexical units from another sentence and sequences of lexical units starting with lexical units at the beginning of sentences can include identifiers in the sequence to identify that there is no word in a position in the sequence.\n33. Processing apparatus according to claim 27, wherein said at least one sequence of lexical units further includes a single semantic content bearing lexical unit.\n34. Processing apparatus according to claim 27, wherein the program code comprises code to control the processor to identify semantic content bearing lexical units by rejecting common words, and words are stemmed to provide the semantic content bearing lexical units as word stems.\n35. Processing apparatus according to claim 27, wherein the program code comprises code to control the processor to store scores for training texts and sequence scores for sequences of lexical units indicating the occurrence of the sequences in the training texts, to determine sequence scores for sequences of lexical units in the text to be classified, and to compare the sequence scores for the training text to the sequenced scores for the text to be classified to determine the scores for the text to be classified.\n36. Processing apparatus according to claim 35, wherein the sequence scores associated with scores for the training texts are stored.\n37. Processing apparatus according to claim 36, wherein the program code comprises code to control the processor to store the sequence scores for groups of scores for the training texts, to determine a group score for each group by comparing the sequence scores for the training text and for the text to be classified, and to determine the scores for the text to be classified from the group scores.\n38. Processing apparatus according to claim 37, wherein the groups of scores comprise a mid range group of mid range scores and at least one other group of scores above and below the mid range group.\n39. Processing apparatus according to claim 27, wherein the program code comprises code to control the processor to determine the scores for the text to be classified by attaching more weight to the comparison of longer sequences of lexical units than to shorter sequences of lexical units.\n40. Processing apparatus for generating classification data for text, the processing apparatus comprising:\nidentifying means for identifying semantic content bearing lexical units in data representing the text to be classified; and\nclassification data determining means for determining classification data as a score for the text to be classified with respect to each of a plurality of qualities by comparing the identified lexical units with stored lexical units having a distribution of lexical scores associated therewith for each of a plural of qualities.\n41. Processing apparatus according to claim 40, including storage means storing said distribution of lexical scores for each of the plurality of qualities, the distribution having been obtained from training data.\n42. Processing apparatus according to claim 40, wherein said classification data determining means is adapted to determine the score for the text to be classified by statistical analysis of the result of the comparison.\n43. Processing apparatus according to claim 40, including sequence determining means for determining sequences of the identified lexical units; wherein said classification data determining means is adapted to determine the score by comparing the determined sequences of the identified lexical units with stored sequences of lexical units for training texts having score distributions associated therewith for the plurality of qualities.\n44. Processing apparatus for generating classification data for text, the processing apparatus comprising:\nprogram memory storing processor readable program code; and\na processor for reading and executing the program code;\nwherein the program code comprises code to control the processor to:\nidentify semantic content bearing lexical units in data representing the text to be classified; and\ndetermine classification data as a score for the text to be classified with respect to each of a plurality of qualities by comparing the identified lexical units with stored lexical units having a distribution of lexical scores associated therewith for each of a plurality of qualities.\n45. Processing apparatus according to claim 44, including storage storing said distribution of lexical scores for each of the plurality of qualities, the distribution having been obtained from training data.\n46. Processing apparatus according to claim 44, wherein the program code comprises code to control the processor to determine the score for the text to be classified by statistical analysis of the result of the comparison.\n47. Processing apparatus according to claim 44, wherein the program code comprises code to control the processor to determine sequences of the identified lexical units; and to determine the score by comparing the determined sequences of the identified lexical units with stored sequences of lexical units for training texts having score distributions associated therewith for the plurality of qualities.\n48. A method of generating classification data for text, the method comprising:\nidentifying semantic content bearing lexical units in data representing the text to be classified; and\ndetermining classification data as a score for the text to be classified with respect to each of a plurality of qualities by comparing the identified lexical units with stored lexical units having a distribution of lexical scores associated therewith for each of a plurality of qualities.\n49. A method according to claim 48, including storing said distribution of lexical scores for each of the plurality of qualities, the distribution having been obtained from training data.\n50. A method according to claim 48, wherein the score for the text to be classified is determined by statistical analysis of the result of the comparison.\n51. A method according to claim 48, including determining sequences of the identified lexical units; wherein the score is determined by comparing the determined sequences of the identified lexical units with stored sequences of lexical units for training texts having score distributions associated therewith for the plurality of qualities.\n52. Processing apparatus for generating classification data in a hierarchical structure for text, the processing apparatus comprising:\nthe processing apparatus according to claim 1 or claim 40;\nwherein said classification data determining means is adapted select a quality having the highest score and to repeat the determination of a score for a set of qualities dependent upon the selected quality.\n53. Processing apparatus according to claim 52, wherein said classification data determining means is adapted to use a sub set of the stored training texts dependant upon the selected quality for the repeated determination.\n54. A method of generating classification data in a hierarchical structure for text, the method comprising:\nthe method of claim 14 or claim 48; and\nselecting a quality having the highest score and repeating the determination of a score for a set of qualities dependent upon the selected quality.\n55. A method according to claim 54, wherein a sub set of the stored training texts dependant upon the selected quality is used for the repeated determination.\n56. Processing apparatus according to claim 1 or claim 40, including training data modifying means for modifying the training data using the classification data if confidence in the classification is high.\n57. A method of claim 14 or claim 48, including modifying the training data using the classification data if confidence in the classification is high.\n58. An automatic text classification system comprising:\nmeans for extracting word stems and word stem sequences from data representing a text to be classified;\nmeans for calculating a probability value for the text to be classified with respect to each of a plurality of qualities based on a correlation between (i) the extracted word stems and word stem sequences and (ii) predetermined training data.\n59. The automatic text classification system according to claim 58, wherein each quality is represented by an axis whose two end points correspond to mutually exclusive characteristics.\n60. The automatic text classification system according to claim 59, wherein the probability value with respect to each of the plurality of qualities is converted into a score on each axis indicating a likelihood of the text having one or the other of the mutually exclusive characteristics.\n61. The automatic text classification system according to claim 58, wherein the training data is derived from a plurality of training texts that have been pre-classified with respect to each of the plurality of qualities, and the training data comprises a distribution value of each word stem and each word stem sequence in each of the plurality of training texts with respect to each of the plurality of qualities.\n62. The automatic text classification system according to claim 61, wherein:\neach quality is represented by an axis that is divided into a plurality of groups and whose two end points correspond to mutually exclusive characteristics; each of the training texts has been pre-classified into one of the groups on each axis;\nthe training data comprises a database of, for each group on each axis, the distribution value of each word stem and word stem sequence in each training text with respect to the one group on each axis into which each training text has been pre-classified;\nthe distribution values represent a probability of each word stem and word stem sequence existing in a group on a given axis; and\nthe probability values of the text to be classified represent a probability of the text being classified in each group on each axis.\n63. The automatic text classification system according to claim 62, wherein each of the training texts has been pre-classified with a specific score on each axis, and each group on each axis comprises a predetermined range of scores.\n64. The automatic text classification system according to claim 63, wherein the training texts are selected so that the pre-classified scores are distributed along each axis between a Bell curve and a flat distribution.\n65. The automatic text classification system according to claim 63, wherein:\neach axis is divided into a first group, a neutral second group, and a third group; and\nthe neutral second group with respect to the pre-classification of the training texts is broader than the neutral second group with respect to the text to be classified, so that the probability values of the text to be classified are more likely to be converted into scores which fall on an appropriate side of each axis.\n66. The automatic text classification system according to claim 58, wherein:\neach word stem is a main stem word that is not a common word;\na modifying word is a common word that adds meaning to a main stem word; and\neach word stem sequence comprises a main stem word and one or more previous words that are either modifying words or other main stem words.\n67. The automatic text classification system according to claim 66, wherein the probability values are calculated such that a correlation between an extracted triple word stem sequence with the training data is more heavily weighted than a correlation between an extracted double word stem sequence with the training data, and such that a correlation between an extracted double word stem sequence with the training data is more heavily weighted than a correlation between a single extracted word stem with the training data.\n68. A system for producing training data comprising:\nmeans for extracting word stems and word stem sequences from each of a plurality of training texts that have been pre-classified with respect to each of a plurality of qualities; and\nmeans for calculating a distribution value of each extracted word stem and word stem sequence in each training text with respect to each of the plurality of qualities.\n69. The system for producing training data according to claim 68, wherein each quality is represented by an axis whose two end points correspond to mutually exclusive characteristics.\n70. The system for producing training data according to claim 68, wherein:\neach quality is represented by an axis that is divided into a plurality of groups and whose two end points correspond to mutually exclusive characteristics;\neach of the training texts has been pre-classified into one of the groups on each axis;\nthe training data comprises a database of, for each group on each axis, a distribution value of each word stem and word stem sequence in each training text with respect to the one group on each axis into which each training text has been pre-classified; and\nthe distribution values represent a plurality of each word stem and word stem sequence existing in a given group on a given axis.\n71. The system for producing training data according to claim 70, wherein each of the training texts has been pre-classified with a specific score on each axis, and each group on each axis comprises a predetermined range of scores.\n72. The system for producing training data according to claim 71, wherein the training texts are selected so that the pre-classified scores are distributed along each axis between a Bell curve and a flat distribution.\n73. The system for producing training data according to claim 68, wherein:\neach word stem is a main stem word that is not a common word;\na modifying word is a common word that adds meaning to a main stem word; and\neach word stem sequence comprises a main stem word and one or more previous words that are either modifying words or other main stem words.\n74. The system for producing training data according to claim 68, further comprising:\nmeans for, after a plurality of new texts have been classified with respect to the plurality of qualities using the training data, selecting a number of the new texts that have been classified with a predetermined degree of probability with respect to at least one of the plurality of qualities;\nmeans for extracting word stems and word stem sequences from each of the selected new texts; and\nmeans for one of (i) recalculating the distribution value of each extracted word stem and word stem sequence which is already present in the training data, and (ii) calculating an initial distribution value of each extracted word and word stem sequence which is not already present in the training data.\n75. A retrieval system comprising:\nmeans for accessing a data store comprising a plurality of word stems and word stem sequences that have been extracted from a plurality of texts, a plurality of identifiers associating each word stem and word stem sequence with at least one of the plurality of texts, and correlation data between (i) each word stem and word stem sequence and (ii) each of a plurality of qualities in terms of which the plurality of texts have been classified;\nmeans for receiving user preference data in terms of at least one of the plurality of qualities;\nmeans for identifying word stems and word stem sequences corresponding to the user preference data based on the correlation data stored in the data store using fuzzy logic; and\nmeans for identifying at least one of the plurality of texts that best matches the user preference data based on the identified word stems and word stem sequences and the plurality of identifiers stored in the data store.\n76. The retrieval system according to claim 75, wherein each quality is represented by an axis whose two end points represent mutually exclusive characteristics.\n77. The retrieval system according to claim 75, wherein:\neach quality is represented by an axis that is divided into a plurality of groups and whose two end points correspond to mutually exclusive characteristics;\neach of the plurality of texts has been classified into one of the groups on each axis;\nthe correlation data comprises, for each group on each axis, a distribution value of each word stem and word stem sequence in each text with respect to the one group on each axis into which each text has been classified; and\nthe distribution values represent a probability of each word stem and word stem sequence existing in a given group on a given axis.\n78. The retrieval system according to claim 77, wherein:\neach word stem is a main stem word that is not a common word;\na modifying word is a common word that adds meaning to a main stem word; and\neach word stem sequence comprises a main stem word and one or more previous words that are either modifying words or other main stem words.\n79. The retrieval system according to claim 75, further comprising a graphical user interface for enabling input of the user preference data.\n80. A system for producing training data comprising:\nmeans for identifying lexical units and lexical unit sequences from each of a plurality of training texts that have been pre-classified with respect to each of a plurality of qualities; and\nmeans for calculating a distribution value of each identified lexical unit and lexical unit sequence in each training text with respect to each of the plurality of qualities.\n81. A method of producing training data comprising:\nidentifying lexical units and lexical unit sequences from each of a plurality of training texts that have been pre-classified with respect to each of a plurality of qualities; and\ncalculating a distribution value of each identified lexical unit and lexical unit sequence in each training text with respect to each of the plurality of qualities.\n82. A carrier medium carrying computer readable code for controlling a processor to carry out the method of any one of claims 14 to 26, 48 to 51, 54, 55 or 57.\n83. A carrier medium carrying computer readable code for controlling a computer to function as the system as claimed in any one of the claims 58 to 79.",
    "status": "Abandoned",
    "citations_own": [
        "US5325298A",
        "US5781879A",
        "US5905980A",
        "US5933822A",
        "US5941944A",
        "US6026359A",
        "US6038527A",
        "US6125362A",
        "US6161130A",
        "US6192360B1",
        "US6460034B1",
        "US6571225B1"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US20020120754A1",
        "US20020174095A1",
        "US20030050929A1",
        "US20030101181A1",
        "US20030167252A1",
        "US20030225763A1",
        "US20040006547A1",
        "US20040139058A1",
        "US20050038893A1",
        "US6938025B1",
        "US20050203899A1",
        "US20050223354A1",
        "US20050262039A1",
        "US20050278363A1",
        "US20060004732A1",
        "US6985950B1",
        "US20060085405A1",
        "US20060142993A1",
        "US20060167930A1",
        "US20070192309A1",
        "US20070260598A1",
        "US20070265996A1",
        "US20080282153A1",
        "US20080312906A1",
        "US20080310718A1",
        "US20080312905A1",
        "US20080312904A1",
        "US20090157714A1",
        "US20090307210A1",
        "US20100257127A1",
        "US20110268323A1",
        "US20120030149A1",
        "US8229864B1",
        "US8244651B1",
        "US8250009B1",
        "USRE43633E1",
        "US8311967B1",
        "WO2012158357A2",
        "US20120303359A1",
        "US20120310944A1",
        "US8364613B1",
        "US8370279B1",
        "US8380696B1",
        "US8438122B1",
        "US8443013B1",
        "US20130138641A1",
        "US20130138430A1",
        "US20130151235A1",
        "US8473431B1",
        "US8533224B2",
        "US8577718B2",
        "US8595154B2",
        "US8640017B1",
        "US8694540B1",
        "US8862580B1",
        "US20140316850A1",
        "US20150019211A1",
        "US20150026104A1",
        "US8952796B1",
        "US9020807B2",
        "JP2015088067A",
        "US20150309989A1",
        "USD744516S1",
        "USD745879S1",
        "US9269353B1",
        "US20160055251A1",
        "US9361382B2",
        "US9454563B2",
        "US9454562B2",
        "US9483159B2",
        "US9630090B2",
        "US9643722B1",
        "US9667513B1",
        "WO2017107696A1",
        "US9892109B2",
        "JP2018106390A",
        "USD824920S1",
        "US10169424B2",
        "US10380258B2",
        "US10387564B2",
        "US20200004870A1",
        "US20200050962A1",
        "CN111078877A",
        "USD886137S1",
        "US10795917B2",
        "US20200342312A1",
        "US10896671B1",
        "USD918231S1",
        "US20210201893A1",
        "US11290530B2",
        "US11288328B2",
        "US11341330B1",
        "US11561684B1",
        "US11561986B1",
        "US11615126B2",
        "US11651032B2",
        "US11664043B2",
        "US11670286B2",
        "US11741301B2",
        "US11755595B2",
        "US11816435B1",
        "US11816438B2"
    ],
    "citedby_ftf": []
}