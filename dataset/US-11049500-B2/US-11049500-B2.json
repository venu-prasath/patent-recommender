{
    "patent_id": "US-11049500-B2",
    "title": "Adversarial learning and generation of dialogue responses ",
    "assignee": "Capital One Services, Llc",
    "publication_date": "2021-06-29",
    "patent_link": "https://patents.google.com/patent/US11049500B2/en",
    "inventors": [
        "Oluwatobi Olabiyi",
        "Erik T. Mueller"
    ],
    "classifications": [
        "G10L15/22",
        "G06F40/35",
        "G06N20/00",
        "G06N3/006",
        "G06N3/044",
        "G06N3/0445",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/047",
        "G06N3/0472",
        "G06N3/08",
        "G10L13/027",
        "G10L15/26"
    ],
    "abstract": "Systems and methods for generating responses to user input such as dialogues, and images are discussed. The system may generate, by a response generation module of at least one server, an optimal generated response to the user communication by applying an generative adversarial network. In some embodiments, the generative adversarial network may include a hierarchical recurrent encoder decoder generative adversarial network including a generator and a discriminator component.",
    "claims": "\n1. A method for training a response generator comprising:\nproviding a response generator of at least one server comprising a generative adversarial network including a generator component and a discriminator component, wherein the discriminator component comprises a convolutional neural network and a recurrent neural network;\nproviding training data to the response generator;\ntraining the generator component to generate a plurality of responses based on the training data;\ntraining the discriminator component to rank the responses from among the generated plurality of responses; and\nadjusting at least one component of the generator component based on the ranking of the generated responses.\n2. The method of claim 1, wherein the generative adversarial network comprises a hierarchical recurrent encoder-decoder adversarial network.\n3. The method of claim 1, wherein the training data comprises noise, historical data and user input.\n4. The method of claim 3, wherein the noise comprises Gaussian noise.\n5. The method of claim 3, wherein the user input comprises at least one of text and images.\n6. The method of claim 3, wherein the historical data comprises at least one of dialogue between characters in a movie or television show, and dialogue between a user and customer service provider.\n7. The method of claim 1, wherein the ranking of the generated plurality of responses is based on at least one of grammatical correctness, responsivity and relevancy to the user query.\n8. The method of claim 7, wherein the ranking of the generated plurality of responses utilizes a stochastic gradient descent algorithm.\n9. The method of claim 1, wherein the discriminator comprises a word-level classification component and a utterance-level classification component.\n10. An improved system for generating responses to user communications comprising:\nat least one server communicatively coupled to a user device by a network, wherein the at least one server further comprises a non-transitory memory storing computer-readable instructions and at least one processor;\nthe execution of the computer-readable instructions causing the at least one server to:\ntrain a response generator of the server, wherein the response generator comprises a generative adversarial network having a generator component and a discriminator component, the discriminator component comprising a convolutional neural network and a recurrent neural network wherein training the response generator causes the at least one server to:\nreceive training data;\ngenerate, by the generator component, a plurality of responses to training data;\nrank, by the discriminator component, the generated plurality of responses; and\nadjust at least one parameter of the generator component based on the ranking of the generated computer responses.\n11. The system of claim 10, wherein the generative adversarial network comprises a hierarchical recurrent encoder-decoder adversarial network.\n12. The system of claim 10, wherein the training data comprises noise, historical data and user input.\n13. The system of claim 12, wherein the noise comprises Gaussian noise.\n14. The system of claim 12, wherein the user input comprises at least one of text and images.\n15. The system of claim 12, wherein the historical data comprises at least one of dialogue between characters in a movie or television show, and dialogue between a user and customer service provider.\n16. The system of claim 10, wherein the ranking of the generated plurality of responses is based on at least one of grammatical correctness, responsivity and relevancy to the user query.\n17. The system of claim 16, wherein the ranking of the generated plurality of responses utilizes a stochastic gradient descent algorithm.\n18. The system of claim 10, wherein the discriminator comprises a word-level classification component and a utterance-level classification component.\n19. A non-transitory computer-readable medium storing instructions for\nperforming, when executed by a processor, a method of training a response generator, the method comprising:\nproviding a response generator of at least one server comprising a generative adversarial network including a generator component and a discriminator component, wherein the discriminator component comprises a convolutional neural network and a recurrent neural network;\nproviding training data to the response generator;\ntraining the generator component to generate a plurality of responses based on the training data;\ntraining the discriminator component to rank the responses from among the generated plurality of responses; and\nadjusting at least one component of the generator component based on the ranking of the generated responses.\n20. The computer-readable medium of claim 19, wherein the generative adversarial network comprises a hierarchical recurrent encoder-decoder adversarial network.",
    "status": "Active",
    "citations_own": [
        "US7606714B2",
        "US20170031920A1",
        "US20170278135A1",
        "US9786084B1",
        "US20180005631A1",
        "US20180047406A1",
        "US20180075581A1",
        "US20180082150A1",
        "US20180314932A1",
        "US20190080205A1",
        "US20190122101A1",
        "US20190236148A1"
    ],
    "citations_ftf": [
        "US11599729B2",
        "US10152970B1"
    ],
    "citedby_own": [
        "US20200175383A1",
        "US20200372898A1",
        "US20220058273A1",
        "US11281976B2",
        "US20220335219A1",
        "US11481416B2"
    ],
    "citedby_ftf": [
        "US10672164B2",
        "US10614557B2",
        "US10152970B1",
        "US10691894B2",
        "US10755391B2",
        "US11797864B2",
        "US11341983B2",
        "US11151334B2",
        "US11663483B2",
        "CN109584846B",
        "CN109657156B",
        "CN109885667A",
        "CN110147435B",
        "US10872297B2",
        "US10776720B2",
        "US11763129B2",
        "CN110070587B",
        "WO2020188507A1",
        "CN110070174B",
        "EP3745412A1",
        "EP3977392A4",
        "CN110379418B",
        "US11269622B2",
        "CN110443355B",
        "CN110598846B",
        "US10878008B1",
        "CN110705399A",
        "US11586912B2",
        "CN110706692B",
        "CN110795549B",
        "CN110808057A",
        "CN112133293A",
        "US11710046B2",
        "CN111160512B",
        "CN111123927A",
        "CN111276132A",
        "CN111860351B",
        "CN111737439B",
        "CN111914552A",
        "US20220058444A1",
        "CN112560438A",
        "KR102372642B1",
        "US11550991B2",
        "US11550831B1",
        "US20230140142A1"
    ]
}