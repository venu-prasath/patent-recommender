{
    "patent_id": "US-11670304-B2",
    "title": "Speaker recognition in the call center ",
    "assignee": "Pindrop Security, Inc.",
    "publication_date": "2023-06-06",
    "patent_link": "https://patents.google.com/patent/US11670304B2/en",
    "inventors": [
        "Elie Khoury",
        "Matthew Garland"
    ],
    "classifications": [
        "G10L17/00",
        "H04M1/271",
        "G06N7/01",
        "G10L15/07",
        "G10L15/19",
        "G10L15/26",
        "G10L17/04",
        "G10L17/08",
        "G10L17/24",
        "H04M2203/40"
    ],
    "abstract": "Utterances of at least two speakers in a speech signal may be distinguished and the associated speaker identified by use of diarization together with automatic speech recognition of identifying words and phrases commonly in the speech signal. The diarization process clusters turns of the conversation while recognized special form phrases and entity names identify the speakers. A trained probabilistic model deduces which entity name(s) correspond to the clusters.",
    "claims": "\n1. A computer-implemented method comprising:\nextracting, by a computer, from one or more audio signals, a first set of audio features associated with a first speaker and a second set of audio features associated with a second speaker;\ngenerating, by the computer, a first cluster associated with the first speaker based upon the first set of features;\ngenerating, by the computer, a second cluster associated with the second speaker based upon the second set of features;\nextracting, by the computer, from each respective set of audio features an entity name and a phrase; and\ngenerating, by the computer, a trained probabilistic model for each respective speaker based upon the respective cluster, the respective entity name, and the respective phrase.\n2. The method according to claim 1, further comprising executing, by the computer, a speech activity detector on an audio signal, the speech activity detector generating one or more speech portions and removing non-speech portions.\n3. The method according to claim 1, further comprising performing, by the computer, speaker diarization on an audio signal, the speaker diarization generating the first cluster and the second cluster.\n4. The method according to claim 1, wherein the computer extracts the one or more features from the one or more audio signals at a given interval, and wherein the computer partitionally clusters the one or more features into the respective clusters for the first speaker and the second speaker at the given interval.\n5. The method according to claim 1, wherein extracting the entity name and the phrase comprises:\nidentifying, by the computer, text content in each of the clusters extracted from an audio signal by executing an automatic speech recognition process configured to recognize the text content of speech in each of the clusters,\nwherein the computer extracts each entity name and each phrase from the text content identified in the first cluster and in the second cluster.\n6. The method according to claim 5, wherein at least one phrase is associated with a respective prior probability that an adjacent utterance is the entity name.\n7. The method according to claim 1, further comprising:\nreceiving, by the computer, a second audio signal involving at least one of the first speaker and the second speaker; and\ngenerating, by the computer, a next cluster using the probabilistic model associated with the first speaker or the probabilistic model associated with the second speaker.\n8. The method according to claim 1, further comprising identifying, by the computer, at least one of the first speaker and the second speaker as a caller based upon at least one of the trained probabilistic models generated by the computer.\n9. The method according to claim 1, further comprising identifying, by the computer, at least one of the first speaker and the second speaker as an agent based upon at least one of the trained probabilistic models generated by the computer.\n10. The method according to claim 1, wherein the first set of audio features and the second set of audio features each comprise at least one of: mel-frequency cepstral coefficients (MFCCs), linear predictive cepstral coefficients (LPCCs), and perceptual linear prediction (PLP).\n11. A system comprising:\na non-transitory storage medium storing a plurality of computer program instructions; and\na processor electrically coupled to the non-transitory storage medium and configured to execute the computer program instructions to:\nextract from one or more audio signals, a first set of audio features associated with a first speaker and a second set of audio features associated with a second speaker;\ngenerate a first cluster associated with the first speaker based upon the first set of features;\ngenerate a second cluster associated with the second speaker based upon the second set of features;\nextract from each respective set of audio features, an entity name and a phrase; and\ngenerate a trained probabilistic model for each respective speaker based upon the respective cluster, the respective entity name, and the respective phrase.\n12. The system according to claim 11, wherein the processor is further configured to:\nexecute a speech activity detector an audio signal, the speech activity detector configured to generate one or more speech portions and removing non-speech portions.\n13. The system according to claim 11, wherein the processor is further configured to:\nperform speaker diarization on an audio signal, the speaker diarization configured to generate the first cluster and the second cluster.\n14. The system according to claim 11, wherein the processor is configured to extract the one or more features from the one or more audio signals at a given interval, and wherein the processor is configured to partitionally cluster the one or more features into the respective clusters for the first speaker and the second speaker at the given interval.\n15. The system according to claim 11, wherein to extract the entity name and the phrase the processor is configured to:\nidentify text content in each of the clusters extracted from an audio signal by executing an automatic speech recognition process configured to recognize the text content of speech in each of the clusters,\nwherein the processor extracts each entity name and each phrase from the text content identified in the first cluster and in the second cluster.\n16. The system according to claim 15, wherein at least one phrase is associated with a respective prior probability that an adjacent utterance is the entity name.\n17. The system according to claim 11, wherein the processor is further configured to:\nreceive a second audio signal involving at least one of the first speaker and the second speaker; and\ngenerate a next cluster using the probabilistic model associated with the first speaker or the probabilistic model associated with the second speaker.\n18. The system according to claim 11, wherein the processor is further configured to:\nidentify at least one of the first speaker and the second speaker as a caller based upon at least one of the trained probabilistic models generated by the computer.\n19. The system according to claim 11, wherein the processor is further configured to:\nidentify at least one of the first speaker and the second speaker as an agent based upon at least one of the trained probabilistic models generated by the computer.\n20. The system according to claim 11, wherein the first set of audio features and the second set of audio features each comprise at least one of: mel-frequency cepstral coefficients (MFCCs), linear predictive cepstral coefficients (LPCCs), and perceptual linear prediction (PLP)."
}