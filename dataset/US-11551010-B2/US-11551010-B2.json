{
    "patent_id": "US-11551010-B2",
    "title": "Generating replacement sentences for a particular sentiment ",
    "assignee": "Intuit, Inc.",
    "publication_date": "2023-01-10",
    "patent_link": "https://patents.google.com/patent/US11551010B2/en",
    "inventors": [
        "Manav Kohli",
        "Cindy Osmon",
        "Nicholas Roberts"
    ],
    "classifications": [
        "G06F40/30",
        "G06F40/284",
        "G06N20/00",
        "G06N3/044",
        "G06N3/045",
        "G06N3/08",
        "G06N5/041"
    ],
    "abstract": "Certain aspects of the present disclosure provide techniques for generating a replacement sentence with the same or similar meaning but a different sentiment than an input sentence. The method generally includes receiving a request for a replacement sentence and iteratively determining a next word of the replacement sentence word-by-word based on an input sentence. Iteratively determining the next word generally includes evaluating a set of words of the input sentence using a language model configured to output candidate sentences and evaluating the candidate sentences using a sentiment model configured to output sentiment scores for the candidates sentences. Iteratively determining the next word further includes calculating convex combinations for the candidate sentences and selecting an ending word of one of the candidate sentences as the next word of the replacement sentence. The method further includes transmitting the replacement sentence in response to the request for the replacement sentence.",
    "claims": "\n1. A method for transforming sentiment in text, comprising:\nreceiving a request for a replacement sentence based on an input sentence with a first sentiment, wherein the replacement sentence has a different sentiment;\nreceiving, from a language model based on the input sentence, a first set of candidate sentences, wherein:\neach candidate sentence of the first set of candidate sentences comprises a word from the input sentence and an ending word, and\nthe ending word of each candidate sentence of the first set of candidate sentences is different than each other candidate sentence in the first set of candidate sentences;\ndetermining a meaning of each respective candidate sentence of the first set based at least on each ending word of the respective candidate sentence of the first set and the word from the input sentence;\nreceiving, from a sentiment model based on the first set of candidate sentences, a sentiment of each candidate sentence of the first set of candidate sentences;\nchoosing a first candidate sentence of the first set of candidate sentences based on a meaning of the first candidate sentence and a sentiment of the first candidate sentence;\ncomparing the meaning of the first candidate sentence to a meaning of the input sentence;\ncomparing the sentiment of the first candidate sentence to a requested sentiment; and\nproviding the first candidate sentence in response to the request for the replacement sentence based on comparing the meaning of the first candidate sentence to the meaning of the input sentence and comparing the sentiment of the first candidate sentence to the requested sentiment.\n2. The method of claim 1, further comprising:\nrandomly initializing vector representations of words for use in training the language model; and\nduring training of the language model, refining the vector representations of words based on results of the training.\n3. The method of claim 1, wherein choosing the first candidate sentence of the first set of candidate sentences based on the meaning of the first candidate sentence and the sentiment of the first candidate sentence comprises:\ncalculating a convex combination for each candidate sentence of the first set based on the sentiment of each candidate sentence of the first set and the meaning of each respective candidate sentence of the first set; and\nchoosing the first candidate sentence based on a highest convex combination for the first candidate sentence.\n4. The method of claim 1, further comprising:\nreceiving a second request for a second replacement sentence with the requested sentiment; and\nproviding the first candidate sentence in response to receiving the second request.\n5. The method of claim 1, wherein the language model is a bi-directional long short-term memory (Bi-LSTM) model trained with a set of historical conversations between human support agents and users.\n6. The method of claim 5, further comprising training the language model by providing the language model with a series of fifteen word sequences taken from the set of historical conversations, wherein a label for each fifteen word sequence is a sixteenth word following the fifteen word sequence.\n7. The method of claim 1, wherein the sentiment model calculates the sentiment of each candidate sentence of the first set based on the requested sentiment, wherein the requested sentiment is:\nprovided with the input sentence, and\nis different from the first sentiment.\n8. The method of claim 1, wherein:\nproviding the first candidate sentence in response to the request for the replacement sentence comprises providing the first candidate sentence to the language model; and\nthe method further comprises:\nreceiving, based on the first candidate sentence and the input sentence, a second set of candidate sentences, wherein:\neach candidate sentence of the second set of candidate sentences comprises the first candidate sentence and an ending word, and\nthe ending word of each candidate sentence of the second set of candidate sentences is different than each other candidate sentence in the second set of candidate sentences;\nchoosing a second candidate sentence of the second set of candidate sentences based on a meaning of the second candidate sentence and a sentiment of the second candidate sentence; and\nproviding the second candidate sentence to a user associated with the requested sentiment.\n9. The method of claim 1, wherein providing the first candidate sentence in response to the request for the replacement sentence comprises providing the first candidate sentence to a user associated with the request.\n10. The method of claim 1, wherein:\ncomparing the meaning of the first candidate sentence to the meaning of the input sentence comprises determining if the meaning of the first candidate sentence meets a first threshold; and\ncomparing the sentiment of the first candidate sentence to the requested sentiment comprises determining if the meaning of the first candidate sentence meets a second threshold; and\nproviding the first candidate sentence in response to the request for the replacement sentence based on comparing the meaning of the first candidate sentence to the meaning of the input sentence and comparing the sentiment of the first candidate sentence to the requested sentiment comprises providing the first candidate sentence based on whether the meaning of the first candidate sentence meets the second threshold and whether the sentiment of the first candidate sentence meets the second threshold.\n11. The method of claim 1, further comprising:\ndetermining a maximum length for each candidate sentence of the first set of candidate sentences based on the input sentence, wherein no candidate sentence of the first set of candidate sentences exceeds the maximum length.\n12. A processing system, comprising:\na memory storing executable instructions; and\na processor configured to execute the executable instructions and cause the processing system to:\nreceive a request for a replacement sentence based on an input sentence with a first sentiment, wherein the replacement sentence has a different sentiment;\nreceive, from a language model based on the input sentence, a first set of candidate sentences, wherein:\neach candidate sentence of the first set of candidate sentences comprises a word from the input sentence and an ending word, and\nthe ending word of each candidate sentence of the first set of candidate sentences is different than each other candidate sentence in the first set of candidate sentences;\ndetermine a meaning of each respective candidate sentence of the first set based at least on each ending word of the respective candidate sentence of the first set and the word from the input sentence;\nreceive, from a sentiment model based on the first set of candidate sentences, a sentiment of each candidate sentence of the first set of candidate sentences;\nchoose a first candidate sentence of the first set of candidate sentences based on a meaning of the first candidate sentence and a sentiment of the first candidate sentence;\ndetermine whether the meaning of the first candidate sentence retains a meaning of the input sentence;\ndetermine a closeness between the sentiment of the first candidate sentence and a requested sentiment; and\nprovide the first candidate sentence in response to the request for the replacement sentence based on the meaning of the first candidate sentence retaining the meaning of the input sentence and the closeness between the sentiment of the first candidate sentence and the requested sentiment.\n13. The processing system of claim 12, wherein the processor is further configured to cause the processing system to:\nrandomly initialize vector representations of words for use in training the language model; and\nduring training of the language model, refine the vector representations of words based on results of the training.\n14. The processing system of claim 12, wherein the processor being configured to choose the first candidate sentence of the first set of candidate sentences based on the meaning of the first candidate sentence and the sentiment of the first candidate sentence comprises the processor being configured to:\ncalculate a convex combination for each candidate sentence of the first set based on the sentiment of each candidate sentence of the first set and the meaning of each respective candidate sentence of the first set; and\nchoose the first candidate sentence of the first set of candidate sentences based on a highest convex combination associated with the first candidate sentence.\n15. The processing system of claim 12, wherein the processor is further configured to cause the processing system to:\nreceive a second request for a second replacement sentence with the requested sentiment; and\nprovide the first candidate sentence in response to receiving the second request.\n16. The processing system of claim 12, wherein the language model is a bi-directional long short-term memory (Bi-LSTM) model trained with a set of historical conversations between human support agents and users.\n17. The processing system of claim 16, wherein the processor is further configured to cause the processing system to train the language model by providing the language model with a series of fifteen word sequences taken from the set of historical conversations, wherein a label for each fifteen word sequence is a sixteenth word following the fifteen word sequence.\n18. The processing system of claim 12, wherein the sentiment model calculates the sentiment of each candidate sentence of the first set of candidate sentences based on the requested sentiment, wherein the requested sentiment is:\nprovided with the input sentence, and\nis different from the first sentiment.\n19. The processing system of claim 12, wherein:\nthe processor being configured to provide the first candidate sentence in response to the request for the replacement sentence comprises the processor being configured to provide the first candidate sentence to the language model; and\nthe processor is further configured to cause the processing system to:\nreceive, based on the first candidate sentence and the input sentence, a second set of candidate sentence, wherein:\neach candidate sentence of the second set of candidate sentences comprises the first candidate sentence and an ending word, and\nthe ending word of each candidate sentence of the second set of candidate sentences is different than each other candidate sentence in the second set of candidate sentences;\nchoose a second candidate sentence of the second set of candidate sentences based on a meaning of the second candidate sentence and a sentiment of the second candidate sentence; and\nprovide the second candidate sentence to a user associated with the requested sentiment.\n20. A method for transforming sentiment in text, comprising:\nreceiving a request for a replacement sentence based on an input sentence with a first sentiment, wherein the replacement sentence has a different sentiment;\nreceiving, from a language model based on the input sentence, a first set of candidate sentences, wherein:\neach candidate sentence of the first set of candidate sentences comprises a word from the input sentence and an ending word, and\nthe ending word of each candidate sentence of the first set of candidate sentences is different than each other candidate sentence in the first set of candidate sentences;\nreceiving, from a sentiment model based on the first set of candidate sentences, a sentiment of each candidate sentence of the first set of candidate sentences;\ndetermine a semantic drift between the input sentence and each respective candidate sentence of the first set of candidate sentences based on comparing a vector distance between words of the input sentence and words of each respective candidate sentence;\nchoosing a first candidate sentence of the first set of candidate sentences based on a semantic drift of the first candidate sentence and a sentiment of the first candidate sentence;\ncomparing the semantic drift of the first candidate sentence to a threshold;\ncomparing the sentiment of the first candidate sentence to a requested sentiment; and\nproviding the first candidate sentence in response to the request for the replacement sentence based on comparing the semantic drift of the first candidate sentence to the threshold and comparing the sentiment of the first candidate sentence to the requested sentiment.",
    "status": "Active",
    "citations_own": [
        "US8037086B1",
        "US20150242391A1",
        "US20160283588A1",
        "US20170140755A1",
        "US20170147919A1",
        "US9812151B1",
        "US9875258B1",
        "US20190130289A1",
        "US20200012697A1",
        "US20200073947A1",
        "US20200110836A1",
        "US20200218781A1",
        "US20200250139A1",
        "US10778705B1",
        "US20210365643A1"
    ],
    "citations_ftf": [],
    "citedby_own": [],
    "citedby_ftf": [
        "KR20210097588A",
        "US11321526B2",
        "US11636850B2",
        "US11741296B2",
        "US20220318499A1",
        "US20220382979A1",
        "US11755626B1",
        "US11477142B1",
        "CN115062606B",
        "CN115811630B"
    ]
}