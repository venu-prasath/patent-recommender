{
    "patent_link": "https://patents.google.com/patent/US20200364569A1/en",
    "abstract": "Methods, systems, and apparatus, including computer programs encoded on computer storage media, for training a Q network used to select actions to be performed by an agent interacting with an environment. One of the methods includes obtaining a plurality of experience tuples and training the Q network on each of the experience tuples using the Q network and a target Q network that is identical to the Q network but with the current values of the parameters of the target Q network being different from the current values of the parameters of the Q network.",
    "claims": "1-18. (canceled)19. A method of training a Q network used to select actions to be performed by an agent that interacts with an environment by receiving observations characterizing states of the environment and performing actions from a set of actions in response to the observations,\nwherein the Q network is a deep neural network that is configured to receive as input an input observation and an input action and to generate an estimated future cumulative reward from the input in accordance with a set of parameters, and\nwherein the method comprises:\nobtaining a plurality of experience tuples, wherein each experience tuple includes a training observation, an action performed by the agent in response to receiving the training observation, a reward received in response to the agent performing the action, and a next training observation that characterizes a next state of the environment; and\ntraining the Q network on each of the experience tuples, comprising, for each experience tuple:\nprocessing the training observation in the experience tuple and the action in the experience tuple using the Q network to determine a current estimated future cumulative reward for the experience tuple in accordance with current values of the parameters of the Q network;\nfor each action in the set of actions, processing the next observation in the experience tuple and the action using the Q network to determine a respective next estimated future cumulative reward for the action in accordance with current values of the parameters of the Q network;\nselecting an action from the set of actions that has the highest next estimated future cumulative reward;\nprocessing the next observation in the experience tuple and the selected action using a target Q network to determine the next target estimated future cumulative reward for the selected action in accordance with the current values of the parameters of the target Q network, wherein the target Q network has a same neural network architecture as the Q network but the current values of the parameters of the Q network are different from the current values of the parameters of the target Q network;\ndetermining an error for the experience tuple from the reward in the experience tuple, the next target estimated future cumulative reward for the selected action, and the current estimated future cumulative reward; and\nusing the error for the experience tuple to update the current values of the parameters of the Q network.wherein the Q network is a deep neural network that is configured to receive as input an input observation and an input action and to generate an estimated future cumulative reward from the input in accordance with a set of parameters, andwherein the method comprises:\nobtaining a plurality of experience tuples, wherein each experience tuple includes a training observation, an action performed by the agent in response to receiving the training observation, a reward received in response to the agent performing the action, and a next training observation that characterizes a next state of the environment; and\ntraining the Q network on each of the experience tuples, comprising, for each experience tuple:\nprocessing the training observation in the experience tuple and the action in the experience tuple using the Q network to determine a current estimated future cumulative reward for the experience tuple in accordance with current values of the parameters of the Q network;\nfor each action in the set of actions, processing the next observation in the experience tuple and the action using the Q network to determine a respective next estimated future cumulative reward for the action in accordance with current values of the parameters of the Q network;\nselecting an action from the set of actions that has the highest next estimated future cumulative reward;\nprocessing the next observation in the experience tuple and the selected action using a target Q network to determine the next target estimated future cumulative reward for the selected action in accordance with the current values of the parameters of the target Q network, wherein the target Q network has a same neural network architecture as the Q network but the current values of the parameters of the Q network are different from the current values of the parameters of the target Q network;\ndetermining an error for the experience tuple from the reward in the experience tuple, the next target estimated future cumulative reward for the selected action, and the current estimated future cumulative reward; and\nusing the error for the experience tuple to update the current values of the parameters of the Q network.obtaining a plurality of experience tuples, wherein each experience tuple includes a training observation, an action performed by the agent in response to receiving the training observation, a reward received in response to the agent performing the action, and a next training observation that characterizes a next state of the environment; andtraining the Q network on each of the experience tuples, comprising, for each experience tuple:\nprocessing the training observation in the experience tuple and the action in the experience tuple using the Q network to determine a current estimated future cumulative reward for the experience tuple in accordance with current values of the parameters of the Q network;\nfor each action in the set of actions, processing the next observation in the experience tuple and the action using the Q network to determine a respective next estimated future cumulative reward for the action in accordance with current values of the parameters of the Q network;\nselecting an action from the set of actions that has the highest next estimated future cumulative reward;\nprocessing the next observation in the experience tuple and the selected action using a target Q network to determine the next target estimated future cumulative reward for the selected action in accordance with the current values of the parameters of the target Q network, wherein the target Q network has a same neural network architecture as the Q network but the current values of the parameters of the Q network are different from the current values of the parameters of the target Q network;\ndetermining an error for the experience tuple from the reward in the experience tuple, the next target estimated future cumulative reward for the selected action, and the current estimated future cumulative reward; and\nusing the error for the experience tuple to update the current values of the parameters of the Q network.processing the training observation in the experience tuple and the action in the experience tuple using the Q network to determine a current estimated future cumulative reward for the experience tuple in accordance with current values of the parameters of the Q network;for each action in the set of actions, processing the next observation in the experience tuple and the action using the Q network to determine a respective next estimated future cumulative reward for the action in accordance with current values of the parameters of the Q network;selecting an action from the set of actions that has the highest next estimated future cumulative reward;processing the next observation in the experience tuple and the selected action using a target Q network to determine the next target estimated future cumulative reward for the selected action in accordance with the current values of the parameters of the target Q network, wherein the target Q network has a same neural network architecture as the Q network but the current values of the parameters of the Q network are different from the current values of the parameters of the target Q network;determining an error for the experience tuple from the reward in the experience tuple, the next target estimated future cumulative reward for the selected action, and the current estimated future cumulative reward; andusing the error for the experience tuple to update the current values of the parameters of the Q network.20. The method of claim 19, wherein the error E satisfies:\n\nE=R+\u03b3*NTER\u2212CER,E=R+\u03b3*NTER\u2212CER,where R is the reward in the experience tuple, \u03b3 is a specified discount factor, NTER is the next target estimated future cumulative reward for the selected action, and CER is the current estimated future cumulative reward.21. The method of claim 19, wherein using the error for the experience tuple to update the current values of the parameters of the Q network comprises:\nupdating the current values of the parameters of the Q network to reduce the error using a machine learning training technique.updating the current values of the parameters of the Q network to reduce the error using a machine learning training technique.22. The method of claim 19, wherein the values of the parameters of the target Q network are periodically synchronized with the values of the parameters of the Q network.23. The method of claim 22, further comprising:\nafter training the Q network on each of the experience tuples to determine updated values of the parameters of the Q network, updating the current values of the target Q network to match the updated values of the parameters of the Q network.after training the Q network on each of the experience tuples to determine updated values of the parameters of the Q network, updating the current values of the target Q network to match the updated values of the parameters of the Q network.24. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform operations for training a Q network used to select actions to be performed by an agent that interacts with an environment by receiving observations characterizing states of the environment and performing actions from a set of actions in response to the observations,\nwherein the Q network is a deep neural network that is configured to receive as input an input observation and an input action and to generate an estimated future cumulative reward from the input in accordance with a set of parameters, and\nwherein the operations comprise:\nobtaining a plurality of experience tuples, wherein each experience tuple includes a training observation, an action performed by the agent in response to receiving the training observation, a reward received in response to the agent performing the action, and a next training observation that characterizes a next state of the environment; and\ntraining the Q network on each of the experience tuples, comprising, for each experience tuple:\nprocessing the training observation in the experience tuple and the action in the experience tuple using the Q network to determine a current estimated future cumulative reward for the experience tuple in accordance with current values of the parameters of the Q network;\nfor each action in the set of actions, processing the next observation in the experience tuple and the action using the Q network to determine a respective next estimated future cumulative reward for the action in accordance with current values of the parameters of the Q network;\nselecting an action from the set of actions that has the highest next estimated future cumulative reward;\nprocessing the next observation in the experience tuple and the selected action using a target Q network to determine the next target estimated future cumulative reward for the selected action in accordance with the current values of the parameters of the target Q network, wherein the target Q network has a same neural network architecture as the Q network but the current values of the parameters of the Q network are different from the current values of the parameters of the target Q network;\ndetermining an error for the experience tuple from the reward in the experience tuple, the next target estimated future cumulative reward for the selected action, and the current estimated future cumulative reward; and\nusing the error for the experience tuple to update the current values of the parameters of the Q network.wherein the Q network is a deep neural network that is configured to receive as input an input observation and an input action and to generate an estimated future cumulative reward from the input in accordance with a set of parameters, andwherein the operations comprise:\nobtaining a plurality of experience tuples, wherein each experience tuple includes a training observation, an action performed by the agent in response to receiving the training observation, a reward received in response to the agent performing the action, and a next training observation that characterizes a next state of the environment; and\ntraining the Q network on each of the experience tuples, comprising, for each experience tuple:\nprocessing the training observation in the experience tuple and the action in the experience tuple using the Q network to determine a current estimated future cumulative reward for the experience tuple in accordance with current values of the parameters of the Q network;\nfor each action in the set of actions, processing the next observation in the experience tuple and the action using the Q network to determine a respective next estimated future cumulative reward for the action in accordance with current values of the parameters of the Q network;\nselecting an action from the set of actions that has the highest next estimated future cumulative reward;\nprocessing the next observation in the experience tuple and the selected action using a target Q network to determine the next target estimated future cumulative reward for the selected action in accordance with the current values of the parameters of the target Q network, wherein the target Q network has a same neural network architecture as the Q network but the current values of the parameters of the Q network are different from the current values of the parameters of the target Q network;\ndetermining an error for the experience tuple from the reward in the experience tuple, the next target estimated future cumulative reward for the selected action, and the current estimated future cumulative reward; and\nusing the error for the experience tuple to update the current values of the parameters of the Q network.obtaining a plurality of experience tuples, wherein each experience tuple includes a training observation, an action performed by the agent in response to receiving the training observation, a reward received in response to the agent performing the action, and a next training observation that characterizes a next state of the environment; andtraining the Q network on each of the experience tuples, comprising, for each experience tuple:\nprocessing the training observation in the experience tuple and the action in the experience tuple using the Q network to determine a current estimated future cumulative reward for the experience tuple in accordance with current values of the parameters of the Q network;\nfor each action in the set of actions, processing the next observation in the experience tuple and the action using the Q network to determine a respective next estimated future cumulative reward for the action in accordance with current values of the parameters of the Q network;\nselecting an action from the set of actions that has the highest next estimated future cumulative reward;\nprocessing the next observation in the experience tuple and the selected action using a target Q network to determine the next target estimated future cumulative reward for the selected action in accordance with the current values of the parameters of the target Q network, wherein the target Q network has a same neural network architecture as the Q network but the current values of the parameters of the Q network are different from the current values of the parameters of the target Q network;\ndetermining an error for the experience tuple from the reward in the experience tuple, the next target estimated future cumulative reward for the selected action, and the current estimated future cumulative reward; and\nusing the error for the experience tuple to update the current values of the parameters of the Q network.processing the training observation in the experience tuple and the action in the experience tuple using the Q network to determine a current estimated future cumulative reward for the experience tuple in accordance with current values of the parameters of the Q network;for each action in the set of actions, processing the next observation in the experience tuple and the action using the Q network to determine a respective next estimated future cumulative reward for the action in accordance with current values of the parameters of the Q network;selecting an action from the set of actions that has the highest next estimated future cumulative reward;processing the next observation in the experience tuple and the selected action using a target Q network to determine the next target estimated future cumulative reward for the selected action in accordance with the current values of the parameters of the target Q network, wherein the target Q network has a same neural network architecture as the Q network but the current values of the parameters of the Q network are different from the current values of the parameters of the target Q network;determining an error for the experience tuple from the reward in the experience tuple, the next target estimated future cumulative reward for the selected action, and the current estimated future cumulative reward; andusing the error for the experience tuple to update the current values of the parameters of the Q network.25. The system of claim 24, wherein the error E satisfies:\n\nE=R+\u03b3*NTER\u2212CER,E=R+\u03b3*NTER\u2212CER,where R is the reward in the experience tuple, \u03b3 is a specified discount factor, NTER is the next target estimated future cumulative reward for the selected action, and CER is the current estimated future cumulative reward.26. The system of claim 24, wherein using the error for the experience tuple to update the current values of the parameters of the Q network comprises:\nupdating the current values of the parameters of the Q network to reduce the error using a machine learning training technique.updating the current values of the parameters of the Q network to reduce the error using a machine learning training technique.27. The system of claim 24, wherein the values of the parameters of the target Q network are periodically synchronized with the values of the parameters of the Q network.28. The system of claim 27, the operations further comprising:\nafter training the Q network on each of the experience tuples to determine updated values of the parameters of the Q network, updating the current values of the target Q network to match the updated values of the parameters of the Q network.after training the Q network on each of the experience tuples to determine updated values of the parameters of the Q network, updating the current values of the target Q network to match the updated values of the parameters of the Q network.29. One or more non-transitory computer-readable storage media encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform operations for training a Q network used to select actions to be performed by an agent that interacts with an environment by receiving observations characterizing states of the environment and performing actions from a set of actions in response to the observations,\nwherein the Q network is a deep neural network that is configured to receive as input an input observation and an input action and to generate an estimated future cumulative reward from the input in accordance with a set of parameters, and\nwherein the operations comprise:\nobtaining a plurality of experience tuples, wherein each experience tuple includes a training observation, an action performed by the agent in response to receiving the training observation, a reward received in response to the agent performing the action, and a next training observation that characterizes a next state of the environment; and\ntraining the Q network on each of the experience tuples, comprising, for each experience tuple:\nprocessing the training observation in the experience tuple and the action in the experience tuple using the Q network to determine a current estimated future cumulative reward for the experience tuple in accordance with current values of the parameters of the Q network;\nfor each action in the set of actions, processing the next observation in the experience tuple and the action using the Q network to determine a respective next estimated future cumulative reward for the action in accordance with current values of the parameters of the Q network;\nselecting an action from the set of actions that has the highest next estimated future cumulative reward;\nprocessing the next observation in the experience tuple and the selected action using a target Q network to determine the next target estimated future cumulative reward for the selected action in accordance with the current values of the parameters of the target Q network, wherein the target Q network has a same neural network architecture as the Q network but the current values of the parameters of the Q network are different from the current values of the parameters of the target Q network;\ndetermining an error for the experience tuple from the reward in the experience tuple, the next target estimated future cumulative reward for the selected action, and the current estimated future cumulative reward; and\nusing the error for the experience tuple to update the current values of the parameters of the Q network.wherein the Q network is a deep neural network that is configured to receive as input an input observation and an input action and to generate an estimated future cumulative reward from the input in accordance with a set of parameters, andwherein the operations comprise:\nobtaining a plurality of experience tuples, wherein each experience tuple includes a training observation, an action performed by the agent in response to receiving the training observation, a reward received in response to the agent performing the action, and a next training observation that characterizes a next state of the environment; and\ntraining the Q network on each of the experience tuples, comprising, for each experience tuple:\nprocessing the training observation in the experience tuple and the action in the experience tuple using the Q network to determine a current estimated future cumulative reward for the experience tuple in accordance with current values of the parameters of the Q network;\nfor each action in the set of actions, processing the next observation in the experience tuple and the action using the Q network to determine a respective next estimated future cumulative reward for the action in accordance with current values of the parameters of the Q network;\nselecting an action from the set of actions that has the highest next estimated future cumulative reward;\nprocessing the next observation in the experience tuple and the selected action using a target Q network to determine the next target estimated future cumulative reward for the selected action in accordance with the current values of the parameters of the target Q network, wherein the target Q network has a same neural network architecture as the Q network but the current values of the parameters of the Q network are different from the current values of the parameters of the target Q network;\ndetermining an error for the experience tuple from the reward in the experience tuple, the next target estimated future cumulative reward for the selected action, and the current estimated future cumulative reward; and\nusing the error for the experience tuple to update the current values of the parameters of the Q network.obtaining a plurality of experience tuples, wherein each experience tuple includes a training observation, an action performed by the agent in response to receiving the training observation, a reward received in response to the agent performing the action, and a next training observation that characterizes a next state of the environment; andtraining the Q network on each of the experience tuples, comprising, for each experience tuple:\nprocessing the training observation in the experience tuple and the action in the experience tuple using the Q network to determine a current estimated future cumulative reward for the experience tuple in accordance with current values of the parameters of the Q network;\nfor each action in the set of actions, processing the next observation in the experience tuple and the action using the Q network to determine a respective next estimated future cumulative reward for the action in accordance with current values of the parameters of the Q network;\nselecting an action from the set of actions that has the highest next estimated future cumulative reward;\nprocessing the next observation in the experience tuple and the selected action using a target Q network to determine the next target estimated future cumulative reward for the selected action in accordance with the current values of the parameters of the target Q network, wherein the target Q network has a same neural network architecture as the Q network but the current values of the parameters of the Q network are different from the current values of the parameters of the target Q network;\ndetermining an error for the experience tuple from the reward in the experience tuple, the next target estimated future cumulative reward for the selected action, and the current estimated future cumulative reward; and\nusing the error for the experience tuple to update the current values of the parameters of the Q network.processing the training observation in the experience tuple and the action in the experience tuple using the Q network to determine a current estimated future cumulative reward for the experience tuple in accordance with current values of the parameters of the Q network;for each action in the set of actions, processing the next observation in the experience tuple and the action using the Q network to determine a respective next estimated future cumulative reward for the action in accordance with current values of the parameters of the Q network;selecting an action from the set of actions that has the highest next estimated future cumulative reward;processing the next observation in the experience tuple and the selected action using a target Q network to determine the next target estimated future cumulative reward for the selected action in accordance with the current values of the parameters of the target Q network, wherein the target Q network has a same neural network architecture as the Q network but the current values of the parameters of the Q network are different from the current values of the parameters of the target Q network;determining an error for the experience tuple from the reward in the experience tuple, the next target estimated future cumulative reward for the selected action, and the current estimated future cumulative reward; andusing the error for the experience tuple to update the current values of the parameters of the Q network.30. The computer-readable storage media of claim 29, wherein the error E satisfies:\n\nE=R+\u03b3*NTER\u2212CER,E=R+\u03b3*NTER\u2212CER,where R is the reward in the experience tuple, \u03b3 is a specified discount factor, NTER is the next target estimated future cumulative reward for the selected action, and CER is the current estimated future cumulative reward.31. The computer-readable storage media of claim 29, wherein using the error for the experience tuple to update the current values of the parameters of the Q network comprises:\nupdating the current values of the parameters of the Q network to reduce the error using a machine learning training technique.updating the current values of the parameters of the Q network to reduce the error using a machine learning training technique.32. The computer-readable storage media of claim 29, wherein the values of the parameters of the target Q network are periodically synchronized with the values of the parameters of the Q network.33. The computer-readable storage media of claim 32, the operations further comprising:\nafter training the Q network on each of the experience tuples to determine updated values of the parameters of the Q network, updating the current values of the target Q network to match the updated values of the parameters of the Q network.after training the Q network on each of the experience tuples to determine updated values of the parameters of the Q network, updating the current values of the target Q network to match the updated values of the parameters of the Q network.34. The method of claim 19, further comprising:\nusing the trained Q network to control the agent.using the trained Q network to control the agent.35. The system of claim 24, the operations further comprising:\nusing the trained Q network to control the agent.using the trained Q network to control the agent.36. The computer-readable storage media of claim 29, the operations further comprising:\nusing the trained Q network to control the agent.using the trained Q network to control the agent.37. The method of claim 19, further comprising:\nproviding data specifying the trained Q network for use in controlling the agent.providing data specifying the trained Q network for use in controlling the agent.38. The system of claim 24, the operations further comprising:\nproviding data specifying the trained Q network for use in controlling the agent.providing data specifying the trained Q network for use in controlling the agent.",
    "patent_id": "US-2020364569-A1"
}