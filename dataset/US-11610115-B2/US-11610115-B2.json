{
    "patent_id": "US-11610115-B2",
    "title": "Learning to generate synthetic datasets for training neural networks ",
    "assignee": "Nvidia Corporation",
    "publication_date": "2023-03-21",
    "patent_link": "https://patents.google.com/patent/US11610115B2/en",
    "inventors": [
        "Amlan Kar",
        "Aayush Prakash",
        "Ming-Yu Liu",
        "David Jesus Acuna Marrero",
        "Antonio Torralba Barriuso",
        "Sanja Fidler"
    ],
    "classifications": [
        "G06N3/08",
        "G06F16/9024",
        "G06N20/20",
        "G06N3/045",
        "G06N3/0454",
        "G06T11/00",
        "G06T11/60",
        "G06V10/426",
        "G06V10/764",
        "G06V10/774",
        "G06V10/82",
        "G06N20/10",
        "G06N3/044",
        "G06N3/047",
        "G06N5/01",
        "G06N7/01",
        "G06T2210/61"
    ],
    "abstract": "In various examples, a generative model is used to synthesize datasets for use in training a downstream machine learning model to perform an associated task. The synthesized datasets may be generated by sampling a scene graph from a scene grammar\u2014such as a probabilistic grammar\u2014and applying the scene graph to the generative model to compute updated scene graphs more representative of object attribute distributions of real-world datasets. The downstream machine learning model may be validated against a real-world validation dataset, and the performance of the model on the real-world validation dataset may be used as an additional factor in further training or fine-tuning the generative model for generating the synthesized datasets specific to the task of the downstream machine learning model.",
    "claims": "\n1. A method comprising:\ncomputing, using at least one machine learning model and based at least in part on first data representative of one or more initial scene graphs, second data representative of one or more transformed scene graphs including one or more transformed attributes different from one or more initial attributes corresponding to the one or more initial scene graphs;\nrendering third data representative of one or more synthetic images based at least in part on the second data;\ndetermining at least one discrepancy by, at least in part, comparing one or more synthetic attribute distributions corresponding to the one or more synthetic images to one or more real-world attribute distributions corresponding to one or more real-world images;\nbased at least in part on the at least one discrepancy, generating fourth data representative of network update information; and\ntraining the at least one machine learning model using the fourth data.\n2. The method of claim 1, wherein at least one of the one or more synthetic attribute distributions or the one or more real-world attribute distributions are determined at least in part by:\napplying fifth data representative of at least one of the one or more real-world images or the one or more synthetic images to at least one other machine learning model trained for feature extraction; and\ndetermining at least one of the one or more synthetic attribute distributions or the one or more real-world attribute distributions based at least in part on output of the at least one other machine learning model.\n3. The method of claim 1, wherein one or more initial scene graphs are sampled from a probabilistic scene grammar.\n4. The method of claim 1, wherein the at least one machine learning model includes a graph convolutional network.\n5. The method of claim 1, wherein the at least one discrepancy includes a maximum mean discrepancy (MMD).\n6. The method of claim 1, further comprising:\ntraining at least one other machine learning model using the one or more synthetic images and corresponding ground truth;\ntesting the at least one other machine model on a real-world validation set;\ndetermining at least one performance score of the at least one other machine learning model based at least in part on the testing; and\nfurther training the at least one machine learning model based at least in part on the at least one performance score.\n7. The method of claim 6, wherein at least one first loss function is used for the training the at least one machine learning model using the fourth data and at least one second loss function is used for the training the at least one machine learning model based at least in part on the at least one performance score, the at least one first loss function being different from the at least one second loss function.\n8. The method of claim 6, wherein the ground truth is automatically generated based at least in part on the third data.\n9. The method of claim 1, wherein one or more parameters of the at least one machine learning model are initialized by, at least in part, training the at least one machine learning model to compute one or more updated scene graphs such that the one or more updated scene graphs are within a threshold similarity to the one or more initial scene graphs.\n10. The method of claim 1, wherein initializing one or more parameters of the at least one machine learning model is performed using at least one auto-encoder.\n11. A system comprising:\na computing device including one or more processing devices to cause instantiation of:\na distribution transformer to compute, using at least one machine learning model and based at least in part on first data representative of one or more initial scene graphs, second data representative of one or more transformed scene graphs each including one or more transformed attributes different from one or more initial attributes corresponding to the one or more initial scene graphs;\na renderer to render third data representative of one or more synthetic images based at least in part on the second data;\na discrepancy determiner to determine at least one discrepancy between one or more synthetic attribute distributions corresponding to the one or more synthetic images and one or more real-world attribute distributions corresponding to one or more real-world images; and\na model trainer to generate fourth data representative of network update information for training the at least one machine learning model, the network update information generated based at least in part on the third data.\n12. The system of claim 11, wherein the one or more processing devices are further to cause instantiation of:\na task network trainer to train at least one task network using the third data and the fourth data; and\na task network validator to determine at least one performance score for the at least one task network when processing at least one real-world validation set,\nwherein the model trainer further trains the at least one machine learning model based at least in part on the at least one performance score.\n13. The system of claim 11, wherein the distribution transformer includes at least one graph convolutional network.\n14. The system of claim 11, wherein the model trainer uses at least one first loss function to train the at least one machine learning model using the fourth data and at least one second loss function different from the at least one first loss function to train the at least one machine learning model based at least in part on at least one performance score.\n15. A processor comprising:\none or more circuits to:\ncompute, using at least one machine learning model and based at least in part on one or more initial scene graphs, one or more transformed scene graphs including one or more transformed attributes different from one or more initial attributes corresponding to the one or more initial scene graphs;\nrender one or more synthetic images based at least in part on the one or more transformed scene graphs;\ndetermine at least one discrepancy by, at least in part, comparing one or more synthetic attribute distributions corresponding to the one or more synthetic images to one or more real-world attribute distributions corresponding to one or more real-world images;\nbased at least in part on the at least one discrepancy, generate network update information; and\ntrain the at least one machine learning model using the network update information.\n16. The processor of claim 15, wherein at least one of the one or more synthetic attribute distributions or the one or more real-world attribute distributions are determined at least in part by:\napplying at least one of the one or more real-world images or the one or more synthetic images to at least one other machine learning model trained for feature extraction; and\ndetermining at least one of the one or more synthetic attribute distributions or the one or more real-world attribute distributions based at least in part on output of the at least one other machine learning model.\n17. The processor of claim 15, wherein one or more initial scene graphs are sampled from a probabilistic scene grammar.\n18. The processor of claim 15, wherein the at least one machine learning model includes a graph convolutional network.\n19. The processor of claim 15, wherein the at least one discrepancy includes a maximum mean discrepancy (MMD).\n20. The processor of claim 15, wherein the one or more circuits are further to:\ntrain at least one other machine learning model using the one or more synthetic images and corresponding ground truth;\ntest the at least one other machine model on a real-world validation set;\ndetermine at least one performance score of the at least one other machine learning model based at least in part on the testing; and\nfurther train the at least one machine learning model based at least in part on the at least one performance score.\n21. The processor of claim 20, wherein the ground truth is automatically generated based at least in part on the one or more synthetic images.\n22. The processor of claim 15, wherein at least one first loss function is used for the training the at least one machine learning model using the network update information and at least one second loss function is used for the training the at least one machine learning model based at least in part on at least one performance score, the at least one first loss function being different from the at least one second loss function.\n23. The processor of claim 15, wherein one or more parameters of the at least one machine learning model are initialized by, at least in part, training the at least one machine learning model to compute one or more updated scene graphs such that the one or more updated scene graphs are within a threshold similarity to the one or more initial scene graphs.",
    "status": "Active",
    "citations_own": [
        "US20040252864A1",
        "US20070154068A1",
        "US20070182528A1",
        "EP1930863A2",
        "US7409295B2",
        "US20090125177A1",
        "US20090256840A1",
        "EP2384009A2",
        "WO2012011713A2",
        "US20150054824A1",
        "US20150346716A1",
        "US9373057B1",
        "US20160247290A1",
        "US20160321074A1",
        "US9489635B1",
        "WO2016183074A1",
        "US20170010108A1",
        "US20170090478A1",
        "DE102015221920A1",
        "EP3185113A1",
        "DE102015226762A1",
        "US9701307B1",
        "US20170220876A1",
        "US20170236013A1",
        "US20170259801A1",
        "US20170344808A1",
        "US20170364083A1",
        "US20170371340A1",
        "WO2018002910A1",
        "US20180089833A1",
        "US20180121273A1",
        "US20180136332A1",
        "US20180158244A1",
        "WO2018102717A1",
        "US10007269B1",
        "US20180188059A1",
        "US20180203959A1",
        "US20180232663A1",
        "US20180267558A1",
        "US20180276278A1",
        "US10108867B1",
        "US10134278B1",
        "US20180348374A1",
        "US20180349746A1",
        "US10157331B1",
        "US20180373980A1",
        "US20180370540A1",
        "US20190065933A1",
        "US20190066328A1",
        "US20190071101A1",
        "US10235601B1",
        "US20190102668A1",
        "US20190102646A1",
        "US20190129831A1",
        "US20190147600A1",
        "US20190147610A1",
        "US20190171912A1",
        "US20190179979A1",
        "US20190213481A1",
        "US20190212749A1",
        "US20190251442A1",
        "US20190250622A1",
        "US20190258251A1",
        "US20190286153A1",
        "US10474917B2",
        "US20200013176A1",
        "US10580158B1",
        "US10599546B1",
        "US10625748B1",
        "US20200143205A1",
        "US10679046B1",
        "US10730517B2",
        "US10740954B2",
        "US10816978B1",
        "US10829793B2",
        "US10885698B2",
        "US10942030B2",
        "US10997433B2",
        "US11080590B2",
        "US11079764B2",
        "US11099558B2",
        "US11150663B2",
        "US11210537B2"
    ],
    "citations_ftf": [
        "CN101714262B",
        "US10296816B2",
        "US10430978B2",
        "CN108288088B"
    ],
    "citedby_own": [
        "US20210327376A1",
        "US20220092317A1",
        "US20220156638A1"
    ],
    "citedby_ftf": [
        "US10678244B2",
        "US11409692B2",
        "US10671349B2",
        "US11157441B2",
        "US11561791B2",
        "US11215999B2",
        "US11636333B2",
        "US11562231B2",
        "US11196678B2",
        "US11537811B2",
        "US11610117B2",
        "US11580379B1",
        "US10997461B2",
        "US11567514B2",
        "US10860878B2",
        "US10956755B2",
        "US11619618B2",
        "US10902551B1",
        "US10853385B1",
        "CN111768369B",
        "US11586919B2",
        "CN111783996B",
        "AU2021303407A1",
        "US11694038B2",
        "CN112164097A",
        "CN112150575B",
        "US20220147838A1",
        "CN112561644B",
        "CN112613411B",
        "US20220215966A1",
        "US11636132B1",
        "WO2022177728A1",
        "DE102021104077B3",
        "CN113221659B",
        "US20220335258A1",
        "CN114061586A",
        "CN114528772B"
    ]
}