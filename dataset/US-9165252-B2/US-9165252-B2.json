{
    "patent_id": "US-9165252-B2",
    "title": "Utilizing failures in question and answer system responses to enhance the accuracy of question and answer systems ",
    "assignee": "International Business Machines Corporation",
    "publication_date": "2015-10-20",
    "patent_link": "https://patents.google.com/patent/US9165252B2/en",
    "inventors": [
        "Michael A. Barborak",
        "Jennifer Chu-Carroll",
        "David A. Ferrucci",
        "James W. Murdock, IV",
        "Wlodek W. Zadrozny"
    ],
    "classifications": [
        "G06N5/04",
        "G09B7/04",
        "G06N5/041"
    ],
    "abstract": "A computerized device for enhancing the accuracy of a question-answer system is disclosed. The computerized device comprises a question-answer system comprising software for performing a plurality of question answering processes. A receiver receives a question into the question-answer system. A processor that generates a plurality of candidate answers to the question is connected to the question-answer system. The processor determines a confidence score for each of the plurality of candidate answers. The processor evaluates sources of evidence used to generate the plurality of candidate answers. The processor identifies missing information from a corpus of data. The missing information comprises any information that improves a confidence score for a candidate answer. The processor generates at least one follow-on inquiry based on the missing information. A network interface outputs the at least one follow-on inquiry to external sources separate from the question-answer system.",
    "claims": "\n1. A device, comprising:\na question-answer system comprising software for performing question answering processes;\na receiver receiving a question into said question-answer system;\na processor connected to said question-answer system, said processor:\ngenerating candidate answers to said question,\ndetermining a confidence score for each of said candidate answers,\nevaluating sources of evidence used to generate said candidate answers,\nidentifying missing information from a corpus of data, said missing information comprising any information that improves the ability of said question-answer system to understand and evaluate said sources of evidence associated with said candidate answers, said identifying said missing information comprising:\nevaluating a piece of evidence and producing an evidence score according to a scoring process that reflects a degree to which said piece of evidence supports or refutes said candidate answer,\nclassifying said piece of evidence as marginal evidence if said score is below a threshold value, and\nevaluating said marginal evidence to identify said missing information, and\ngenerating a follow-on inquiry based on said missing information; and\na network interface outputting said follow-on inquiry to external sources separate from said question-answer system to obtain responses to said follow-on inquiry and receiving from said external sources a response to said follow-on inquiry,\nsaid processor inputting said response into said corpus of data.\n2. The device according to claim 1, said processor developing additional logical rules and additional evidence for said question-answer system based on said response to said follow-on inquiry.\n3. The device according to claim 1, in response to said processor inputting said response into said question-answer system, said processor re-evaluating said confidence score for said candidate answers.\n4. The device according to claim 1, further comprising:\na parser, said parser:\nparsing said question into a first collection of elements; and\nparsing said piece of evidence into a second collection of elements; and\nsaid processor further analyzing said first collection of elements for said question and said piece of evidence in order to determine a relationship between a first element of said first collection of elements for said question and a second element of said second collection of elements for said piece of evidence; and locating a missing relationship between said first element and said second element, said missing relationship comprising said missing information.\n5. The device according to claim 1, said processor further:\nevaluating whether said confidence score for any of said candidate answers is below a confidence threshold value; and\nif said confidence score is below said confidence threshold value, triggering said step of identifying missing information.\n6. The device according to claim 1, said processor validating that said response is accurate prior to adding said response to said corpus of data, said validating comprising one of:\nidentifying a most common response received from external sources and selecting said most common response;\nselecting responses from first outside sources, said first outside sources having a previously established level of accuracy in providing previous responses to previous inquiries;\nwhen said question-answer system comprises a total amount of evidence, said total amount of evidence having been classified by said question-answer system as one of good evidence and bad evidence, said good evidence enabling said question-answer system to provide a correct answer to said question, and said bad evidence being irrelevant to said question, selecting responses from second outside sources, said second outside sources having a previously established level of accuracy in classifying said total amount of evidence as good evidence or bad evidence that matches classification of said total amount of evidence by said question-answer system as good evidence and bad evidence; and\nselecting answers from third outside sources, said third outside sources having demonstrated a previously established level of accuracy in providing responses that match responses from a known source.\n7. A question answering (QA) system comprising:\na processor;\nan evidence analysis module connected to said processor;\na first interface connected to said processor;\na second interface connected to said processor; and\na corpus of data connected to said evidence analysis module, said corpus of data comprising a total amount of evidence, said evidence analysis module classifying said total amount of evidence as good evidence or marginal evidence,\nsaid first interface receiving a first question to be answered by said QA system,\nsaid processor creating a collection of candidate answers to said first question from said corpus of data, each said candidate answer having supporting evidence and a confidence score generated by said processor,\nsaid good evidence having an evidence score above a previously established evidence threshold value, and enabling said QA system to provide a candidate answer to said first question with a confidence score above a previously established confidence threshold value, and\nsaid marginal evidence having an evidence score below said previously established evidence threshold value, enabling said QA system to provide a candidate answer to said first question with a confidence score below said previously established confidence threshold value, and requiring said QA system to obtain additional information to improve said confidence score for said candidate answer to said first question,\nsaid processor identifying missing information from said corpus of data, said missing information comprising any information that improves the ability of said QA system to understand and evaluate said supporting evidence associated with said collection of candidate answers,\nsaid evidence analysis module producing a second question based on said missing information,\nsaid processor presenting said second question through said second interface to external sources separate from said QA system to obtain responses to said second question,\nsaid processor receiving a response or knowledge item from said external sources through said second interface,\nsaid processor inputting said response or knowledge item into said corpus of data, and\nsaid processor automatically developing additional logical rules and additional evidence for said QA system based on said response or knowledge item.\n8. The question answering system according to claim 7, further comprising:\nsaid evidence analysis module evaluating a piece of evidence relevant to said candidate answer to identify said missing information, said missing information comprising a data, a fact, a syntactical relationship, a grammatical relationship, a logical rule, or a taxonomy rule.\n9. The question answering system according to claim 7, said corpus of data further comprising textual passages, said processor comparing said first question to said textual passages to provide said collection of candidate answers.\n10. The question answering system according to claim 7, said processor further evaluating said response by at least one of:\nidentifying a most common response received from said external sources and selecting said most common response;\nselecting responses from first outside sources, said first outside sources having a previously established level of accuracy in providing previous responses to previous inquiries;\nwhen said QA system comprises a total amount of evidence, said total amount of evidence having been classified by said QA system as one of good evidence and bad evidence, said good evidence enabling said QA system to provide a correct answer to said question, and said bad evidence being irrelevant to said question, selecting responses from second outside sources, said second outside sources having a previously established level of accuracy in classifying said total amount of evidence as good evidence or bad evidence that matches classification of said total amount of evidence by said QA system as good evidence and bad evidence; and\nselecting answers from third outside sources, said third outside sources having demonstrated a previously established level of accuracy in providing responses that match responses from a known source.\n11. The question answering system according to claim 7, in response to inputting said response or knowledge item into said corpus of data, said processor re-evaluating said confidence score for said collection of candidate answers.\n12. A method, comprising:\ngenerating candidate answer to a question\ndetermining a confidence score for each of said candidate answers;\nevaluating sources of evidence used to generate said candidates answers;\nidentifying missing information from a corpus of data, said missing information comprising any information that improves the ability of a question-answer system to understand and evaluate sources of evidence associated with a candidate answer;\ngenerating a follow-on inquiry, said follow-on inquiry prompting for said missing information to be provided;\noutputting said follow-on inquiry to an external source;\nreceiving a response to said follow-on inquiry from said external source; and\nadding said response to said corpus of data.\n13. The method of claim 12, said missing information comprising a data, a fact, a syntactical relationship, a grammatical relationship, a logical rule, or a taxonomy rule.\n14. The method according to claim 12, said identifying missing information further comprising:\nevaluating a piece of evidence relevant to said candidate answer to identify said missing information, said missing information comprising a data, a fact, a syntactical relationship, a grammatical relationship, a logical rule, or a taxonomy rule.\n15. The method according to claim 14, said evaluating a piece of evidence further comprising:\nparsing said question into a first collection of elements;\nparsing said piece of evidence into a second collection of elements;\nanalyzing said first collection of elements for said question and said piece of evidence in order to determine a relationship between an element of said first collection of elements for said question and an element of said second collection of elements for said piece of evidence, said relationship between elements comprising a lexical relationship, a grammatical relationship, or a semantic relationship; and\nlocating a missing relationship between said elements, said missing relationship comprising said missing information.\n16. The method of claim 15, said missing relationship between said first element and said second element comprising a lexical relationship, a grammatical relationship, or a semantic relationship.\n17. The method according to claim 12, further comprising:\nreceiving said question into said question-answer system; and\ngenerating said candidate answer to said question using said question-answer system and said corpus of data.\n18. The method according to claim 17, said identifying said missing information further comprising:\nevaluating a piece of evidence to produce an evidence score according to a scoring process that reflects a degree to which said piece of evidence supports or refutes said candidate answer;\nclassifying said piece of evidence as marginal evidence if said score is below a threshold value; and\nevaluating said marginal evidence to identify said missing information.\n19. The method according to claim 12, further comprising:\nevaluating whether said confidence score is below a confidence threshold value;\nand if said confidence score is below said confidence threshold value, triggering said step of identifying missing information.\n20. The method according to claim 12, further comprising:\nvalidating said response is accurate prior to adding said response to said corpus of data, said validating comprising one of:\nidentifying a most common response received from external sources and selecting said most common response;\nselecting responses from first outside sources, said first outside sources having a previously established level of accuracy in providing previous responses to previous inquiries;\nwhen said question-answer system comprises a total amount of evidence, said total amount of evidence having been classified by said question-answer system as one of good evidence and bad evidence, said good evidence enabling said question-answer system to provide a correct answer to said question, and said bad evidence being irrelevant to said question, selecting responses from second outside sources, said second outside sources having a previously established level of accuracy in classifying said total amount of evidence as good evidence or bad evidence that matches classification of said total amount of evidence by said question-answer system as good evidence and bad evidence; and\nselecting answers from third outside sources, said third outside sources having demonstrated a previously established level of accuracy in providing responses that match responses from a known source.",
    "status": "Active",
    "citations_own": [
        "US5519608A",
        "US6594654B1",
        "US20060286530A1",
        "US20070094210A1",
        "US7337158B2",
        "US20080049939A1",
        "US7349840B2",
        "US7512978B1",
        "US7519529B1",
        "US20090192968A1",
        "US20090287678A1",
        "US20090292687A1",
        "US7680891B1",
        "US20100191686A1",
        "US7809664B2",
        "US7841940B2",
        "US20110087656A1",
        "US20110119212A1",
        "US20110125734A1",
        "US20120054834A1",
        "US20130019285A1",
        "US8380503B2",
        "US20130066886A1",
        "US8412514B1"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US20170140290A1",
        "US10592502B2",
        "US10614725B2",
        "US10628749B2"
    ],
    "citedby_ftf": [
        "US10176827B2",
        "US10489434B2",
        "US8943094B2",
        "US9122744B2",
        "US20130066693A1",
        "US9836177B2",
        "US8909534B1",
        "US9223537B2",
        "US9536049B2",
        "US9754215B2",
        "WO2014124311A1",
        "US9171478B2",
        "US10445115B2",
        "US9342608B2",
        "US9390404B2",
        "US9965548B2",
        "US10720071B2",
        "US10088972B2",
        "US9418566B2",
        "WO2015115052A1",
        "US9959315B1",
        "US9378273B2",
        "US20150309988A1",
        "US9563688B2",
        "US9607035B2",
        "JP5737641B2",
        "US9542496B2",
        "US9697099B2",
        "US9633309B2",
        "CN104123351B",
        "US9740769B2",
        "US9619513B2",
        "US20160071517A1",
        "US20160098737A1",
        "US9690862B2",
        "US9501525B2",
        "US11204929B2",
        "US9892362B2",
        "US9472115B2",
        "US11244113B2",
        "US10318870B2",
        "US9727642B2",
        "US10331673B2",
        "US10387793B2",
        "US9881313B2",
        "US9390374B2",
        "US9898170B2",
        "US10127284B2",
        "US10108906B2",
        "US9928269B2",
        "US20160217209A1",
        "US10366107B2",
        "US9996604B2",
        "US10795921B2",
        "US9912736B2",
        "US10169326B2",
        "US10152534B2",
        "US10592540B2",
        "US11113614B2",
        "US10769185B2",
        "US10255349B2",
        "US10586161B2",
        "JP6727562B2",
        "US9478145B1",
        "US10229188B2",
        "US9589049B1",
        "US9940384B2",
        "US11227113B2",
        "US9471668B1",
        "AU2017200378A1",
        "JP2017151860A",
        "US10229189B2",
        "US10229187B2",
        "US10606952B2",
        "US10104232B2",
        "US10009466B2",
        "US20180018616A1",
        "US10606893B2",
        "US10503739B2",
        "US10769138B2",
        "CN108446322B",
        "US11397851B2",
        "CN108959488B",
        "US10795899B2",
        "US11568175B2",
        "US11196863B2",
        "CN111177328B",
        "US10831989B2",
        "US11455298B2",
        "GB201911760D0",
        "RU2019128018A",
        "US11302323B2",
        "CN111211969B",
        "US11620456B2",
        "US20220012600A1",
        "US20220027768A1",
        "US20220092098A1",
        "US20230080674A1"
    ]
}