{
    "patent_id": "US-10956831-B2",
    "title": "Detecting interaction during meetings ",
    "assignee": "International Business Machines Corporation",
    "publication_date": "2021-03-23",
    "patent_link": "https://patents.google.com/patent/US10956831B2/en",
    "inventors": [
        "Rachel K. E. Bellamy",
        "II Jonathan H. Connell",
        "Robert G. Farrell",
        "Brian P. Gaucher",
        "Jonathan Lenchner",
        "David O. S. Melville",
        "Valentina Salapura"
    ],
    "classifications": [
        "G06N20/00",
        "G06V40/20",
        "G06K9/00335",
        "G06N3/006",
        "G06N5/022",
        "G06N20/20",
        "G06Q10/1095"
    ],
    "abstract": "In one embodiment, in accordance with the present invention, a method, computer program product, and system for performing actions based on captured interpersonal interactions during a meeting is provided. One or more computer processors capture the interpersonal interactions between people in a physical space during a period of time, using machine learning algorithms to detect the interpersonal interactions and a state of each person based on vision and audio sensors in the physical space. The one or more computer processors analyze and categorize the interactions and state of each person, and tag representations of each person with the respectively analyzed and categorized interactions and states of the respective person over the period of time. The one or more computer processors then take an action based on the analysis.",
    "claims": "\n1. A computer-implemented method comprising:\ncapturing, by one or more computer processors, interpersonal interactions between people in a physical space during a period of time, using machine learning algorithms to detect the interpersonal interactions and a state of each person based on visual and audio sensors in the physical space;\nanalyzing and categorizing, by one or more computer processors, the interactions and state of each person, and tagging representations of each person with respectively analyzed and categorized interactions and states for the respective person over the period of time;\ndetermining, by one or more computer processors, levels of engagement for each person with respect to a speaker based, at least in part, on the respectively analyzed and categorized interactions and states for the respective person; and\ndisplaying, by one or more computer processors, the representations of each person on a screen, wherein the representations migrate toward the center of the screen in proportion to the determined levels of engagement for each person with respect to the speaker.\n2. The computer-implemented method of claim 1, wherein capturing interpersonal interactions between people in a physical space during a period of time comprises:\ncollecting, by one or more processors, data from a plurality of computing devices, wherein the plurality of computing devices includes a camera, a microphone, a heartrate monitor, and an infra-red camera.\n3. The computer-implemented method of claim 1, wherein detecting the interpersonal interactions and state of each person comprises:\ndetermining, by one or more computer processors, using the machine learning algorithms, one or more states of a meeting participant, wherein the one or more states of the meeting participant are selected from the group consisting of:\nan eye movement, a gaze, or pupil dilation,\na pulse rate,\nan infra-red body heat pattern, and\na group interaction.\n4. The computer-implemented method of claim 1, wherein analyzing and categorizing the interactions and state of each person, and tagging representations of each person with the respectively analyzed and categorized interactions and states of the respective person over the period of time comprises:\nlearning, by one or more computer processors, interactions and behaviors of each person over the period of time and tagging representations of each person with data collected from a plurality of computing devices.\n5. The computer-implemented method of claim 4, wherein tagging representations of each person with data from a plurality of computing devices comprises:\nidentifying and isolating, by one or more computer processors, each person in a physical space using the data from the plurality of computing devices.\n6. The computer-implemented method of claim 1, wherein displaying, by one or more computer processors, the representations of each person on a screen further comprises:\ndisplaying, by one or more computer processors, on the screen, a unified meeting room with avatars that represent each person as a participant in a meeting, and wherein the computer-implemented method further comprises:\ndetermining, by one or more computer processors, using the machine learning algorithms, that a meeting participant is not paying attention to the meeting, based on the respectively analyzed and categorized interactions and states of the meeting participant; and\nindicating, by one or more computer processors, to a meeting host, that the meeting participant that is not paying attention using an avatar that represents the meeting participant.\n7. The computer-implemented method of claim 6, further comprising:\nupon detecting, using the machine learning algorithms, a lack of consensus on one or more topics, automatically scheduling, by one or more computer processors, a follow-up meeting involving a subset of the original meeting participants; and\nupon detecting, using the machine learning algorithms, a disruptive meeting participant in the meeting, automatically removing, by one or more computer processors, the disruptive meeting participant from the meeting.\n8. A computer program product comprising:\none or more computer readable storage media and program instructions stored on the one or more computer readable storage media, wherein the one or more computer readable storage media are not transitory signals per se, the stored program instructions comprising:\nprogram instructions to capture interpersonal interactions between people in a physical space during a period of time, using machine learning algorithms to detect the interpersonal interactions and a state of each person based on visual and audio sensors in the physical space;\nprogram instructions to analyze and categorize the interactions and state of each person, and tag representations of each person with respectively analyzed and categorized interactions and states for the respective person over the period of time;\nprogram instructions to determine levels of engagement for each person with respect to a speaker based, at least in part, on the respectively analyzed and categorized interactions and states for the respective person; and\nprogram instructions to display the representations of each person on a screen, wherein the representations migrate toward the center of the screen in proportion to the determined levels of engagement for each person with respect to the speaker.\n9. The computer program product of claim 8, wherein the program instructions to capture interpersonal interactions between people in a physical space during a period of time comprise:\nprogram instructions to collect data from a plurality of computing devices, wherein the plurality of computing devices includes a camera, a microphone, a heartrate monitor, and an infra-red camera.\n10. The computer program product of claim 8, wherein the program instructions to detect the interpersonal interactions and state of each person comprise:\nprogram instructions to determine, using the machine learning algorithms, one or more states of a meeting participant, wherein the one or more states of the meeting participant are selected from the group consisting of:\nan eye movement, a gaze, or pupil dilation,\na pulse rate,\nan infra-red body heat pattern, and\na group interaction.\n11. The computer program product of claim 8, wherein the program instructions to analyze and categorize the interactions and state of each person, and tag representations of each person with the respectively analyzed and categorized interactions and states of the respective person over the period of time comprise:\nprogram instructions to learn interactions and behaviors of each person over the period of time and tag representations of each person with data collected from a plurality of computing devices.\n12. The computer program product of claim 11, wherein the program instructions to tag representations of each person with data from a plurality of computing devices comprise:\nprogram instructions to identify and isolate each person in a physical space using the data from the plurality of computing devices.\n13. The computer program product of claim 8, wherein the program instructions to display the representations of each person on a screen further comprise:\nprogram instructions to display, on the screen, a unified meeting room with avatars that represent each person as a participant in a meeting, and wherein the stored program instructions further comprise:\nprogram instructions to determine, using the machine learning algorithms, that a meeting participant is not paying attention to the meeting, based on the respectively analyzed and categorized interactions and states of the meeting participant; and\nprogram instructions to indicate, to a meeting host, that the meeting participant that is not paying attention using an avatar that represents the meeting participant.\n14. The computer program product of claim 13, the stored program instructions further comprising:\nprogram instructions to, upon detecting a lack of consensus on one or more topics using the machine learning algorithms, automatically schedule a follow-up meeting involving a subset of the original meeting participants; and\nprogram instructions to, upon detecting a disruptive meeting participant in the meeting using the machine learning algorithms, automatically remove the disruptive meeting participant from the meeting.\n15. A computer system comprising:\none or more computer processors;\none or more computer readable storage media;\nprogram instructions stored on the computer readable storage media for execution by at least one of the one or more processors, the stored program instructions comprising:\nprogram instructions to capture interpersonal interactions between people in a physical space during a period of time, using machine learning algorithms to detect the interpersonal interactions and a state of each person based on visual and audio sensors in the physical space;\nprogram instructions to analyze and categorize the interactions and state of each person, and tag representations of each person with respectively analyzed and categorized interactions and states for the respective person over the period of time;\nprogram instructions to determine levels of engagement for each person with respect to a speaker based, at least in part, on the respectively analyzed and categorized interactions and states for the respective person; and\nprogram instructions to display the representations of each person on a screen, wherein the representations migrate toward the center of the screen in proportion to the determined levels of engagement for each person with respect to the speaker.\n16. The computer system of claim 15, wherein the program instructions to capture interpersonal interactions between people in a physical space during a period of time comprise:\nprogram instructions to collect data from a plurality of computing devices, wherein the plurality of computing devices includes a camera, a microphone, a heartrate monitor, and an infra-red camera.\n17. The computer system of claim 15, wherein the program instructions to detect the interpersonal interactions and state of each person comprise:\nprogram instructions to determine, using the machine learning algorithms, one or more states of a meeting participant, wherein the one or more states of the meeting participant are selected from the group consisting of:\nan eye movement, a gaze, or pupil dilation,\na pulse rate,\nan infra-red body heat pattern, and\na group interaction.\n18. The computer system of claim 15, wherein the program instructions to analyze and categorize the interactions and state of each person, and tag representations of each person with the respectively analyzed and categorized interactions and states of the respective person over the period of time comprise:\nprogram instructions to learn interactions and behaviors of each person over the period of time and tag representations of each person with data collected from a plurality of computing devices.\n19. The computer system of claim 18, wherein the program instructions to tag representations of each person with data from a plurality of computing devices comprise:\nprogram instructions to identify and isolate each person in a physical space using the data from the plurality of computing devices.\n20. The computer system of claim 15, wherein the program instructions to display the representations of each person on a screen further comprise:\nprogram instructions to display, on the screen, a unified meeting room with avatars that represent each person as a participant in a meeting, and wherein the stored program instructions further comprise:\nprogram instructions to determine, using the machine learning algorithms, that a meeting participant is not paying attention to the meeting, based on the respectively analyzed and categorized interactions and states of the meeting participant; and\nprogram instructions to indicate, to a meeting host, that the meeting participant that is not paying attention using an avatar that represents the meeting participant."
}