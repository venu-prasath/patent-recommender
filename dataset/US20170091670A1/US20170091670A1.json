{
    "patent_link": "https://patents.google.com/patent/US20170091670A1/en",
    "patent_id": "US20170091670A1",
    "title": "Method of and system for generating a prediction model and determining an accuracy of a prediction model",
    "abstract": "A computer-implemented method of and a system for generating a prediction model and determining an accuracy parameter of a trained decision tree prediction model. The method comprises accessing the trained decision tree prediction model having been generated at least partially based on a set of training objects; generating a subset of random parameters of interest; associating the subset of random parameters of interests with a given leaf; determining a leaf accuracy parameter based on (i) the parameters of interest associated with the given leaf and (ii) the subset of random parameters of interest of the given leaf; and determining the accuracy parameter of the trained decision tree prediction model based on the determined leaf accuracy parameter.",
    "inventors": [
        "Andrey Vladimirovich GULIN",
        "Andrey Sergeevich MISHCHENKO",
        "Konstantin Vyacheslavovich VORONTSOV",
        "Yevgeny Andreevich SOKOLOV"
    ],
    "assignee": "Yandex LLC",
    "classifications": [
        "G06N5/02",
        "G06N99/005",
        "G06N20/00",
        "G06N5/022",
        "G06F16/00"
    ],
    "claims": "\n1. A method of determining an accuracy parameter of a trained decision tree prediction model, the method being executable at a machine learning system, the method comprising:\naccessing, from a non-transitory computer-readable medium, the trained decision tree prediction model having been generated at least partially based on a set of training objects, each training object of the set of training objects comprising features and a parameter of interest, the trained decision tree prediction model comprising nodes associated with factors and leaves associated with parameters of interest of training objects of the set of training objects, the association between the leaves and the parameters of interest having been determined by a comparison of at least two of the factors and the features of the training objects of the set of training objects;\ngenerating, by a processor, a subset of random parameters of interest;\nassociating, in the non-transitory computer-readable medium, the subset of random parameters of interests with a given leaf;\ndetermining, by the processor, a leaf accuracy parameter based on (i) the parameters of interest associated with the given leaf and (ii) the subset of random parameters of interest of the given leaf; and\ndetermining, by the processor, the accuracy parameter of the trained decision tree prediction model based on the determined leaf accuracy parameter.\n2. The method of claim 1, wherein the comparison of the at least two of the factors and the features of the training objects comprises comparing, by the processor, conditions associated with the at least two of the factors and at least two values associated with the features of the corresponding training object.\n3. The method of claim 1, wherein generating, by the processor, the subset of random parameters of interest comprises generating random values of a target function associated with the trained decision tree prediction model.\n4. The method of claim 3, wherein the random values are selected so as to increase an error associated with a best factor amongst the factors while maintaining a previously generated accuracy parameter of the trained decision tree prediction model below a minimum threshold.\n5. The method of claim 4, wherein the best factor amongst the factors is determined as the factor having a higher positive impact on the previously generated accuracy parameter of the trained decision tree prediction model.\n6. The method of claim 3, wherein the random values are selected based on values of the parameters of interest associated with the given leaf.\n7. The method of claim 6, wherein the random values are selected so as to be comprised within a range comprising a minimum value defined as a lowest value of the parameters of interest associated with the given leaf and a maximum value defined as a highest value of the parameters of interest associated with the given leaf.\n8. The method of claim 1, wherein the subset of random parameters of interest comprises a number of random parameters of interest equals to a number of parameters of interest of the given leaf with which the subset of random parameters of interest is associated.\n9. The method of claim 1, wherein determining, by the processor, the accuracy parameter of the trained decision tree prediction model based on the determined leaf accuracy parameter comprises determining a total error in the leaves in accordance with the formula:\n10. The method of claim 9, wherein the number of parameters of interest associated with the j-th leaf is equal to a number of training objects associated with the j-th leaf.\n11. The method of claim 1, wherein determining, by the processor, the accuracy parameter of the trained decision tree prediction model is based on a plurality of determined leaf accuracy parameters, each one of the plurality of determined leaf accuracy parameters being associated with a distinct leaf.\n12. The method of claim 1, wherein the features are indicative of at least one of a number of clicks, a number of views, a document ranking, a URL, a domain name, an IP address, a search query and a key word.\n13. The method of claim 1, wherein the parameter of interest is indicative of at least one of a search result prediction, a probability of click, a document relevance, a user interest, a URL, a number of clicks and a click-through rate (CTR).\n14. The method of claim 1, wherein the accuracy parameter of the trained decision tree prediction model is reflective of an accuracy of a target function associated with the trained decision tree prediction model.\n15. The method of claim 1, wherein each one of the factors is associated with one of (i) a condition applicable to a binary feature, (ii) a condition applicable to a numerical feature and (iii) a condition applicable to a categorical feature.\n16. A method of generating a trained decision tree prediction model, the method being executable at a machine learning system, the method comprising:\naccessing, from a non-transitory computer-readable medium, a set of factors;\nidentifying, by the processor, from the set of factors, a factor associated with a best accuracy parameter of a preliminary trained decision tree prediction model for a given position of a node associated with the factor in the preliminary trained decision tree prediction model, the best accuracy parameter of the preliminary trained decision tree prediction model being selected amongst a plurality of accuracy parameters of a plurality of preliminary decision tree prediction models, the plurality of accuracy parameters of the plurality of preliminary decision tree prediction models having been generated in accordance with the method of claim 2;\nassociating, by the processor, the factor with the given position of the node of the trained decision tree prediction model to be generated; and\ngenerating, by the processor, the trained decision tree prediction model, the trained decision tree prediction model comprising the node associated with the factor for the given position.\n17. The method of claim 16, wherein each one of the plurality of accuracy parameters is associated with a corresponding one of the plurality of preliminary decision tree prediction models.\n18. The method of claim 16, wherein the method further comprises:\nidentifying, by the processor, from the set of factors, another factor associated with a best accuracy parameter of another preliminary trained decision tree prediction model for another given position of another node associated with the other factor in the other preliminary trained decision tree prediction model; and\nassociating, by the processor, the other factor with the other given position of the other node of the trained decision tree prediction model to be generated.\n19. The method of claim 16, wherein the trained decision tree prediction model further comprises the other node associated with the other factor for the other given position.\n20. A method of determining an accuracy parameter of a trained decision tree prediction model, the method being executable at a machine learning system, the method comprising:\naccessing, from a non-transitory computer-readable medium, a set of training objects, each training object of the set of training objects comprising features and a parameter of interest;\ngenerating, by a processor, the trained decision tree prediction model at least partially based on a set of training objects, each training object of the set of training objects comprising features and a parameter of interest, the trained decision tree prediction model comprising nodes associated with factors and leaves associated with parameters of interest of training objects of the set of training objects, the association between the leaves and the parameters of interest having been determined by a comparison of at least two of the factors and the features of the training objects of the set of training objects;\ngenerating, by the processor, a subset of random parameters of interest;\nassociating, in the non-transitory computer-readable medium, the subset of random parameters of interests with a given leaf;\ndetermining, by the processor, a leaf accuracy parameter based on (i) the parameters of interest associated with the given leaf and (ii) the subset of random parameters of interest of the given leaf; and\ndetermining, by the processor, the accuracy parameter of the trained decision tree prediction model based on the determined leaf accuracy parameter.",
    "status": "Active",
    "citations_own": [
        "US20030176931A1",
        "US20040215430A1",
        "US20100082421A1",
        "US20140122381A1",
        "US20150012465A1"
    ],
    "citations_ftf": [
        "US5652829A",
        "US5978497A",
        "US6115802A",
        "US5657424A",
        "US6360220B1",
        "US6279004B1",
        "US6523015B1",
        "CN1241135C",
        "US7113932B2",
        "US20020143787A1",
        "US20030014420A1",
        "US20030014405A1",
        "US6871201B2",
        "US6748401B2",
        "JP3791908B2",
        "US7272590B2",
        "US7349917B2",
        "US7020593B2",
        "US7606714B2",
        "US7197497B2",
        "US8136025B1",
        "US7409587B2",
        "US6988180B2",
        "US7702628B1",
        "US20060026138A1",
        "US7287012B2",
        "EP1723596A1",
        "US7349926B2",
        "US7574409B2",
        "US20060112121A1",
        "US7613701B2",
        "CA2594181A1",
        "US7451166B2",
        "US7328218B2",
        "US20070005646A1",
        "US7673233B2",
        "US8341158B2",
        "US20070208730A1",
        "US7949186B2",
        "WO2007117423A2",
        "US20070244747A1",
        "US8694318B2",
        "US7801836B2",
        "US8661029B1",
        "US7668851B2",
        "US8250075B2",
        "US7743003B1",
        "FR2917259B1",
        "US8287639B2",
        "US7916728B1",
        "NO327653B1",
        "US20090182723A1",
        "US8584233B1",
        "US20090319481A1",
        "US8972410B2",
        "US8965881B2",
        "NZ572036A",
        "US8190537B1",
        "US8572071B2",
        "US20100161385A1",
        "US8150723B2",
        "US8935483B2",
        "US8537832B2",
        "US8032550B2",
        "US8032551B2",
        "US8396287B2",
        "US8611592B2",
        "US10528972B2",
        "US20110153611A1",
        "US20110188715A1",
        "WO2011127158A1",
        "US8370337B2",
        "US8510236B1",
        "US8521664B1",
        "US8438122B1",
        "US8473431B1",
        "US8543517B2",
        "US20120079212A1",
        "WO2012061162A1",
        "US8543586B2",
        "US8533222B2",
        "US8595154B2",
        "US8924365B2",
        "RU2637610C2",
        "US8533224B2",
        "WO2012154657A2",
        "AU2012203348B2",
        "US8868472B1",
        "US8909564B1",
        "US8762299B1",
        "EP2724269B1",
        "US8489632B1",
        "US8843427B1",
        "US20130117684A1",
        "US9355095B2",
        "RU2491622C1",
        "US8965829B2",
        "US8655029B2",
        "US8694444B2",
        "US9955965B2",
        "JP5943762B2",
        "WO2014056093A1",
        "US8880446B2",
        "US10262330B2",
        "US20140195972A1",
        "KR101822463B1",
        "US9324040B2",
        "US9953270B2",
        "US9639807B2",
        "US9886670B2",
        "US10339465B2",
        "US9348920B1",
        "RU2632133C2",
        "RU2015141339A",
        "US10380127B2"
    ],
    "citedby_own": [
        "US20170213154A1",
        "US20190179940A1",
        "US10360214B2",
        "US10387801B2",
        "US20200081934A1",
        "US10649988B1",
        "CN111581545A",
        "CN111695739A",
        "US11120299B2",
        "CN113792904A",
        "US20210406222A1",
        "US11256991B2",
        "US11494692B1",
        "US11526529B2",
        "US11775850B2"
    ],
    "citedby_ftf": [
        "RU2694001C2",
        "US11036607B2"
    ]
}