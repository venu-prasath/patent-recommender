{
    "patent_id": "US-11663827-B2",
    "title": "Generating a video segment of an action from a video ",
    "assignee": "Google Llc",
    "publication_date": "2023-05-30",
    "patent_link": "https://patents.google.com/patent/US11663827B2/en",
    "inventors": [
        "Sudheendra Vijayanarasimhan",
        "Alexis Bienvenu",
        "David Ross",
        "Timothy NOVIKOFF",
        "Arvind Balasubramanian"
    ],
    "classifications": [
        "G06V20/47",
        "G06F3/0484",
        "G06N20/20",
        "G06N3/04",
        "G06V20/30",
        "G06V20/41",
        "G06V20/44"
    ],
    "abstract": "A computer-implemented method includes receiving a video that includes multiple frames. The method further includes identifying a start time and an end time of each action in the video based on application of one or more of an audio classifier, an RGB classifier, and a motion classifier. The method further includes identifying video segments from the video that include frames between the start time and the end time for each action in the video. The method further includes generating a confidence score for each of the video segments based on a probability that a corresponding action corresponds to one or more of a set of predetermined actions. The method further includes selecting a subset of the video segments based on the confidence score for each of the video segments.",
    "claims": "\n1. A system comprising:\none or more processors; and\na memory coupled to the one or more processors, with instructions stored thereon that, when executed by the one or more processors, cause the one or more processors to perform operations comprising:\nproviding, as input to a trained machine-learning model, a video that includes multiple frames, wherein the trained machine-learning model is trained to identify one or more segments from input videos based on actions depicted in the input videos;\nidentifying, by the trained machine-learning model, one or more video segments from the video, wherein each video segment depicts a respective action and includes a plurality of frames between a start time and an end time of the respective action;\ngenerating a confidence score for each video segment based on a probability that the respective action in the video segment corresponds to a particular type of action in a set of predetermined actions; and\nselecting a subset of the video segments based on the confidence score for each of the video segments.\n2. The system of claim 1, wherein the trained machine-learning model includes an audio classifier that outputs the start time and the end time of the respective action in the video where audio in the video corresponds to the respective action and wherein the particular type of action is associated with the audio.\n3. The system of claim 1, wherein the trained machine-learning model includes a red green blue (RGB) classifier that generates one or more feature vectors from the video and outputs features associated with the respective action and the particular type of action.\n4. The system of claim 1, wherein the trained machine-learning model includes a motion classifier that outputs an optical flow in the video that is associated an action and a type of action.\n5. The system of claim 1, wherein the trained machine-learning model analyzes features within a plurality of subsets of sequential frames of the video and the operations further include after analysis of the plurality of subsets of sequential frames of the video by the trained machine-learning model is completed, advancing a sliding window in increments to obtain a subsequent subset of sequential frames for analysis by the trained machine-learning model.\n6. The system of claim 5, wherein the trained machine-learning model scores features within the plurality of subsets of sequential frames and selects a top predetermined number of actions.\n7. The system of claim 1, wherein the operations further comprise:\ngenerating a segment score to each of the video segments based on personalization information for a user, wherein selecting the subset of the video segments is further based on a combination of the confidence score and the segment score for each of the video segments exceeding a threshold score; and\ngenerating a video clip that includes the subset of the video segments.\n8. A method comprising:\nproviding, as input to a trained machine-learning model, a video that includes multiple frames, wherein the trained machine-learning model is trained to identify one or more segments from input videos based on actions depicted in the input videos;\nidentifying, by the trained machine-learning model, one or more video segments from the video, wherein each video segment depicts a respective action and includes a plurality of frames between a start time and an end time of the respective action;\ngenerating a confidence score for each video segment based on a probability that the respective action in the video segment corresponds to a particular type of action in a set of predetermined actions; and\nselecting a subset of the video segments based on the confidence score for each of the video segments.\n9. The method of claim 8, wherein the trained machine-learning model includes an audio classifier that outputs the start time and the end time of the respective action in the video where audio in the video corresponds to the respective action and wherein the particular type of action is associated with the audio.\n10. The method of claim 8, wherein the trained machine-learning model includes a red green blue (RGB) classifier that generates one or more feature vectors from the video and outputs features associated with the respective action and the particular type of action.\n11. The method of claim 8, wherein the trained machine-learning model includes a motion classifier that outputs an optical flow in the video that is associated an action and a type of action.\n12. The method of claim 8, wherein the trained machine-learning model analyzes features within a plurality of subsets of sequential frames of the video and the method further comprises after analysis of the plurality of subsets of sequential frames of the video by the trained machine-learning model is completed, advancing a sliding window in increments to obtain a subsequent subset of sequential frames for analysis by the trained machine-learning model.\n13. The method of claim 12, wherein the trained machine-learning model scores features within the plurality of subsets of sequential frames and selects a top predetermined number of actions.\n14. The method of claim 8, further comprising:\ngenerating a segment score to each of the video segments based on personalization information for a user, wherein selecting the subset of the video segments is further based on a combination of the confidence score and the segment score for each of the video segments exceeding a threshold score; and\ngenerating a video clip that includes the subset of the video segments.\n15. A non-transitory computer readable medium with instructions that, when executed by one or more computers, cause the one or more computers to perform operations, the operations comprising:\nproviding, as input to a trained machine-learning model, a video that includes multiple frames, wherein the trained machine-learning model is trained to identify one or more segments from input videos based on actions depicted in the input videos;\nidentifying, by the trained machine-learning model, one or more video segments from the video, wherein each video segment depicts a respective action and includes a plurality of frames between a start time and an end time of the respective action;\ngenerating a confidence score for each video segment based on a probability that the respective action in the video segment corresponds to a particular type of action in a set of predetermined actions; and\nselecting a subset of the video segments based on the confidence score for each of the video segments.\n16. The non-transitory computer readable medium of claim 15, wherein the trained machine-learning model includes an audio classifier that outputs the start time and the end time of the respective action in the video where audio in the video corresponds to the respective action and wherein the particular type of action is associated with the audio.\n17. The non-transitory computer readable medium of claim 15, wherein the trained machine-learning model includes a red green blue (RGB) classifier that generates one or more feature vectors from the video and outputs features associated with the respective action and the particular type of action.\n18. The non-transitory computer readable medium of claim 15, wherein the trained machine-learning model includes a motion classifier that outputs an optical flow in the video that is associated an action and a type of action.\n19. The non-transitory computer readable medium of claim 15, wherein the trained machine-learning model analyzes features within a plurality of subsets of sequential frames of the video and the operations further include after analysis of the plurality of subsets of sequential frames of the video by the trained machine-learning model is completed, advancing a sliding window in increments to obtain a subsequent subset of sequential frames for analysis by the trained machine-learning model.\n20. The non-transitory computer readable medium of claim 19, wherein the trained machine-learning model scores features within the plurality of subsets of sequential frames and selects a top predetermined number of actions."
}