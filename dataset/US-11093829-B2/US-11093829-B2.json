{
    "patent_id": "US-11093829-B2",
    "title": "Interaction-aware decision making ",
    "assignee": "Honda Motor Co., Ltd.",
    "publication_date": "2021-08-17",
    "patent_link": "https://patents.google.com/patent/US11093829B2/en",
    "inventors": [
        "Yeping Hu",
        "Alireza Nakhaei Sarvedani",
        "Masayoshi Tomizuka",
        "Kikuo Fujimura"
    ],
    "classifications": [
        "G06N3/006",
        "G06N3/08",
        "B60W10/04",
        "B60W10/18",
        "B60W10/20",
        "B60W30/18109",
        "B60W30/18163",
        "B60W50/00",
        "G05D1/0088",
        "G06N3/084",
        "G06N5/043",
        "B60W2050/0014",
        "B60W2556/00",
        "B60W2710/18",
        "B60W2710/20",
        "B60W2720/106"
    ],
    "abstract": "Interaction-aware decision making may include training a first agent based on a first policy gradient, training a first critic based on a first loss function to learn goals in a single-agent environment using a Markov decision process, training a number N of agents based on the first policy gradient, training a second policy gradient and a second critic based on the first loss function and a second loss function to learn goals in a multi-agent environment using a Markov game to instantiate a second agent neural network, and generating an interaction-aware decision making network policy based on the first agent neural network and the second agent neural network. The N number of agents may be associated with a driver type indicative of a level of cooperation. When a collision occurs, a negative reward or penalty may be assigned to each agent involved based on a lane priority level of respective agents.",
    "claims": "\n1. A method for interaction-aware decision making, comprising:\ntraining a first agent based on a first policy gradient and training a first critic based on a first loss function to learn one or more goals in a single-agent environment, where the first agent is the only agent present, using a Markov decision process,\nwherein the first agent is associated with a first agent neural network and the first critic is associated with a first critic neural network;\ntraining a number N of agents based on the first policy gradient and training a second policy gradient and a second critic based on the first loss function and a second loss function to learn one or more of the goals in a multi-agent environment including the first agent and the N number of agents using a Markov game to instantiate a second agent neural network,\nwherein each one of the N number of agents is associated with a driver type indicative of a level of cooperation for each one of the N number of respective agents;\ngenerating a multi-goal, multi-agent, multi-stage, interaction-aware decision making network policy based on the first agent neural network and the second agent neural network; and\ndetermining a mask to be applied to a subset of a set of possible actions for the first agent for a time interval based on a layout of the multi-agent environment or positioning of the first agent and the N number of agents.\n2. The method for interaction-aware decision making of claim 1, wherein each of the first agent and the N number of agents is associated with a lane priority level based on a position of each one of the N number of respective agents and a layout of the multi-agent environment, and wherein during the training of the number N of agents, when a collision occurs between two or more of the agents of the multi-agent environment, a negative reward or a penalty is assigned to each one of the N number of respective agents involved with the collision based on the lane priority level of each one of the N number of respective agents.\n3. The method for interaction-aware decision making of claim 1, wherein the driver type is cooperative or competitive.\n4. The method for interaction-aware decision making of claim 1, wherein during training of the N number of agents in the multi-agent environment, an agent of the N number of agents changes driver type mid-training.\n5. The method for interaction-aware decision making of claim 1, comprising training the first agent based on a remaining set of actions by excluding the masked set of actions from the set of possible actions.\n6. The method for interaction-aware decision making of claim 1, wherein the set of possible actions includes a no-operation action, an acceleration action, a deceleration action, a brake release action, a shift left one sub-lane action, or a shift right one sub-lane action.\n7. The method for interaction-aware decision making of claim 1, wherein the first critic is a decentralized critic and the second critic is a centralized critic.\n8. The method for interaction-aware decision making of claim 1, wherein training the first agent in the single-agent environment occurs prior to training the N number of agents in the multi-agent environment.\n9. The method for interaction-aware decision making of claim 1, comprising training the N number of agents based on a combined policy gradient derived from the first policy gradient and the second policy gradient.\n10. A system for interaction-aware decision making, comprising:\na processor;\na memory;\na simulator implemented via the processor and memory, performing:\ntraining a first agent based on a first policy gradient and training a first critic based on a first loss function to learn one or more goals in a single-agent environment, where the first agent is the only agent present, using a Markov decision process,\nwherein the first agent is associated with a first agent neural network and the first critic is associated with a first critic neural network;\ntraining a number N of agents based on the first policy gradient and training a second policy gradient and a second critic based on the first loss function and a second loss function to learn one or more of the goals in a multi-agent environment including the first agent and the N number of agents using a Markov game to instantiate a second agent neural network,\nwherein each of the first agent and the N number of agents is associated with a lane priority level based on a position of each one of the N number of respective agents and a layout of the multi-agent environment,\nwherein during the training of the number N of agents, when a collision occurs between two or more of the agents of the multi-agent environment, a negative reward or a penalty is assigned, by the simulator, to each one of the N number of respective agents involved with the collision based on the lane priority level of each one of the N number of respective agents;\ngenerating a multi-goal, multi-agent, multi-stage, interaction-aware decision making network policy based on the first agent neural network and the second agent neural network; and\na Q-Masker determining a mask to be applied to a subset of a set of possible actions for the first agent for a time interval based on a layout of the multi-agent environment or positioning of the first agent and the N number of agents.\n11. The system for interaction-aware decision making of claim 10, wherein each one of the N number of respective agents is associated with a driver type indicative of a level of cooperation for the respective agent, wherein the driver type is cooperative or competitive.\n12. The system for interaction-aware decision making of claim 10, wherein the simulator trains the first agent based on a remaining set of actions by excluding the masked set of actions from the set of possible actions.\n13. The system for interaction-aware decision making of claim 10, wherein the set of possible actions includes a no-operation action, an acceleration action, a deceleration action, a brake release action, a shift left one sub-lane action, or a shift right one sub-lane action.\n14. The system for interaction-aware decision making of claim 10, wherein the first critic is a decentralized critic and the second critic is a centralized critic.\n15. The system for interaction-aware decision making of claim 10, wherein the simulator trains the first agent in the single-agent environment occurs prior to training the N number of agents in the multi-agent environment.\n16. The system for interaction-aware decision making of claim 10, wherein the simulator trains the N number of agents based on a combined policy gradient derived from the first policy gradient and the second policy gradient.\n17. The system for interaction-aware decision making of claim 10, comprising a communication interface transmitting the multi-goal, multi-agent, multi-stage, interaction-aware decision making network policy to a server or a vehicle.\n18. A vehicle for interaction-aware decision making, comprising: a controller including a processor and a memory; one or more vehicle systems; and a vehicle communication interface receiving a multi-goal, multi-agent, multi-stage, interaction-aware decision making network policy, wherein the controller operates one or more of the vehicle systems of the vehicle according to the multi-goal, multi-agent, multi-stage, interaction-aware decision making network policy, and wherein the multi-goal, multi-agent, multi-stage, interaction-aware decision making network policy is generated by: training a first agent based on a first policy gradient and training a first critic based on a first loss function to learn one or more goals in a single-agent environment, where the first agent is the only agent present, using a Markov decision process at a first stage, wherein the first agent is associated with a first agent neural network and the first critic is associated with a first critic neural network; training a number N of agents based on the first policy gradient and training a second policy gradient and a second critic based on the first loss function and a second loss function to learn one or more of the goals in a multi-agent environment including the first agent and the N number of agents using a Markov game to instantiate a second agent neural network at a second stage, wherein each one of the N number of agents is associated with a driver type indicative of a level of cooperation for each one of the N number of respective agents; generating the multi-goal, multi-agent, multi-stage, interaction-aware decision making network policy based on the first agent neural network and the second agent neural network; and\na Q-Masker determining a mask to be applied to a subset of a set of possible actions for the first agent for a time interval based on a layout of the multi-agent environment or positioning of the first agent and the N number of agents.",
    "status": "Active",
    "citations_own": [
        "US6405132B1",
        "US9015093B1",
        "US20160257305A1",
        "US20170031361A1",
        "US9632502B1",
        "US20170236052A1",
        "US20180060701A1",
        "US20180107215A1",
        "US20180121768A1",
        "US20180120843A1",
        "US20180150701A1",
        "US20180373980A1",
        "US20190065951A1",
        "US20190228571A1",
        "US20190243372A1",
        "US20190291727A1"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US20210146964A1",
        "US20220089164A1",
        "US11379995B2",
        "RU2791840C2",
        "WO2023121514A1"
    ],
    "citedby_ftf": [
        "US10678241B2",
        "US20190185012A1",
        "US11130497B2",
        "US11580429B2",
        "CN112585660B",
        "US10733510B2",
        "US10703370B2",
        "US10940863B2",
        "US20200218263A1",
        "US10635915B1",
        "US11074480B2",
        "CN110069064B",
        "US11205319B2",
        "CN110654389B",
        "CN111566583A",
        "CN111045445A",
        "CN111178496A",
        "EP3866070A1",
        "US11400934B2",
        "US11468774B2",
        "CN111967645B",
        "CN112034735B",
        "CN112034888B",
        "CN112182485B",
        "CN112249002B",
        "CN112488320B",
        "CN112700642B",
        "KR102558509B1",
        "CN113264064B",
        "CN113276884B",
        "US20220410878A1",
        "CN113900445A",
        "US20230186416A1",
        "CN114727407B"
    ]
}