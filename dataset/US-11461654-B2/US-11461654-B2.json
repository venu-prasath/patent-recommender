{
    "patent_id": "US-11461654-B2",
    "title": "Multi-agent cooperation decision-making and training method ",
    "assignee": "Peking University",
    "publication_date": "2022-10-04",
    "patent_link": "https://patents.google.com/patent/US11461654B2/en",
    "inventors": [
        "Zongqing LU",
        "Jiechuan JIANG"
    ],
    "classifications": [
        "G06N3/084",
        "G06N3/044",
        "G06F16/9024",
        "G06F17/15",
        "G06F17/16",
        "G06N3/042",
        "G06N3/045",
        "G06N3/047",
        "G06N3/0472",
        "G06N5/043"
    ],
    "abstract": "The present invention provides a multi-agent cooperation decision-making and training method, including the following steps: S1: encoding, by an encoder, local observations obtained by agents by using a multi-layer perceptron or a convolutional neural network as feature vectors in a receptive field; S2: calculating, by a graph convolution layer, relationship strength between the agents by using a relationship unit of a multi-headed attention mechanism, integrating, by a relationship convolution kernel of the relationship unit, the feature vectors in the receptive field into new feature vectors, and iterating the graph convolution layer for multiple times to obtain a relationship description of the multi-headed attention mechanism in a larger receptive field and at a higher order; S3: splicing the feature vectors in the receptive field and the new feature vectors integrated by the graph convolution layer, sending the spliced vectors to a value network, wherein the value network selects and performs an action decision with the highest future feedback expectation; and S4: storing a local observation set and related sets of the agents in a buffer region, collecting samples in the buffer region for training, and optimizing and rewriting a loss function.",
    "claims": "\n1. A multi-agent cooperation decision-making and training method, comprising the following steps:\nS1: encoding, by an encoder, local observations obtained by agents by using a multi-layer perceptron or a convolutional neural network as feature vectors in a receptive field;\nS2: calculating, by a graph convolution layer, relationship strength between the agents by using a relationship unit of a multi-headed attention mechanism, integrating, by a relationship convolution kernel of the relationship unit, the feature vectors in the receptive field into new feature vectors, and iterating the graph convolution layer for multiple times to obtain a relationship description of the multi-headed attention mechanism in a larger receptive field and at a higher order;\nS3: splicing the feature vectors in the receptive field and the new feature vectors integrated by the graph convolution layer, sending the spliced vectors to a value network, wherein the value network selects and performs an action decision with a highest future feedback expectation; and\nS4: storing a local observation set and related sets of the agents in a buffer region, collecting samples in the buffer region for training, and optimizing and rewriting a loss function.\n2. The decision-making and training method according to claim 1, wherein at any moment, if the local observation acquired by each agent is low-dimensional vector data, the encoder performs the encoding by using the multi-layer perceptron;\nif the local observation acquired by each agent is a visual image input, the encoder performs the encoding by using the convolutional neural network.\n3. The decision-making and training method according to claim 1, wherein in each layer of graph convolution operation, each agent acquires the feature vector in the receptive field through a communication channel;\nthe feature vectors of all the agents are spliced into one feature matrix Ft in a size of N\u00d7L,\nwherein N is a total number of the agents in the environment, and L is a length of the feature vector;\nan adjacent matrixCt i in a size of (K+1)\u00d7N is constructed for each agent i, and K is a number of the agents in the receptive field, and t is the time;\nthe first line of the adjacent matrixCt i is expressed by a one-hot of an index of the agent i, and the residual jth line is expressed by a one-hot of an index of an agent j in the receptive field; and a feature vector set Ct i\u00d7Ft in a local region of the agent i is obtained through point multiplication operation.\n4. The decision-making and training method according to claim 3, wherein the relationship strength is expressed by:\n5. The decision-making and training method according to claim 4, wherein the new feature vectors generated by the multi-headed attention mechanism are weighted and averaged according to the relationship strength, and a feature vector hi\u2032 of this layer of graph convolution is obtained through a nonlinear transformation function \u03c3:\n6. The decision-making and training method according to claim 5, wherein the value network generates an expected value of future feedback for each feasible action, and executes an action with a highest expected value by a probability of 1\u2212\u2208, or a random action by a probability of \u2208; and \u2208 represents an execution probability, and is more than or equal to 0 and less than or equal to 1.\n7. The decision-making and training method according to claim 6, wherein after the value network executes each action, a quintuple (O, A, O\u2032, R, C) is stored in the buffer region; 0={o1, o2, . . . , oN} represents a local observation set of the agents at a current time step; A={a1, a2, . . . , aN} represents an action set selected by the agents; O\u2032={o1\u2032, o2\u2032, . . . , oN\u2032} represents a local observation set of the agents at a next time step; R={r1, r2, . . . , rN} represents a real-time environment feedback set obtained by the agents; and C represents a local connection structure of the agents.\n8. The decision-making and training method according to claim 7, wherein the training is performed by using time series differential learning of Q-learning; a small set including S samples is randomly sampled from the buffer region at each time, and the loss function is optimized by a back propagation method:\n9. The decision-making and training method according to claim 8, wherein a regular term, which is a KL divergence represented by a relationship at a higher order in two continuous steps, is added into the loss function, and the loss function is rewritten as:",
    "status": "Active",
    "citations_own": [
        "CN104463191A",
        "US20150193583A1",
        "CN105898288A",
        "CN106970615A",
        "CN108197698A",
        "CN108388900A",
        "US20180285678A1",
        "CN108628823A",
        "US20210349954A1",
        "US20210382963A1",
        "US20220075945A1"
    ],
    "citations_ftf": [
        "CN105225232B",
        "CN105700555B",
        "KR102449377B1",
        "US10546066B2",
        "CN108062505B"
    ],
    "citedby_own": [],
    "citedby_ftf": [
        "US11657266B2",
        "CN110363568B",
        "CN110390340B",
        "CN110554604B",
        "CN110353675B",
        "CN110427006A",
        "JP2021039426A",
        "CN110705310B",
        "CN110811558B",
        "CN111178496A",
        "CN111047014B",
        "CN111709275A",
        "CN111667884B",
        "CN111814988B",
        "CN111966865B",
        "CN112087749B",
        "CN112232478A",
        "CN112115378B",
        "US20220121920A1",
        "CN112241814B",
        "CN112465301B",
        "CN112733764A",
        "CN112784913B",
        "CN112884129B",
        "CN112966641B",
        "CN113095498B",
        "CN113301134B",
        "CN113435475B",
        "CN113254872A",
        "CN113609548B",
        "CN113641192B",
        "CN113392935B",
        "CN113625561B",
        "CN113743468B",
        "CN113625757B",
        "CN113592101B",
        "CN113792844B",
        "CN113515130A",
        "CN113848703A",
        "CN113726894B",
        "CN113848718B",
        "CN113609311A",
        "DE102022211767A1",
        "CN114580937B",
        "CN114741886B",
        "CN114896899B",
        "CN115018017B",
        "CN116361662B"
    ]
}