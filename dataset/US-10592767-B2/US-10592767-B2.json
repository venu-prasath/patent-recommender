{
    "patent_id": "US-10592767-B2",
    "title": "Interpretable counting in visual question answering ",
    "assignee": "Salesforce.Com, Inc.",
    "publication_date": "2020-03-17",
    "patent_link": "https://patents.google.com/patent/US10592767B2/en",
    "inventors": [
        "Alexander Richard Trott",
        "Caiming Xiong",
        "Richard Socher"
    ],
    "classifications": [
        "G06K9/46",
        "G06V10/82",
        "G06F16/3329",
        "G06F18/00",
        "G06K9/00",
        "G06N3/006",
        "G06N3/044",
        "G06N3/0445",
        "G06N3/08",
        "G06N5/04",
        "G06N5/041",
        "G06V20/60",
        "G06T2210/12"
    ],
    "abstract": "Approaches for interpretable counting for visual question answering include a digital image processor, a language processor, and a counter. The digital image processor identifies objects in an image, maps the identified objects into an embedding space, generates bounding boxes for each of the identified objects, and outputs the embedded objects paired with their bounding boxes. The language processor embeds a question into the embedding space. The scorer determines scores for the identified objects. Each respective score determines how well a corresponding one of the identified objects is responsive to the question. The counter determines a count of the objects in the digital image that are responsive to the question based on the scores. The count and a corresponding bounding box for each object included in the count are output. In some embodiments, the counter determines the count interactively based on interactions between counted and uncounted objects.",
    "claims": "\n1. A system for counting objects in a digital image, the system comprising:\na digital image processor that identifies objects in an image, maps the identified objects into an embedding space, generates bounding boxes for each of the identified objects, and outputs the embedded objects paired with their bounding boxes;\na language processor that embeds a question into the embedding space;\na scorer that determines scores for the identified objects, each respective score determines how well a corresponding one of the identified objects is responsive to the question;\na counter that determines a count of the objects in the digital image that are responsive to the question based on the scores; and\nthe system outputs the count and a corresponding bounding box for each object included in the count.\n2. The system of claim 1, wherein the counter:\ndetermines whether to add a first identified object to the count;\niteratively applies an interaction filter to remaining identified objects to determine an uncounted object that is most likely responsive to the question based on each previously counted object;\nwith each iteration, determines whether to add said uncounted object to the count; and\ndetermines when to terminate processing objects in the image.\n3. The system of claim 2, wherein the counter determines the first identified object to count by identifying which of the identified objects has a highest measure of match to the embedding of the question.\n4. The system of claim 3, wherein to determine when to terminate processing objects in the image, the counter determines whether the highest measure of match is less than a termination value.\n5. The system of claim 2, wherein the counter determines the interaction filter for a first unselected object of the identified objects based on the embedding of the question, the embedded object corresponding to the first identified object, the embedded object corresponding to the first unselected object, and overlaps between the bounding boxes of the first identified object and the first unselected object.\n6. The system of claim 1, wherein the counter generates the count based on a weighted sum of the scores.\n7. The system of claim 1, wherein the question is a natural language question.\n8. A method comprising:\nreceiving, by a digital image processor, an image;\nidentifying, by the digital image processor, objects in an image;\nembedding, by the digital image processor, the identified objects into an embedding space;\nidentifying, by the digital image processor, a bounding box for each of the identified objects;\nreceiving, by a language processor, a question;\nmapping, by the language processor, the question into the embedding space;\ndetermining, by a scorer, scores for the identified objects, each respective score determining how well a corresponding one of the identified objects is responsive to the question;\ndetermining, by a counter, a count of the objects in the image that are responsive to the question based on the scores; and\noutputting the count and the bounding box for each object included in the count.\n9. The method of claim 8, wherein determining the count comprises:\ndetermining whether to add a first identified object to the count;\niteratively applying an interaction filter to remaining identified objects to determine an uncounted object that is most likely responsive to the question based on each previously counted object;\nwith each iteration, determining whether to add said uncounted object to the count; and\ndetermining when to terminate counting objects in the image.\n10. The method of claim 9, wherein determining the first identified object to count comprises identifying which of the identified objects has a highest measure of match to the embedding of the question.\n11. The method of claim 10, wherein determining when to terminate processing objects in the image comprises determining whether the highest measure of match is less than a termination value.\n12. The method of claim 9, further comprising determining the interaction filter for a first unselected object of the identified objects based on the embedding of the question, the embedding corresponding to the first identified object, the embedding corresponding to the first unselected object, and overlaps between the bounding box of the first identified object and the bounding box of the first unselected object.\n13. The method of claim 8, wherein generating the count comprises determining a weighted sum of the scores.\n14. The method of claim 8, wherein the question is a natural language question.\n15. A counter comprising:\na logits module that initializes logit values for each of a plurality of candidate objects based on a scoring vector from a scorer, the scoring vector encoding how well a plurality of candidate objects in an image match criteria in a question, each logit value indicating how likely a corresponding candidate object is to be counted;\nan object selector that selects a first object from the candidate objects that has a highest logit value and counts the first object; and\nan interaction module that provides updates to the logit values based on the selection of the first object by the object selector.\n16. The counter of claim 15, wherein:\nthe object selector further iteratively selects one or more additional objects from the candidate objects based on their logit values;\nthe interaction module further provides updates to the logit values with the selection of each of the one or more additional objects; and\nthe object selector stops iteratively selecting the one or more additional objects when logit values of each of the candidate objects is less than a termination value.\n17. The counter of claim 16, wherein the termination value is trainable.\n18. The counter of claim 15, further comprising:\na weighting unit that receives the scoring vector from the scorer and applies trainable weights to each score in the scoring vector;\na summing unit that adds a trainable bias to the weighted scoring vector;\na transfer function that is applied to an output of the summing unit and generates initial logits values for the logits module.\n19. The counter of claim 15, wherein the interaction module determines an update to a first logit value of a first unselected object based on an embedding of the question, an object embedding corresponding to the first object, an object embedding corresponding to the first unselected object, and overlaps between bounding boxes of the first object and the first unselected object.\n20. The counter of claim 15, wherein the object selector further outputs a bounding box correspond to the first object.",
    "status": "Active",
    "citations_own": [
        "US20110161076A1",
        "US20130330008A1",
        "US20160078127A1",
        "US20160342895A1",
        "US20160350653A1",
        "US20170024645A1",
        "US20170032280A1",
        "US20170124432A1",
        "US20170140240A1",
        "US20180082171A1",
        "US20180096267A1",
        "US20180121788A1",
        "US20180129937A1",
        "US20180129938A1",
        "US20180144248A1",
        "US20180268298A1",
        "US20180300400A1",
        "US10198671B1",
        "US20190043201A1",
        "US20190108439A1",
        "US20190108432A1",
        "US20190130248A1",
        "US20190130897A1",
        "US20190130896A1",
        "US10282663B2",
        "US20190149834A1",
        "US10346721B2"
    ],
    "citations_ftf": [
        "US8355550B2",
        "US8121367B2",
        "KR20200094747A"
    ],
    "citedby_own": [
        "US20210081728A1",
        "US11256754B2",
        "US11263476B2",
        "US11328731B2",
        "US11409945B2",
        "US11416688B2",
        "US11481636B2",
        "US11487999B2",
        "US11537801B2",
        "US11562147B2",
        "US11573957B2",
        "US11625436B2",
        "US11625543B2",
        "US11640505B2",
        "US11669745B2",
        "US11720559B2",
        "US11775617B1"
    ],
    "citedby_ftf": [
        "US10565305B2",
        "US10565318B2",
        "US11386327B2",
        "US11417235B2",
        "US11604956B2",
        "US11562287B2",
        "US10542270B2",
        "US11276002B2",
        "WO2019148315A1",
        "US10776581B2",
        "US11227218B2",
        "US10929607B2",
        "US10783875B2",
        "US11106182B2",
        "US10909157B2",
        "US10970486B2",
        "US11436481B2",
        "US11514915B2",
        "US11029694B2",
        "US11645509B2",
        "US11087177B2",
        "CN109800294B",
        "US11568306B2",
        "US11003867B2",
        "US11366969B2",
        "US11580445B2",
        "US11087092B2",
        "US11232308B2",
        "US11281863B2",
        "US11487939B2",
        "US11620572B2",
        "US11604965B2",
        "US11562251B2",
        "US11687588B2",
        "US11775775B2",
        "US11669712B2",
        "US11657269B2",
        "CN110348535B",
        "CN110414684A",
        "US11615240B2",
        "CN110516791B",
        "US11568000B2",
        "US11599792B2",
        "US11640527B2",
        "US11620515B2",
        "US11347708B2",
        "US11288438B2",
        "US11334766B2",
        "US10699129B1"
    ]
}