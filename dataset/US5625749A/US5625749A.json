{
    "patent_link": "https://patents.google.com/patent/US5625749A/en",
    "patent_id": "US5625749A",
    "title": "Segment-based apparatus and method for speech recognition by analyzing multiple speech unit frames and modeling both temporal and spatial correlation",
    "abstract": "Phonetic recognition is provided by capturing dynamical behavior and statistical dependencies of the acoustic attributes used to represent a subject speech waveform. A segment based framework is employed. Temporal behavior is modelled explicitly by creating dynamic templates, called tracks, of the acoustic attributes used to represent the speech waveform, and by generating the estimation of the acoustic spatio-temporal correlation structure. An error model represents this estimation as the temporal and spatial correlations between the input speech waveform and track generated speech segment. Models incorporating these two components (track and error estimation) are created for both phonetic units and for phonetic transitions. Phonetic contextual influences are accounted for by merging context-dependent tracks and pooling error statistics over the different contexts. This allows for a large number of contextual models without compromising the robustness of the statistical parameter estimates. The transition models also supply contextual information.",
    "inventors": [
        "William D. Goldenthal",
        "James R. Glass"
    ],
    "assignee": "Massachusetts Institute of Technology",
    "classifications": [
        "G10L15/02",
        "G10L15/148",
        "G10L2015/025"
    ],
    "claims": "\n1. In a digital processor, speech recognition apparatus for decoding an input speech signal to a corresponding speech unit, the apparatus comprising:\na source providing an input speech signal formed of multiple observation frames;\na plurality of unit templates, each unit template for representing acoustic attributes of a respective speech unit and each unit template generating a respective synthetic segment indicative of the respective speech unit;\na plurality of error models associated with the unit templates, each unit template having an error model for explicitly measuring and quantitatively representing temporal and spatial correlations between the synthetic segments and a subject speech signal, the temporal and spatial correlations being between acoustic attributes in the observation frames of the subject speech signal; and\nprocessor means coupled to the unit templates and error models and coupled to the source to receive the input speech signal, the processor means comparing the synthetic segments to different plural observation frames of the input speech signal to define a set of error sequences and based on the error models, the processor means analyzing the error sequences and determining the corresponding speech unit of the input speech signal.\n2. Apparatus as claimed in claim 1 wherein the unit templates employ a generation function to generate the synthetic segments.\n3. Apparatus as claimed in claim 2 wherein the generation function is used to form each unit template.\n4. Apparatus as claimed in claim 1 wherein each error model is formed from a probability density function; and\nthe processor means determines the corresponding speech unit of the input speech signal to be the respective speech unit of the unit template corresponding to the most likely error model.\n5. Apparatus as claimed in claim 1 wherein each error model is formed from a distance metric; and\nthe processor means determines the corresponding speech unit of the input speech signal to be the respective speech unit of the unit template corresponding to the best error model.\n6. Apparatus as claimed in claim 1 wherein each error sequence is normalized to a single error feature vector of fixed dimension before the processor means generates the error models.\n7. Apparatus as claimed in claim 1 wherein the plurality of unit templates includes transition unit templates for representing acoustic transition dynamics between speech units within a speech signal.\n8. Apparatus as claimed in claim 7 wherein the transition unit templates provide an indication of one of location of a transition in the input speech signal and speech units involved in the transition.\n9. Apparatus as claimed in claim 1 further comprising a multiplicity of merged templates formed by a combination of a plurality of unit templates.\n10. Apparatus as claimed in claim 1 wherein certain ones of the unit templates are templates for representing context-dependent acoustic attributes of a respective speech unit.\n11. Apparatus as claimed in claim 1 wherein the respective speech unit for each unit template is a phonetic unit or a string of phonetic units.\n12. In a digital processor, a method for decoding an input speech signal to a corresponding speech unit comprising the steps of:\nproviding an input speech signal formed of multiple observation frames;\nproviding a plurality of unit templates in stored memory of the digital processor, each unit template for representing acoustic attributes of a respective speech unit and for generating a respective target speech unit;\nproviding a plurality of error models associated with the unit templates in stored memory, each unit template having an error model for explicitly measuring and quantitatively representing temporal and spatial correlations between the synthetic segments and a subject speech signal, the temporal and spatial correlations being between acoustic attributes in the observation frames of the subject speech signal;\nreceiving the input speech signal in working memory of the digital processor;\ncomparing the target speech units with different plural observation frames of the input speech signal in working memory such that the comparison defines a set of error sequences in working memory;\nand\nusing the error models, analyzing the error sequences and determining the corresponding speech unit of the input speech signal.\n13. A method as claimed in claim 12 wherein the unit templates employ a generation function to generate the target speech units.\n14. A method as claimed in claim 13 wherein the generation function is used to form each unit template.\n15. A method as claimed in claim 12 wherein:\nthe step of generating the error models includes forming each error model from a probability density function; and\nthe step of determining the corresponding speech unit includes determining a most likely error model such that the respective speech unit of the unit template corresponding to the most likely error model is the corresponding speech unit of the input speech signal.\n16. A method as claimed in claim 12 wherein:\nthe step of generating the error models includes forming each error model from a distance metric; and\nthe step of determining the corresponding speech unit includes determining a best error model, such that the respective speech unit of the unit template corresponding to the best error model is the corresponding speech unit of the input speech signal.\n17. A method as claimed in claim 12 further comprising the step of normalizing each error sequence to a single error feature vector of fixed dimension before generating the error models.\n18. A method as claimed in claim 17 wherein the step of normalizing includes averaging across each error sequence.\n19. A method as claimed in claim 12 wherein the step of providing a plurality of unit templates includes providing transition unit templates for representing acoustic transition dynamics between speech units within a speech signal.\n20. A method as claimed in claim 19 wherein the transition unit templates provide an indication of one of location of a transition in the input speech signal and speech units involved in the transition.\n21. A method as claimed in claim 12 wherein the step of providing a plurality of unit templates includes combining a plurality of unit templates to form a multiplicity of merged templates that account for contextual effects on the respective speech units of the unit templates.\n22. A method as claimed in claim 12 wherein the step of providing a plurality of unit templates includes providing a multiplicity of templates for representing context dependent acoustic attributes of a respective speech unit.\n23. A method as claimed in claim 12 wherein the step of providing a plurality of unit templates includes providing phonetic unit templates for representing one of phonetic units of speech and strings of phonetic units of speech.\n24. In a digital processor, speech recognition apparatus for decoding an input speech signal to a corresponding speech unit, the apparatus comprising:\na source providing an input speech signal formed of multiple observation frames;\na plurality of unit templates, each unit template for representing acoustic attributes of a respective speech unit and each unit template generating a respective synthetic segment indicative of the respective speech unit;\na plurality of error models associated with the unit templates, each unit template having an error model; and\nprocessor means coupled to the unit templates and error models and coupled to the source to receive the input speech signal, the processor means comparing the synthetic segments to different plural observation frames of the input speech signal to define a set of error sequences, the processor means transforming each error sequence to a fixed dimension error feature vector independent of the number of observation frames, and based on the error models, the processor means computing a score for the error feature vector.\n25. The apparatus of claim 24 wherein each error model explicitly measures and quantitatively represents temporal and spatial correlations between the synthetic segments and a subject speech signal, the temporal and spatial correlations being between acoustic attributes in the observation frames of the subject speech signal.\n26. The apparatus of claim 25 wherein the temporal and spatial correlations are between different acoustic attributes in different observation frames of the subject speech signal.",
    "status": "Expired - Fee Related",
    "citations_own": [
        "US4994983A",
        "US5023911A",
        "US5036539A",
        "US5199077A",
        "US5333236A"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "WO1997033237A1",
        "WO1997040491A1",
        "US5749073A",
        "US5857173A",
        "US5884267A",
        "US6054646A",
        "WO2000031723A1",
        "US6128595A",
        "US6173076B1",
        "WO2001006337A1",
        "WO2001027874A1",
        "US20010032076A1",
        "US6308154B1",
        "US6314392B1",
        "US20010044719A1",
        "US6405168B1",
        "US20020091521A1",
        "US20020111806A1",
        "US6438523B1",
        "US6456970B1",
        "US20020143539A1",
        "US20020198615A1",
        "US20030028375A1",
        "US6560575B1",
        "US20030115169A1",
        "US20030130843A1",
        "WO2003079329A1",
        "US6629073B1",
        "US20030187647A1",
        "US6633845B1",
        "US6633839B2",
        "US6662158B1",
        "US6829578B1",
        "US6832191B1",
        "US6832190B1",
        "US6845358B2",
        "US6934681B1",
        "US6975983B1",
        "US6999620B1",
        "US7065485B1",
        "US20070055502A1",
        "US20070239445A1",
        "US20070288237A1",
        "US7318032B1",
        "US20080059185A1",
        "US20090070116A1",
        "US20090208913A1",
        "US20100125457A1",
        "US20100312550A1",
        "US20110196668A1",
        "US8494850B2",
        "US8676580B2",
        "US8886533B2",
        "US9055147B2",
        "US20150371633A1",
        "US9484019B2",
        "CN106409291A",
        "US9858922B2",
        "US20180130474A1",
        "US10204619B2",
        "US10223934B2",
        "US10529357B2",
        "US10573336B2",
        "US11341958B2",
        "US11348160B1",
        "US11355122B1",
        "US11355120B1",
        "US11354760B1",
        "US20220293083A1",
        "US11514894B2",
        "US11810550B2"
    ],
    "citedby_ftf": []
}