{
    "patent_id": "US-11768504-B2",
    "title": "Light weight and real time slam for robots ",
    "assignee": "AI Incorporated",
    "publication_date": "2023-09-26",
    "patent_link": "https://patents.google.com/patent/US11768504B2/en",
    "inventors": [
        "Ali Ebrahimi Afrouzi",
        "Lukas Fath",
        "Andrew Fitzgerald",
        "Amin Ebrahimi Afrouzi",
        "Brian Highfill"
    ],
    "classifications": [
        "G05D1/0274",
        "G05D1/0248",
        "B25J11/0085",
        "B25J13/087",
        "B25J9/1664",
        "G05D1/0016",
        "G05D1/0022",
        "G05D1/0044",
        "G05D1/0219",
        "G05D1/024",
        "G05D1/027",
        "G05D1/0272",
        "G06F9/5016",
        "G06F9/5038",
        "G06F9/5066",
        "G06N3/006",
        "G06N3/044",
        "G06N3/045",
        "G06N3/047",
        "G06N3/048",
        "G06N3/082",
        "G06N3/084",
        "G06N3/088",
        "G06N7/01",
        "G05D2201/0203",
        "G05D2201/0215",
        "G06F2209/509",
        "G06N10/20",
        "G06N5/01"
    ],
    "abstract": "Some aspects include a method for operating a cleaning robot, including: capturing LIDAR data; generating a first iteration of a map of the environment in real time; capturing sensor data from different positions within the environment; capturing movement data indicative of movement of the cleaning robot; aligning and integrating newly captured LIDAR data with previously captured LIDAR data at overlapping points; generating additional iterations of the map based on the newly captured LIDAR data and at least some of the newly captured sensor data; localizing the cleaning robot; planning a path of the cleaning robot; and actuating the cleaning robot to drive along a trajectory that follows along the planned path by providing pulses to one or more electric motors of wheels of the cleaning robot.",
    "claims": "\n1. A method for operating a cleaning robot, comprising:\ncapturing, by a LIDAR of the cleaning robot, LIDAR data as the cleaning robot performs work within an environment of the cleaning robot, wherein the LIDAR data is indicative of distance from a perspective of the LIDAR to obstacles immediately surrounding the cleaning robot and within reach of a maximum range of the LIDAR;\ngenerating, by a processor of the cleaning robot, a first iteration of a map of the environment in real time at a first position of the cleaning robot based on the LIDAR data and at least some sensor data captured by sensors of the cleaning robot, wherein the map is a bird's-eye view of the environment;\ncapturing, by at least some of the sensors of the cleaning robot, sensor data from different positions within the environment as the cleaning robot performs work in the environment, wherein:\nnewly captured sensor data partly overlaps with previously captured sensor data;\nat least a portion of the newly captured sensor data comprises distances to obstacles that were not visible by the sensors from a previous position of the robot from which the previously captured sensor data was obtained; and\nthe newly captured sensor data is integrated into a previous iteration of the map to generate a larger map of the environment;\ncapturing, by at least one of an IMU sensor, a gyroscope, and a wheel encoder of the cleaning robot, movement data indicative of movement of the cleaning robot;\naligning and integrating, with the processor, newly captured LIDAR data captured from consecutive positions of the cleaning robot with previously captured LIDAR data captured from previous positions of the cleaning robot at overlapping points between the newly captured LIDAR data and the previously captured LIDAR data;\ngenerating, by the processor, additional iterations of the map based on the newly captured LIDAR data and at least some of the newly captured sensor data captured as the cleaning robot traverses into new and undiscovered areas of the environment, wherein successive iterations of the map are larger in size due to the addition of newly discovered areas;\nidentifying, by the processor, a room in the map based on at least a portion of any of the LIDAR data, the sensor data, and the movement data;\ndetermining, by the processor, all areas of the environment are discovered and included in the map based on at least all the newly captured LIDAR data overlapping with the previously captured LIDAR data;\nlocalizing, by the processor, the cleaning robot within the map of the environment in real time and simultaneously to generating the map based on the LIDAR data, at least some of the sensor data, and the movement data;\nplanning, by the processor, a path of the cleaning robot;\nactuating, by the processor, the cleaning robot to drive along a trajectory that follows along the planned path by providing pulses to one or more electric motors of wheels of the cleaning robot;\nwherein:\nthe processor is a processor of a single microcontroller;\nthe processor of the robot executes a simultaneous localization and mapping task in concurrence with a path planning task, an obstacle avoidance task, a coverage tracker task, a control task, and a cleaning operation task by time-sharing computational resources of the single microcontroller;\na scheduler assigns a time slice of the single microcontroller to each of the simultaneous localization and mapping task, the path planning task, the obstacle avoidance task, the coverage tracker task, the control task, and the cleaning operation task according to an importance value assigned to each task;\nthe scheduler preempts lower priority tasks with higher priority tasks, preempts all tasks by an interrupt service request when invoked, and runs a routine associated with the interrupt service request;\na coverage tracker executed by the processor deems an operational session complete and transitions the cleaning robot to a state that actuates the cleaning robot to find a charging station;\nthe map is stored in a memory accessible to the processor during a subsequent operational session of the cleaning robot;\nthe map is transmitted to an application of a smart phone device previously paired with the processor of the robot using a wireless card coupled with the single microcontroller via the internet or a local network; and\nthe application is configured to display the map on a screen of the smart phone.\n2. The method of claim 1, wherein the processor initializes an operation of the cleaning robot at a next operational session by attempting to relocalize the robot in a previously stored map.\n3. The method of claim 2, wherein the processor of the robot creates a new map at a next operational session upon failing to relocalize within the robot within the previously stored map.\n4. The method of claim 1, wherein identified rooms in the map are distinguished by using a different color to represent each identified room in the map.\n5. The method of claim 1, wherein the simultaneous localization and mapping task is bound by one of a hard time constraint, a firm time constraint, or a soft time constraint of real time computing.\n6. The method of claim 1, wherein a finite state machine executed within the single microcontroller causes the cleaning robot to transition from one state to another state based on events and a current state of the cleaning robot.\n7. The method of claim 1, wherein the processor continuously monitors a difference between the planned path and the trajectory of the cleaning robot and the processor corrects a location of the cleaning robot based on the trajectory of the cleaning robot as opposed to the planned path.\n8. The method of claim 1, wherein the simultaneous localization and mapping task in concurrence with the coverage tracking task avoids, minimizes, or controls an amount of overlap in coverage by the cleaning robot.\n9. The method of claim 1, wherein the sensor data captured by the sensors and the LIDAR data captured by the LIDAR are obtained and processed on the single microcontroller executing the simultaneous localization and mapping tasks.\n10. The method of claim 1, wherein actuation and control of any of a main brush motor, a side brush motor, a fan motor, and a wheel motor are processed on the single microcontroller executing the simultaneous localization and mapping tasks.\n11. The method of claim 1, wherein:\nan actuator of the cleaning robot causes the cleaning robot to move along the planned path or a portion of the planned path;\nthe processor determines a distance travelled by the cleaning robot using odometry data; and\nthe actuator of the robot to causes the cleaning robot to stop moving after traveling a distance equal to a length of the planned path or the portion of the planned path or an updated planned path.\n12. The method of claim 1, wherein the cleaning robot cleans a first room prior to cleaning a next room, wherein rooms in the map are partially bounded by a gap.\n13. The method of claim 1, wherein:\nthe processor identifies rooms in the map based on detected boundaries and sensor data indicating hallways and doorways; and\nthe processor proposes a default segmentation of the map into areas based on the identified rooms, the doorways, and the hallways.\n14. The method of claim 1, wherein the scheduler preempts execution of a lower priority task when a higher priority task arrives.\n15. The method of claim 1, wherein the tasks communicate elements within a large data structure in queues by referencing their location in memory using a pointer.\n16. The method of claim 1, wherein the tasks communicate elements within a small data structure directly between one another without instantiating them in a random access memory.\n17. The method of claim 1, wherein data is configured to flow from one electronic address to another by direct memory access.\n18. The method of claim 1, wherein data is transferred between any of a memory to a peripheral, a peripheral to a memory, or a first memory to a second memory.\n19. The method of claim 1, wherein direct memory access is used to reduce usage of computational resources of the single microcontroller.\n20. The method of claim 1, wherein any of components, peripherals, and sensors of the cleaning robot are shut down or put in a standby mode when the cleaning robot is charging or in a standby mode.\n21. The method of claim 1, wherein a clock rate of the single microcontroller is reduced when the robot is in a charging mode or a standby mode.\n22. The method of claim 1, wherein a graphical user interface the application comprises any of: a toggle icon to transition between two states of the cleaning robot, a linear or round slider to set a value from a range of minimum to maximum, multiple choice check boxes to choose multiple setting options, radio buttons to allow a single selection from a set of possible choices, a color theme, an animation theme, an accessibility theme, a power usage theme, a usage mode option, and an invisible mode option wherein the cleaning robot cleans when people are not home.\n23. The method of claim 1, wherein the processor uses data from a temperature sensor positioned on any of a battery, a motor, or another component of the cleaning robot to monitor their respective temperatures.\n24. The method of claim 1, wherein a Hall sensor is used to measure AC/DC currents in an open or a closed loop setup and the processor determines rotational velocity, change of position, or acceleration of the cleaning robot based on the measurements.\n25. The method of claim 1, wherein some data processing of the map is offloaded from the local cleaning robot to the cloud.\n26. The method of claim 1, wherein the processor uses a network of connected computational nodes connected organized in at least three logical layers and processing units to enhance any of perception of the environment, internal and external sensing, localization, mapping, path planning, and actuation of the cleaning robot.\n27. The method of claim 26, wherein the computational nodes are activated by a Rectified Linear Unit through a backpropagation learning process.\n28. The method of claim 26, wherein the at least three layers comprise at least one convolution layer.\n29. A tangible, non-transitory, machine readable medium storing instructions that when executed by a processor of a cleaning robot effectuates operations comprising:\ncapturing, by a LIDAR of the cleaning robot, LIDAR data as the cleaning robot performs work within an environment of the cleaning robot, wherein the LIDAR data is indicative of distance from a perspective of the LIDAR to obstacles immediately surrounding the cleaning robot and within reach of a maximum range of the LIDAR;\ngenerating, by the processor, a first iteration of a map of the environment in real time at a first position of the cleaning robot based on the LIDAR data and at least some sensor data captured by sensors of the cleaning robot, wherein the map is a bird's-eye view of the environment;\ncapturing, by at least some of the sensors of the cleaning robot, sensor data from different positions within the environment as the cleaning robot performs work in the environment, wherein:\nnewly captured sensor data partly overlaps with previously captured sensor data;\nat least a portion of the newly captured sensor data comprises distances to obstacles that were not visible by the sensors from a previous position of the robot from which the previously captured sensor data was obtained; and\nthe newly captured sensor data is integrated into a previous iteration of the map to generate a larger map of the environment;\ncapturing, by at least one of an IMU sensor, a gyroscope, and a wheel encoder of the cleaning robot, movement data indicative of movement of the cleaning robot;\naligning and integrating, with the processor, newly captured LIDAR data captured from consecutive positions of the cleaning robot with previously captured LIDAR data captured from previous positions of the cleaning robot at overlapping points between the newly captured LIDAR data and the previously captured LIDAR data;\ngenerating, by the processor, additional iterations of the map based on the newly captured LIDAR data and at least some of the newly captured sensor data captured as the cleaning robot traverses into new and undiscovered areas of the environment, wherein successive iterations of the map are larger in size due to the addition of newly discovered areas;\nidentifying, by the processor, a room in the map based on at least a portion of any of the LIDAR data, the sensor data, and the movement data;\ndetermining, by the processor, all areas of the environment are discovered and included in the map based on at least all the newly captured LIDAR data overlapping with the previously captured LIDAR data;\nlocalizing, by the processor, the cleaning robot within the map of the environment in real time and simultaneously to generating the map based on the LIDAR data, at least some of the sensor data, and the movement data;\nplanning, by the processor, a path of the cleaning robot;\nactuating, by the processor, the cleaning robot to drive along a trajectory that follows along the planned path by providing pulses to one or more electric motors of wheels of the cleaning robot;\nwherein:\nthe processor is a processor of a single microcontroller;\nthe processor of the robot executes a simultaneous localization and mapping task in concurrence with a path planning task, an obstacle avoidance task, a coverage tracker task, a control task, and a cleaning operation task by time-sharing computational resources of the single microcontroller;\na scheduler assigns a time slice of the single microcontroller to each of the simultaneous localization and mapping task, the path planning task, the obstacle avoidance task, the coverage tracker task, the control task, and the cleaning operation task according to an importance value assigned to each task;\nthe scheduler preempts lower priority tasks with higher priority tasks, preempts all tasks by an interrupt service request when invoked, and runs a routine associated with the interrupt service request;\na coverage tracker executed by the processor deems an operational session complete and transitions the cleaning robot to a state that actuates the cleaning robot to find a charging station;\nthe map is stored in a memory accessible to the processor during a subsequent operational session of the cleaning robot;\nthe map is transmitted to an application of a smart phone device previously paired with the processor of the robot using a wireless card coupled with the single microcontroller via the internet or a local network; and\nthe application is configured to display the map on a screen of the smart phone.\n30. A cleaning robot, comprising:\na chassis;\na set of wheels;\na LIDAR;\nsensors;\na processor; and\na tangible, non-transitory, machine readable medium storing instructions that when executed by the processor effectuates operations comprising:\ncapturing, by the LIDAR, LIDAR data as the cleaning robot performs work within an environment of the cleaning robot, wherein the LIDAR data is indicative of distance from a perspective of the LIDAR to obstacles immediately surrounding the cleaning robot and within reach of a maximum range of the LIDAR;\ngenerating, by the processor, a first iteration of a map of the environment in real time at a first position of the cleaning robot based on the LIDAR data and at least some sensor data captured by the sensors, wherein the map is a bird's-eye view of the environment;\ncapturing, by at least some of the sensors, sensor data from different positions within the environment as the cleaning robot performs work in the environment, wherein:\nnewly captured sensor data partly overlaps with previously captured sensor data;\nat least a portion of the newly captured sensor data comprises distances to obstacles that were not visible by the sensors from a previous position of the robot from which the previously captured sensor data was obtained; and\nthe newly captured sensor data is integrated into a previous iteration of the map to generate a larger map of the environment;\ncapturing, by at least one of an IMU sensor, a gyroscope, and a wheel encoder of the cleaning robot, movement data indicative of movement of the cleaning robot;\naligning and integrating, with the processor, newly captured LIDAR data captured from consecutive positions of the cleaning robot with previously captured LIDAR data captured from previous positions of the cleaning robot at overlapping points between the newly captured LIDAR data and the previously captured LIDAR data;\ngenerating, by the processor, additional iterations of the map based on the newly captured LIDAR data and at least some of the newly captured sensor data captured as the cleaning robot traverses into new and undiscovered areas of the environment, wherein successive iterations of the map are larger in size due to the addition of newly discovered areas;\nidentifying, by the processor, a room in the map based on at least a portion of any of the LIDAR data, the sensor data, and the movement data;\ndetermining, by the processor, all areas of the environment are discovered and included in the map based on at least all the newly captured LIDAR data overlapping with the previously captured LIDAR data;\nlocalizing, by the processor, the cleaning robot within the map of the environment in real time and simultaneously to generating the map based on the LIDAR data, at least some of the sensor data, and the movement data;\nplanning, by the processor, a path of the cleaning robot;\nactuating, by the processor, the cleaning robot to drive along a trajectory that follows along the planned path by providing pulses to one or more electric motors of wheels of the cleaning robot;\nwherein:\nthe processor is a processor of a single microcontroller;\nthe processor of the robot executes a simultaneous localization and mapping task in concurrence with a path planning task, an obstacle avoidance task, a coverage tracker task, a control task, and a cleaning operation task by time-sharing computational resources of the single microcontroller;\na scheduler assigns a time slice of the single microcontroller to each of the simultaneous localization and mapping task, the path planning task, the obstacle avoidance task, the coverage tracker task, the control task, and the cleaning operation task according to an importance value assigned to each task;\nthe scheduler preempts lower priority tasks with higher priority tasks, preempts all tasks by an interrupt service request when invoked, and runs a routine associated with the interrupt service request;\na coverage tracker executed by the processor deems an operational session complete and transitions the cleaning robot to a state that actuates the cleaning robot to find a charging station;\nthe map is stored in a memory accessible to the processor during a subsequent operational session of the cleaning robot;\nthe map is transmitted to an application of a smart phone device previously paired with the processor of the robot using a wireless card coupled with the single microcontroller via the internet or a local network; and\nthe application is configured to display the map on a screen of the smart phone."
}