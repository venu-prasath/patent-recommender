{
    "patent_id": "US-11593631-B2",
    "title": "Explainable transducer transformers ",
    "assignee": "UMNAI Limited",
    "publication_date": "2023-02-28",
    "patent_link": "https://patents.google.com/patent/US11593631B2/en",
    "inventors": [
        "Angelo Dalli",
        "Matthew GRECH",
        "Mauro PIRRONE"
    ],
    "classifications": [
        "G06N3/065",
        "G06N3/084",
        "G06N3/0635",
        "G06F40/40",
        "G06N3/04",
        "G06N3/042",
        "G06N3/044",
        "G06N3/045",
        "G06N3/049",
        "G06N3/08",
        "G06N3/086",
        "G06N3/088",
        "G06N3/092",
        "G06N3/10",
        "G06N5/01",
        "G06N5/025",
        "G06N5/042",
        "G06N5/045",
        "G06N3/047",
        "G06N5/022"
    ],
    "abstract": "An explainable transducer transformer (XTT) may be a finite state transducer, together with an Explainable Transformer. Variants of the XTT may include an explainable Transformer-Encoder and an explainable Transformer-Decoder. An exemplary Explainable Transducer may be used as a partial replacement in trained Explainable Neural Network (XNN) architectures or logically equivalent architectures. An Explainable Transformer may replace black-box model components of a Transformer with white-box model equivalents, in both the sub-layers of the encoder and decoder layers of the Transformer. XTTs may utilize an Explanation and Interpretation Generation System (EIGS), to generate explanations and filter such explanations to produce an interpretation of the answer, explanation, and its justification.",
    "claims": "\n1. A system for providing an explainable transducer-transformer comprising a processor and a memory configured to provide:\nan input layer configured to receive an input and identify one or more input features, and provide the input to a conditional network and a prediction network configured to operate concurrently;\nthe conditional network, comprising: a conditional layer configured to model and evaluate the input features based on one or more partitions, wherein each of the one or more partitions comprises a rule provided in the form of an antecedent IF-condition and a consequent THEN-part having at least one output; an aggregation layer configured to aggregate one or more rules into one or more aggregated partitions; and a switch output layer configured to selectively pool the aggregated partitions from the aggregation layer with the one or more partitions from the conditional layer;\nthe prediction network, comprising: a feature generation and transformation network comprising one or more transformation neurons configured to apply one or more transformations to the input features; a fit layer configured to combine features which have been transformed by the feature generation and transformation network to identify one or more coefficients related to at least one of: one or more features and one or more partitions; a value output layer configured to analyze the one or more coefficients and configured to output a value related to at least one of the one or more features or the one or more partitions; and\nat least one of an encoder layer and a decoder layer, wherein the at least one of the encoder layer and decoder layer comprises an explainable architecture formed from the input; and\nan output layer configured to generate an output which is interpretable and explainable by at least one of a machine program or a human based on a combination of a switch output layer output and a value output layer output;\nwherein one or more execution paths throughout the partitions are identifiable by an external process;\nwherein a difference and/or change between a current explanation and a previously presented explanation is used as input to the explainable transducer-transformer to predict a subsequent change in explanations; and\nwherein a sparse embedded representation of the input and/or the output identifies one or more similarity and contrast elements.\n2. The system for providing the explainable transducer-transformer of claim 1,\nwherein the explainable-transducer transformer is configured to execute a plurality of tasks or modalities using a same set of parameters, wherein a plurality of inputs correspond to one or more tasks associated with one or more input features and a plurality of outputs corresponding to each task;\nwherein the explainable-transducer transformer further comprises a hierarchical partition structure and a crossover structure sub-system configured to crossover knowledge learnt for different tasks;\nwherein the prediction network is configured to identify one or more feature interactions within the hierarchical partition structure and the conditional network is configured to select, orchestrate, and multiplex a path trace through the hierarchical partition structure to link a task with associated inputs and outputs; and\nwherein a sparse explainable model is implemented in at least one of the conditional network and prediction network, wherein the crossover structure sub-system implements at least one of a unidirectional link node, a bidirectional link node, a crossover noise node, and an inter-crossover node link.\n3. The system for providing the explainable transducer-transformer of claim 1, further comprising:\na convolutional layer configured to apply one or more convolution layers and configured to implement: a backmap or reverse indexing mechanism for generating explanations and a kernel labelling method configured to associate human-readable labels with non-textual data using a progressive refinement of kernels, patterns, symbols, and concepts; and\nwherein one or more kernel types comprising at least one of linear, non-linear polynomial, exponential, radial-basis-function, or sigmoid kernels are implemented to reduce an amount of computation and/or resources required using at least one of a quadrature method, a singular-value decomposition, a random Fourier transform, or random binning features; and\nfurther comprising one or more temporal convolutional networks.\n4. The system for providing the explainable transducer-transformer of claim 1, wherein one or more explanations provided by the explainable transducer-transformer comprise scenario-based explanations associated with one or more what-if, what-if-not, counterfactual, but-for, or conditional scenarios for generating explained strategies and scenario-based explanations in accordance with the actions and decisions of an explainable agent; and/or\nwherein the explainable agent is trained to learn suggested actions for a given user with a specific context leading to a change in decision outcome and minimizing total cost of actions, wherein the total costs is an amalgamation of one or more costs associated with each variable based on a metric for each type of cost; and/or\nwherein the system further comprises a connection to an AutoXAI system configured to use scenario-based explanations, comprising use of at least one of a nearest-neighbor method, Identify-Assess-Recommend-Resolve (IAR) framework, Multiple Objective Optimization (MOO), Pareto Front Method, Particle Swarm Optimization (PSO), Genetic Algorithms (GA), Bayesian Optimization, Evolutionary Strategies, Gradient Descent techniques and Monte Carlo Simulation (MCS).\n5. The system for providing the explainable transducer-transformer of claim 1, wherein the at least one of the encoder layer and the decoder layer comprises each of the encoder and the decoder layer;\nwherein the encoder layer is at least one of an inline explainable encoder layer or a parallel explainable encoder layer configured to receive at least one of an output of a multi-head attention component, output of an add-and-normalize component, and explainable information or partitioning information of an input, and\nwherein the decoder layer is at least one of an inline explainable decoder layer or a parallel explainable decoder layer configured to receive an output from the encoder layer and comprises an add-and-normalize component and a multi-head attention component, and wherein the output of the encoder layer is merged with an output of the decoder layer.\n6. The system for providing the explainable transducer-transformer of claim 1,\nwherein the decoder layer is trained on a set of explanation gradients to predict a next gradient given the current explanation and a context window comprising historic explanation gradients.\n7. The system for providing the explainable transducer-transformer of claim 1,\nwherein one or more explanations, a gradient of explanations, or partitioning information of an explainable architecture is used as input to the encoder layer;\nwherein an explainable auto-encoder-decoder (XAED) comprises the encoder layer and decoder layer; and\nwherein the output that is interpretable and explainable by the at least one of the machine program or the human comprises one or more training dataset samples configured to be used as a training input to one or more layers of the explainable transducer-transformer.\n8. The system for providing the explainable transducer-transformer of claim 1,\nwherein coefficients of one or more explainable architectures within the explainable transducer-transformer are modified by human knowledge injection using zero-shot learning or few-shot learning;\nwherein one or more layers of the explainable transducer-transformer implement at least one of inductive logic, deductive logic, abductive logic, and causal logic; and\nwherein the explainable transducer-transformer is configured to remove one or more unexplainable latent spaces.\n9. The system for providing the explainable transducer-transformer of claim 1, wherein the output further applies one or more of: a Fourier transform, integer transform, real number transform, complex number transform, quaternion transform, octonion transform, Walsh function, state-space transform, phase-space transform, Haar and non-Haar wavelets, generalized L2 function, fractal-based transform, Hadamard transform, fuzzy logic, knowledge graph networks, categorical encoding, difference analysis, normalization, standardization, multi-dimensional Bezier curves, recurrence relations, and causal operators.\n10. The system for providing the explainable transducer-transformer of claim 1, further comprising at least one activation function and/or transformation function, wherein the transformation function is configured to transform the output using at least one of: a hierarchical tree, a causal diagram, a directed or undirected graph, hypergraph, or simplicial complex, a multimedia structure, and a set of hyperlinked graphs.\n11. The system for providing the explainable transducer-transformer of claim 1, wherein each partition is configured to fit a linear model and apply a non-linear transformation comprising at least one of a polynomial expansion, rotations, dimensional and dimensionless scaling, state-space and phase-space transforms, integer/real/complex/quaternion/octonion transforms, Fourier transforms, Walsh functions, continuous data bucketization, Haar and non-Haar wavelets, generalized L2 functions, fractal-based transforms, Hadamard transforms, Type 1 and Type 2 fuzzy logic, knowledge graph networks, categorical encoding, topological transform of Kolmogorov/Frechet/Hausdorff/Tychonoff spaces, difference analysis and normalization/standardization of data.\n12. The system for providing the explainable transducer-transformer of claim 1, wherein each partition comprises a cluster configured to group a plurality of data points according to a rule or a distance similarity function, wherein each partition represents a concept or category of data; and\nwherein the partitions are formed based on a clustering algorithm comprising one or more of: a k-means, Bayesian, connectivity based, centroid based, distribution based, grid based, density based, fuzzy logic based, entropy, or a mutual information (MI) based method, wherein the clustering algorithm further comprises an ensemble method resulting in a plurality of overlapping or non-overlapping partitions, an association-based algorithm, or a causality based partitioning function.\n13. The system for providing the explainable transducer-transformer of claim 1, wherein the system further comprises a behavioral model (BM/BMH); and\nwherein the system is configured to impose at least one environmentally plausible constraint on one or more past learnt representations and generate explainable, causal, and environmentally plausible feedback memory, based on the behavioral model and based on at least one of an associated condition, event, trigger, and action state representation.\n14. The system for providing the explainable transducer-transformer of claim 1,\nwherein the input comprises structured and unstructured data, or hierarchically structured data comprising one or more trees, graphs, hypergraphs, and simplicial complexes; and\nwherein the explainable transducer-transformer further comprises a knowledge representation structures comprising at least one of a resource description framework (RDF) tree, RDF graph, or Levi graph, and an audit log configured to identify one or more decision logs and path traces.\n15. The system for providing the explainable transducer-transformer of claim 1, wherein the explainable transducer-transformer further implements at least one causal model configured to identify cause-and-effect associations, interventions, and counterfactual causal logic, and further configured to perform plausibility checks using an environmental model input and configured to augment training data using causally plausible isomorphisms;\nwherein the at least one causal model is generated automatically using an attention model and is configured to identify causal links between antecedent and consequent generalization to generalized targets, extract causal structures, and create an initial causal model;\nfurther comprising one or more neuro-symbolic constraints identifying a historic rate of activations, wherein a rate of activation can be constrained; and\nfurther comprising named reference labels assigned to one or more components or layers within an explainable model, wherein each named reference label comprises at least one description or meta-data and links to external taxonomies, ontologies, and models providing a safety related constraint.\n16. The system for providing the explainable transducer-transformer of claim 1, further comprising one or more input modules and output modules, the one or more input modules and output modules comprising at least one of:\na normalization module configured to normalize the input before the input layer or after the input layer, wherein normalization of the input comprises creating one or more reports and analyses of bias and bias sources, and formulating a strategy for bias reduction, mitigation, or elimination via at least one of a supervised, unsupervised, or semi-supervised process;\na scaling module configured to scale the input before the input layer or after the input layer; and\na standardization module configured to standardize the input before the input layer or after the input layer.\n17. The system for providing the explainable transducer-transformer of claim 1, wherein the explainable transducer-transformer is implemented on a distributed explainable architecture comprising one or more explainable layers or models, and wherein one or more independent models are configured to activate independently of the one or more explainable layers or models.\n18. The system for providing the explainable transducer-transformer of claim 1, wherein the explainable transducer-transformer is configured to be trained using a self-supervised technique and verified using one or more of a Temporal Logic of Actions, Abstract Machine Notation, Petri Nets, Computation Tree Logic, intuitionistic logics, and/or relational semantics.\n19. The system for providing the explainable transducer-transformer of claim 1, wherein the explainable transducer-transformer is configured in at least one manner selected from:\na first manner wherein the explainable transducer-transformer is configured to analyze and parse an existing formal computer program, wherein the computer program is modeled in a hierarchical partition structure within one or more explainable models and a formal specification language is used to specify a problem to be solved using the computer program, wherein the explainable transducer-transformer is configured to generate candidate code solutions for the problem to be solved; or\na second manner wherein the explainable transducer-transformer is configured to convert code according to a pre-defined stylistic standard, highlight inconsistencies or errors, and suggest alternatives and re-factoring or re-writes of code, to de-obfuscate code that has been obfuscated, and apply one or more of alpha-conversion, beta reduction, and eta-reduction to generated code; or\na third manner wherein the explainable transducer-transformer is implemented with an automated theorem proving system to analyze, generate, and auto-complete mathematical expressions, statements, and proofs automatically using an existing collection of proven statements and/or incomplete statements; or\na fourth manner wherein the explainable transducer-transformer further comprises an end-to-end automatic speech recognition architecture to translate a speech audio waveform into corresponding text or into a latent discrete representation; or\na fifth manner wherein the explainable transducer-transformer further comprises a system for multiple object tracking or to match satellite, aerial, or bird-eye-view images for generating a digital surface model or a depth map or to process imagery in monocular, stereoscopic, and multi-view input data, or for an audio visualization classification to predict if an audio clip is present in a video.\n20. The system for providing the explainable transducer-transformer of claim 1, wherein at least one layer is implemented on a hardware comprising at least one of: a flexible architecture or field programmable gate array, a static architecture or application specific integrated circuit, analog or digital electronics, photo-electronics, optical processors, neuromorphic architectures, spintronics, or memristors, discrete computing components, spiking neurons, robotic hardware, autonomous vehicles, industrial control hardware, or quantum computing hardware, and wherein implementation of the at least one layer on the hardware is based on at least one of application of a quantization or hardware-oriented compression technique on the hardware; and\nwherein at least one layer comprises a sparse explainable neural network architecture, and wherein the explainable transducer-transformer implements one or more workflows, process flows, Fast Weights, Robotic Process Automation (RPA), Decision Support System (DSS), Data Lake, Root Cause Analysis (RCA), Goal-Plan-Action (GPA) system, process description, state-transition charts, Petri networks, electronic circuits, logic gates, optical circuits, digital-analogue hybrid circuits, bio-mechanical interfaces, bio-electrical interface, and quantum circuits.\n21. The system for providing the explainable transducer-transformer of claim 1, further comprising at least one explainable model configured to form an explanation structure model (ESM) comprising the explainable model, a statistical structural model which models a plurality of statistical relationships, a causal structural model (CSM), the CSM modeling a plurality of causal relationships, and a symbolic structural model, the symbolic structural model modeling a plurality of symbolic and logical relationships formed as one or more rules and/or symbolic logic, wherein one or more statistical, causal, symbolic, or logical relationships are modeled as an anchor component, and further comprising implementing an explanation interpretation generation system (EIGS) and/or an explanation filter interpretation configured to output an explanation output template (EOT), and\nan explainable self-attention mechanism configured to generate a plurality of levels of explanations comprising at least one of partitioning information, internal coefficients of one or more explainable models, and feature attributions of an input space, wherein the explanations are used as input to an interpreter.\n22. The system for providing the explainable transducer-transformer of claim 1, wherein an explanation further comprises at least one of a basic interpretation, an explanatory interpretation, and a meta-explanatory interpretation, and a neuro-symbolic conditional constraint with a rate of activations in order to constrain a rate of trigger activation with respect to an explainable model, wherein the neuro-symbolic conditional constraint is implemented as at least one of: symbolic rules or a system of symbolic expressions, polynomial expressions, conditional and non-conditional probability distributions, joint probability distributions, state-space and phase-space transforms, integer/real/complex/quaternion/octonion transforms, Fourier transforms, Walsh functions, Haar and non-Haar wavelets, generalized L2 functions, fractal-based transforms, Hadamard transforms, Type 1 and Type 2 fuzzy logics, difference analyses, and knowledge graph networks.\n23. The system for providing the explainable transducer-transformer of claim 1, further comprising an explanation component configured to receive a model output to produce an explanation using an explanation scaffolding, the explanation scaffolding comprising:\nan explanation model component, comprising the model output indicating an answer, a model explanation, and a model fusion and links component, wherein the model fusion and links component is configured to store metadata and information associated with one or more links between one or more systems and databases;\na hypothetical and causal component, configured to model at least one cause-and-effect relationship by forming one or more structural equation models, structural causal models, and/or causal directed acyclic graphs; and\na scenario, interaction, and presentation component.\n24. The system for providing the explainable transducer-transformer of claim 1, wherein the output comprises an answer, a model explanation, and a justification of the answer and/or the model explanation, wherein the justification indicates at least one assumption, process, or decision used by the explainable transducer-transformer in arriving at the answer or the model explanation.\n25. The system for providing the explainable transducer-transformer of claim 1, wherein the output comprises at least one of: a human-readable natural language format, a graphical or visual format, audio, speech, haptic, video, time series, multi-spectral data, hierarchically ordered multimedia content, and 3D data, and wherein the explainable transducer-transformer is configured to process sequence data comprising a sequential format and one or more associated labels indicating an output value or classification or a data point or a continuous or non-continuous interval of data points, wherein an input sequence is mapped to an output sequence in a multi-model and multi-task manner.\n26. The system for providing the explainable transducer-transformer of claim 1, wherein the explainable transducer-transformer further comprises at least one explainable neural network configured to implement a wide learning model with a deep learning model.\n27. The system for providing the explainable transducer-transformer of claim 1, wherein the conditional layer is configured to receive the one or more partitions based on an external partition creation process.\n28. The system for providing the explainable transducer-transformer of claim 1, wherein the one or more partitions are pre-trained or are initialized from a linked taxonomy or ontology.\n29. The system for providing the explainable transducer-transformer of claim 1, wherein a gradient descent method is applied to further refine the partitions, and wherein one or more of the partitions are:\nlocked with static values;\ndynamically adjustable using a backward training technique, multiple objective optimization, genetic algorithm, a Monte Carlo simulation method, or a causal logic and simulation technique; or\nhierarchical.\n30. The system for providing the explainable transducer-transformer of claim 1, further comprising a ranking function configured to select, merge, or split one or more overlapping or non-overlapping partitions, and wherein one or more partition is configured to represent symbols and concepts with at least two different levels of semantic and semiotic detail.",
    "status": "Active",
    "citations_own": [
        "US5499319A",
        "US20140096249A1",
        "US20160155049A1",
        "US20170213156A1",
        "US20190354853A1",
        "US20190370647A1",
        "US20200033868A1",
        "US20200184278A1",
        "US20200293888A1",
        "US20210073282A1",
        "US20210350221A1",
        "US20220036209A1",
        "US20220067511A1",
        "US20220067510A1",
        "US20220147838A1"
    ],
    "citations_ftf": [
        "US11676365B2"
    ],
    "citedby_own": [],
    "citedby_ftf": [
        "CN109902849B",
        "US20210295531A1",
        "US20220237389A1",
        "US20220247548A1",
        "US11531555B2",
        "US20230206645A1",
        "CN115311720B",
        "CN115147315B",
        "CN115510854B",
        "CN115578735B"
    ]
}