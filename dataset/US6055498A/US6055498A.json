{
    "patent_link": "https://patents.google.com/patent/US6055498A/en",
    "patent_id": "US6055498A",
    "title": "Method and apparatus for automatic text-independent grading of pronunciation for language instruction",
    "abstract": "Pronunciation quality is automatically evaluated for an utterance of speech based on one or more pronunciation scores. One type of pronunciation score is based on duration of acoustic units. Examples of acoustic units include phones and syllables. Another type of pronunciation score is based on a posterior probability that a piece of input speech corresponds to a certain model, such as a hidden Markov model, given the piece of input speech. Speech may be segmented into phones and syllable for evaluation with respect to the models. The utterance of speech may be an arbitrary utterance made up of a sequence of words which had not been encountered before. Pronunciation scores are converted into grades as would be assigned by human graders. Pronunciation quality may be evaluated in a client-server language instruction environment.",
    "inventors": [
        "Leonardo Neumeyer",
        "Horacio Franco",
        "Mitchel Weintraub",
        "Patti Price",
        "Vassilios Digalakis"
    ],
    "assignee": "SRI International Inc",
    "classifications": [
        "G09B19/04",
        "G10L15/04",
        "G10L15/26",
        "H04L67/01"
    ],
    "claims": "\n1. In an automatic speech processing system, a method for assessing pronunciation of a student speech sample using a computerized acoustic segmentation system, the method comprising:\naccepting said student speech sample which comprises a sequence of words spoken by a student speaker;\noperating said computerized acoustic segmentation system to define sample acoustic units within said student speech sample based on speech acoustic models within said segmentation system, said speech acoustic models being established using training speech data from at least one speaker, said training speech data not necessarily including said sequence of spoken words;\nmeasuring duration of said sample acoustic units; and\ncomparing said durations of sample acoustic units to a model of exemplary acoustic unit duration to compute a duration score indicative of similarity between said sample acoustic unit durations and exemplary acoustic unit durations.\n2. The method according to claim 1 wherein said exemplary acoustic unit duration model is established using duration-training speech data from at least one exemplary speaker, said duration-training data not necessarily including said sequence of spoken words.\n3. The method according to claim 1 wherein each acoustic unit is shorter in duration than a longest word in the language of said spoken words.\n4. The method according to claim 1 further comprising:\nmapping said duration score to a grade; and\npresenting said grade to a student.\n5. The method according to claim 4 wherein the step of mapping said duration score to a grade comprises:\ncollecting a set of training speech samples from a plurality of language students of various proficiency levels;\ncomputing training duration scores for each of said training speech samples;\ncollecting at least one human evaluation grade from a human grader for each of said training speech samples; and\nadjusting coefficients used in mapping by minimizing an error measurement between said human evaluation grades and said training duration scores.\n6. The method according to claim 4 wherein the step of mapping comprises using a mapping function obtained by linear or non-linear regression from training duration scores, alone or in combination with other machine scores, and corresponding human evaluation grades, all of said scores and grades being collected over a representative training data base of student speech.\n7. The method according to claim 6 wherein said mapping function is obtained by non-linear regression implemented with a neural net which allows arbitrary mappings from machine scores to human expert grades.\n8. The method according to claim 4 wherein the step of mapping comprises using a decision tree or class probability tree whose parameters were established using training duration scores.\n9. The method according to claim 1 wherein the step of operating said acoustic segmentation system comprises the steps of:\ncomputing a path through trained hidden Markov models (HMMs) from among said speech acoustic models, said path being an allowable path through the HMMs that has maximum likelihood of generating an observed acoustic features sequence from said student speech sample; and\ndetermining from said path at least one boundary or duration of one acoustic unit.\n10. The method according to claim 9 wherein:\nsaid spoken sequence of words is spoken according to a known script; and\nthe path computing step comprises using said script in defining allowability of any path through the HMMs.\n11. The method according to claim 9 wherein said spoken sequence of words is unknown, and the path computing step comprises operating a computerized speech recognition system that determines said spoken sequence of words.\n12. The method according to claim 9 wherein:\nsaid sample acoustic units are syllables; and\nthe step of determining at least one acoustic unit boundary or duration comprises the steps of:\nextracting boundaries or durations of at least two phones from said path; and\ncombining portions of at least two phones to obtain a boundary or duration of a syllable acoustic unit.\n13. The method according to claim 12 wherein the step of combining portions of at least two phones comprises measuring the time difference between centers of vowel phones from among said phones to obtain a duration of a syllable acoustic unit.\n14. The method according to claim 1 wherein said sample acoustic units are phones.\n15. The method according to claim 1 wherein said sample acoustic units are syllables.\n16. The method according to claim 1 wherein:\nsaid exemplary acoustic unit duration distribution model is a model of speaker-normalized acoustic unit durations, and the duration measuring step comprises the steps of:\nanalyzing said student speech sample to determine a student speaker normalization factor; and\nemploying said student speaker normalization factor to measure speaker-normalized durations as said measured sample acoustic unit durations, whereby the comparing step compares said speaker-normalized sample acoustic unit durations to said exemplary speaker-normalized acoustic unit duration distribution model.\n17. The method according to claim 16 wherein said student speaker normalization factor is rate of speech.\n18. The method according to claim 1 wherein the step of operating said segmentation system excludes acoustic units in context with silence from analysis.\n19. The method according to claim 1 wherein the step of operating said segmentation system comprises operating a speech recognition system as said acoustic segmentation system.\n20. A system for assessing pronunciation of a student speech sample, said student speech sample comprising a sequence of words spoken by a student speaker, the system comprising:\nspeech acoustic models established using training speech data from at least one speaker, said training speech data not necessarily including said sequence of spoken words;\na computerized acoustic segmentation system configured to identify acoustic units within said student speech sample based on said speech acoustic models;\na duration extractor configured to measure duration of said sample acoustic units;\na model of exemplary acoustic unit duration; and\na duration scorer configured to compare said sample acoustic unit durations to said model of exemplary acoustic unit duration and compute a duration score indicative of similarity between said sample acoustic unit durations and acoustic unit durations in exemplary speech.\n21. In an automatic speech processing system, a method for grading the pronunciation of a student speech sample, the method comprising:\naccepting said student speech sample which comprises a sequence of words spoken by a student speaker;\noperating a set of trained speech models to compute at least one posterior probability from said speech sample, each of said posterior probabilities being a probability that a particular portion of said student speech sample corresponds to a particular known model given said particular portion of said speech sample; and\ncomputing an evaluation score, herein referred to as the posterior-based evaluation score, of pronunciation quality for said student speech sample from said posterior probabilities\nwherein each of said posterior probabilities is derived from a model likelihood by dividing the likelihood that said particular known model generated said particular portion of said student speech sample by the summation of the likelihoods that individual models generated said particular portion of said speech sample.\n22. The method according to claim 21 wherein:\nsaid particular known model is a context-dependent model; and\nindividual models are context-dependent or context-independent models.\n23. The method according to claim 21 wherein said particular portion of said speech sample is a phone.\n24. The method according to claim 21 further comprising:\nmapping said posterior-based evaluation score to a grade as would be assigned by human listener; and\npresenting said grade to said student speaker.\n25. The method according to claim 24 wherein said step of mapping said posterior-based evaluation score to a grade comprises:\ncollecting a set of training speech samples from a plurality of language students of various proficiency levels;\ncollecting a set of human evaluation grades for each of said training samples from human expert listeners listening to said samples; and\nadjusting coefficients used in mapping by minimizing the squared-error between the human expert grades and said evaluation score.\n26. The method according to claim 21 wherein said student speech sample comprises an acoustic features sequence, the method further comprising the steps of:\ncomputing a path through a set of trained hidden Markov models (HMMs) from among said trained speech models, said path being an allowable path through the HMMs that has maximum likelihood of generating said acoustic features sequence; and\nidentifying transitions between phones within said path, thereby defining phones.\n27. The method according to claim 26 wherein the path computing step is performed using the Viterbi search technique.\n28. The method according to claim 26 wherein said spoken sequence of words is unknown, and the path computing step is performed using a computerized speech recognition system that determines said spoken sequence of words.\n29. The method according to claim 21 wherein segments in context with silence are excluded from said student speech sample and from training data used to train said speech models.\n30. In an automatic speech processing system, a method for grading the pronunciation of a student speech sample, the method comprising:\naccepting said student speech sample which comprises a sequence of words spoken by a student speaker;\noperating a set of trained speech models to compute at least one posterior probability from said speech sample, each of said posterior probabilities being a probability that a particular portion of said student speech sample corresponds to a particular known model given said particular portion of said speech sample; and\ncomputing an evaluation score, herein referred to as the posterior-based evaluation score, of pronunciation quality for said student speech sample from said posterior probabilities\nwherein:\nsaid trained speech models comprise a set of phone models;\nsaid student speech sample comprises phones; and\nthe step of operating said speech models comprises computing a frame-based posterior probability for each frame yt within a phone i of a phone type qi: ##EQU14## wherein: p(yt|qi, . . . ) is the probability of the frame yt according to a model corresponding to phone type qi;\nthe sum over q runs over all phone types; and\nP(qi) represents the prior probability of the phone type qi.\n31. The method according to claim 30 wherein the step of computing a frame-based posterior probability uses context-dependent models corresponding to each phone type qi in the numerator, whereby said p(yt .linevert split.qi, . . . ) is a context-dependent likelihood p(yt .linevert split.qi, ctxi), wherein ctxi represents context.\n32. The method according to claim 30 wherein the step of computing said posterior-based evaluation score for said student speech sample comprises computing for a phone i an average of the logarithm of the frame-based posterior probabilities of all frames within said phone i, said average herein referred to as a phone score \u03c1i, which is expressible as: ##EQU15## wherein the sum runs over all di frames of said phone i.\n33. The method according to claim 32 wherein said posterior-based evaluation score for said student speech sample is defined as an average of the individual phone scores \u03c1i for each phone i within said student speech sample: ##EQU16## wherein the sum runs over the number of phones in said student speech sample.\n34. The method according to claim 30 wherein the model corresponding to each phone type is a Gaussian mixture phone model.\n35. The method according to claim 30 wherein the model corresponding to each phone type is a context-independent phone model.\n36. The method according to claim 30 wherein the model corresponding to each phone type is a hidden markov model.\n37. A system for pronunciation training in a client/server environment wherein there exists a client process for presenting prompts to a student and for accepting student speech elicited by said prompts, the system comprising:\na server process for sending control information to said client process to specify a prompt to be presented to said student and for receiving a speech sample derived from said student speech elicited by said presented prompt; and\na pronunciation evaluator invocable by said server process for analyzing said student speech sample wherein:\nsaid pronunciation evaluator is established using training speech data; and\nsaid server process is adapted to specify a prompt for eliciting a sequence of words not necessarily found in said training speech data as said student speech sample.\n38. The system according to claim 37 wherein said server process receives said speech sample over a speech channel that is separate from a communication channel through which said server process and said client process communicate.\n39. The system according to claim 37 wherein said client process and said server process are located on two separate computer processors and communicate via a network.",
    "status": "Expired - Lifetime",
    "citations_own": [
        "US4276445A",
        "US4380438A",
        "US4481593A",
        "US4489434A",
        "US4641343A",
        "US4752958A",
        "US4761815A",
        "US4783803A",
        "US4852180A",
        "US4860360A",
        "US4862408A",
        "US4887212A",
        "US4969194A",
        "US5010495A",
        "US5027406A",
        "US5075896A",
        "US5148489A",
        "US5199077A",
        "US5268990A",
        "US5274739A",
        "US5307444A",
        "WO1994010666A1",
        "US5329608A",
        "US5329609A",
        "US5333275A",
        "US5475792A",
        "US5487671A",
        "US5503560A",
        "US5509104A",
        "US5615296A",
        "US5634086A",
        "US5638487A",
        "US5673362A",
        "US5722418A",
        "US5825978A"
    ],
    "citations_ftf": [
        "US3184549A",
        "US3453749A",
        "US3881059A",
        "US4472833A",
        "FR2546323B1",
        "US4799261A",
        "US5455889A",
        "WO1998014934A1",
        "US8862408B2"
    ],
    "citedby_own": [
        "WO2000067455A1",
        "US6167377A",
        "US6226611B1",
        "US6336089B1",
        "US20020042710A1",
        "WO2002071390A1",
        "US6453290B1",
        "US6463413B1",
        "US6466908B1",
        "US20020160341A1",
        "US20030163316A1",
        "US20030182111A1",
        "US6629073B1",
        "US6662158B1",
        "US20030229497A1",
        "US20040002863A1",
        "US20040006461A1",
        "US20040006468A1",
        "US20040049391A1",
        "US6714911B2",
        "US6732076B2",
        "US6789060B1",
        "US6810379B1",
        "EP1482469A2",
        "US6845358B2",
        "US6847931B2",
        "US20050187771A1",
        "US20060057545A1",
        "US20060069561A1",
        "WO2006034200A2",
        "US20060111905A1",
        "US20060112812A1",
        "US20060136225A1",
        "US20060155538A1",
        "US20070055514A1",
        "US20070179788A1",
        "US20070192093A1",
        "US20070198257A1",
        "US20070213982A1",
        "US7330815B1",
        "WO2008057103A1",
        "US20080306738A1",
        "US20090197233A1",
        "US20090305203A1",
        "US20100004931A1",
        "US20100062403A1",
        "US7702464B1",
        "US20100185435A1",
        "US7873477B1",
        "US20110059423A1",
        "US20110218803A1",
        "US20120034581A1",
        "US20120065977A1",
        "US8457903B1",
        "US8744856B1",
        "US20140180694A1",
        "TWI469100B",
        "US20150037778A1",
        "US20150111183A1",
        "US20150248898A1",
        "US20150339950A1",
        "US20150371630A1",
        "US20160225374A1",
        "WO2017049350A1",
        "US9947322B2",
        "US9953646B2",
        "US20180174601A1",
        "US10013971B1",
        "CN109697977A",
        "CN109863554A",
        "US10529357B2",
        "US10553218B2",
        "US10679630B2",
        "US10735402B1",
        "US10854205B2",
        "US10965595B1",
        "US11019201B2",
        "US11335349B1",
        "US11355103B2",
        "US11468901B2",
        "US11601374B2",
        "US11646018B2",
        "US11659082B2"
    ],
    "citedby_ftf": [
        "US6036496A",
        "AU3910500A",
        "US6224383B1",
        "US7062441B1",
        "US7149690B2",
        "US7216077B1",
        "US6461166B1",
        "US7177810B2",
        "US7074128B2",
        "US6953343B2",
        "JP3799280B2",
        "US20040176960A1",
        "US7357640B2",
        "TWM249950U",
        "US7364432B2",
        "US20060058999A1",
        "US8178025B2",
        "WO2006136061A1",
        "US20070067174A1",
        "CN101366065A",
        "US8135590B2",
        "JP5327054B2",
        "US8175882B2",
        "US8387082B2",
        "US8447603B2",
        "US8768697B2",
        "US20110224982A1",
        "US8401856B2",
        "EP2450877B1",
        "CN102724543B",
        "CN103514765A",
        "CN108257615A",
        "US20220028390A1",
        "EP3979239A1"
    ]
}