{
    "patent_id": "US-10706352-B2",
    "title": "Training action selection neural networks using off-policy actor critic reinforcement learning ",
    "assignee": "Deepmind Technologies Limited",
    "publication_date": "2020-07-07",
    "patent_link": "https://patents.google.com/patent/US10706352B2/en",
    "inventors": [
        "Ziyu Wang",
        "Nicolas Manfred Otto Heess",
        "Victor Constant Bapst"
    ],
    "classifications": [
        "G06N3/0454",
        "G06N3/006",
        "G06N3/045",
        "G06N3/047",
        "G06N3/0472",
        "G06N3/084",
        "G06N3/088",
        "G06N7/01"
    ],
    "abstract": "Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for training an action selection neural network. One of the methods includes maintaining a replay memory that stores trajectories generated as a result of interaction of an agent with an environment; and training an action selection neural network having policy parameters on the trajectories in the replay memory, wherein training the action selection neural network comprises: sampling a trajectory from the replay memory; and adjusting current values of the policy parameters by training the action selection neural network on the trajectory using an off-policy actor critic reinforcement learning technique.",
    "claims": "\n1. A method comprising:\nmaintaining a replay memory that stores trajectories generated as a result of interaction of an agent with an environment, each trajectory comprising respective action selection data at each of a plurality of time steps, the action selection data at each time step identifying:\n(i) an observation characterizing a state of the environment,\n(ii) an action performed by the agent in response to the observation,\n(iii) a reward received in response to the agent performing the action, and\n(iv) at least an action selection score assigned to the performed action in determining which action to perform in response to the observation; and\ntraining an action selection neural network having policy parameters on the trajectories in the replay memory, wherein the action selection neural network is configured to:\nreceive a provided observation characterizing a state of the environment; and\nprocess the provided observation to generate a network output that defines a score distribution over possible actions that can be performed by the agent in response to the provided observation, and\nwherein training the action selection neural network comprises:\nsampling a trajectory from the replay memory; and\nadjusting current values of the policy parameters by training the action selection neural network on the trajectory using an off-policy actor critic reinforcement learning technique, comprising, for each time step in the trajectory from the final time step in the trajectory to the initial time step in the trajectory:\ndetermining, from the reward for the time step and a current Retrace estimate, a Retrace estimate for the time step; and\ndetermining a main gradient for the time step using at least the Retrace estimate for the time step.\n2. The method of claim 1, wherein the set of possible actions that can be performed by the agent in response to the observation is finite, wherein the network output is the score distribution, and wherein determining the main gradient for the time step further comprises:\nprocessing the observation for the time step using the action selection neural network to determine a main score distribution;\nprocessing the observation for the time step using a critic neural network to determine a respective Q value for each of the possible actions; and\ndetermining the main gradient for the time step using the Q values and the main score distribution in addition to the Retrace estimate for the time step.\n3. The method of claim 2, wherein determining the main gradient for the time step further comprises:\ndetermining, from the Q values and the main score distribution, a value estimate for the time step; and\ndetermining the main gradient for the time step using the value estimate for the time step in addition to the main score distribution, the Q values, and the Retrace estimate for the time step.\n4. The method of claim 3, wherein determining the main gradient for the time step further comprises:\ndetermining a truncated importance weight from the score for the performed action in the main score distribution and the action selection score assigned to the performed action in determining which action to perform in response to the observation; and\napplying the truncated importance weight in determining a first term of the main gradient for the time step.\n5. The method of claim 4, wherein determining the main gradient for the time step further comprises:\ndetermining a correction gradient term from the scores in the main score distribution and action selection scores for the actions in the set of actions used in determining which action to perform in response to the observation.\n6. The method of claim 3, further comprising:\ndetermining a critic gradient from the Q value for the performed action and the current Retrace estimate;\ndetermining an update for current values of the parameters of the critic neural network from the critic gradient; and\nupdating the current Retrace estimate.\n7. The method of claim 1, wherein adjusting current values of the policy parameters further comprises:\nprocessing the observation for the time step using the action selection neural network to determine a main score distribution;\nprocessing, using an average neural network having a plurality of average network parameters and in accordance with current values of the average network parameters, the observation for the time step to generate an auxiliary score distribution\nwherein the current values of the average network parameters represent a running average of values of the parameters of the action selection network during the training;\ndetermining an auxiliary gradient of a measure of a difference between the auxiliary score distribution and the main score distribution;\ndetermining a final gradient from the main gradient and the auxiliary gradient; and\ndetermining an update to the current values of the policy parameters from the final gradient.\n8. The method of claim 7, wherein the measure of the difference is a Kullback-Leibler divergence.\n9. The method of claim 7, wherein determining the final gradient from the main gradient and the auxiliary gradient comprises:\ndetermining, from the auxiliary gradient and the main gradient, a scaling factor for the main gradient; and\napplying the scaling factor to the main gradient to determine the final gradient.\n10. The method of claim 9, wherein the scaling factors satisfies:\n11. The method of claim 9, wherein applying the scaling factor comprises subtracting the scaling factor from the main gradient.\n12. The method of claim 7, further comprising:\nupdating the current values of the policy parameters using the updates for the trajectory to determine updated values of the policy parameters; and\nupdating the current values of the average network parameters using the updated values of the policy parameters to determine updated values of the average network parameters.\n13. The method of claim 12, wherein the updated values of the average network parameters are a weighted sum of the current values of the average network parameters and the updated values of the main network parameters.\n14. A method of training a main neural network having a plurality of main network parameters to determine trained values of the main network parameters by optimizing an objective function, the method comprising:\nreceiving a batch of training data comprising a plurality of training examples; and\nfor each of the plurality of training examples in the batch:\nprocessing, using the main neural network in accordance with current values of the main network parameters, the training example to generate a main network output for the training example;\ndetermining a main gradient of the objective function from the main network output;\nprocessing, using an average neural network having a plurality of average network parameters and in accordance with current values of the average network parameters, the training example to generate an average network output for the training example,\nwherein the current values of the average network parameters represent a running average of values of the main network parameters during the training;\ndetermining an auxiliary gradient of a measure of a difference between the average network output and the main network output;\ndetermining a final gradient from the main gradient and the auxiliary gradient; and\ndetermining an update to the current values of the main network parameters from the final gradient.\n15. The method of claim 14, wherein the auxiliary measure of the difference is a Kullback-Leibler divergence.\n16. The method of claim 14, wherein determining the final gradient from the main gradient and the auxiliary gradient comprises:\ndetermining, from the auxiliary gradient and the main gradient, a scaling factor for the main gradient; and\napplying the scaling factor to the main gradient to determine the final gradient.\n17. The method of claim 16, wherein the scaling factors satisfies:\n18. The method of claim 17, wherein applying the scaling factor comprises subtracting the scaling factor from the main gradient.\n19. The method of claim 14, further comprising:\nupdating the current values of the main network parameters using the updates for the training examples in the batch to determine updated values of the main network parameters; and\nupdating the current values of the average network parameters using the updated values of the main network parameters to determine updated values of the average network parameters.\n20. The method of claim 19, wherein the updated values of the average network parameters are a weighted sum of the current values of the average network parameters and the updated values of the main network parameters.\n21. A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to perform operations comprising:\nmaintaining a replay memory that stores trajectories generated as a result of interaction of an agent with an environment, each trajectory comprising respective action selection data at each of a plurality of time steps, the action selection data at each time step identifying:\n(i) an observation characterizing a state of the environment,\n(ii) an action performed by the agent in response to the observation,\n(iii) a reward received in response to the agent performing the action, and\n(iv) at least an action selection score assigned to the performed action in determining which action to perform in response to the observation; and\ntraining an action selection neural network having policy parameters on the trajectories in the replay memory, wherein the action selection neural network is configured to:\nreceive a provided observation characterizing a state of the environment; and\nprocess the provided observation to generate a network output that defines a score distribution over possible actions that can be performed by the agent in response to the provided observation, and\nwherein training the action selection neural network comprises:\nsampling a trajectory from the replay memory; and\nadjusting current values of the policy parameters by training the action selection neural network on the trajectory using an off-policy actor critic reinforcement learning technique, comprising, for each time step in the trajectory from the final time step in the trajectory to the initial time step in the trajectory:\ndetermining, from the reward for the time step and a current Retrace estimate, a Retrace estimate for the time step; and\ndetermining a main gradient for the time step using at least the Retrace estimate for the time step.\n22. A system comprising:\none or more computers; and\none or more storage devices communicatively coupled to the one or more computers, wherein the one or more storage devices store instructions that, when executed by the one or more computers, cause the one or more computers to perform operations for training a main neural network having a plurality of main network parameters to determine trained values of the main network parameters by optimizing an objective function, the operations comprising:\nreceiving a batch of training data comprising a plurality of training examples; and\nfor each of the plurality of training examples in the batch:\nprocessing, using the main neural network in accordance with current values of the main network parameters, the training example to generate a main network output for the training example;\ndetermining a main gradient of the objective function from the main network output;\nprocessing, using an average neural network having a plurality of average network parameters and in accordance with current values of the average network parameters, the training example to generate an average network output for the training example,\nwherein the current values of the average network parameters represent a running average of values of the main network parameters during the training;\ndetermining an auxiliary gradient of a measure of a difference between the average network output and the main network output;\ndetermining a final gradient from the main gradient and the auxiliary gradient; and\ndetermining an update to the current values of the main network parameters from the final gradient."
}