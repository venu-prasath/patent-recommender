{
    "patent_id": "US-11087488-B2",
    "title": "Automated gesture identification using neural networks ",
    "assignee": "Avodah, Inc.",
    "publication_date": "2021-08-10",
    "patent_link": "https://patents.google.com/patent/US11087488B2/en",
    "inventors": [
        "Trevor Chandler",
        "Dallas Nash",
        "Michael Menefee"
    ],
    "classifications": [
        "G06T7/73",
        "G06V10/82",
        "G06F18/256",
        "G06F3/017",
        "G06F3/0304",
        "G06K9/00248",
        "G06K9/00342",
        "G06K9/00355",
        "G06N3/044",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/08",
        "G06T7/20",
        "G06T7/90",
        "G06V40/165",
        "G06V40/23",
        "G06V40/28",
        "G06N3/006"
    ],
    "abstract": "Disclosed are methods, apparatus and systems for gesture recognition based on neural network processing. One exemplary method for identifying a gesture communicated by a subject includes receiving a plurality of images associated with the gesture, providing the plurality of images to a first 3-dimensional convolutional neural network (3D CNN) and a second 3D CNN, where the first 3D CNN is operable to produce motion information, where the second 3D CNN is operable to produce pose and color information, and where the first 3D CNN is operable to implement an optical flow algorithm to detect the gesture, fusing the motion information and the pose and color information to produce an identification of the gesture, and determining whether the identification corresponds to a singular gesture across the plurality of images using a recurrent neural network that comprises one or more long short-term memory units.",
    "claims": "\n1. An artificial intelligence system adapted for processing images associated with a gesture performed by a subject, comprising:\na plurality of pipeline structures, each of the pipeline structures configured to include three components:\nan associated pre-rule component configured to process an input to the pipeline structure,\nan associated pipeline component, and\nan associated post-rule component configured to process an output of the pipeline component,\nwherein a first pipeline structure comprises:\na first pre-rule component configured to determine whether an input stream includes a plurality of input images comprising pixels,\na first pipeline component configured to generate recognition information comprising at least one characteristic in each of the plurality of input images, the at least one characteristic comprising a pose, a color or a gesture type, and\na first post-rule component configured to determine whether the at least one characteristic is associated with the gesture,\nwherein a second pipeline structure comprises:\na second pre-rule component configured to determine whether the plurality of input images is associated with the gesture, and\na second pipeline component configured to perform a facial recognition or an emotional recognition operation on the plurality of input images and generate a first result,\nwherein a third pipeline structure comprises:\na third pre-rule component configured to determine whether the first result from the second pipeline structure is compatible with the recognition information generated by the first pipeline structure, and\na third pipeline component configured to determine, using a feedback connection, whether the recognition information corresponds to a singular gesture across the plurality of input images.\n2. The system of claim 1, wherein the first pipeline structure comprises a three-dimensional convolutional neural network (3D CNN) configured to receive the input stream and output the recognition information, wherein the second pipeline structure comprises a facial or emotional recognition (FER) module, and wherein the third pipeline structure comprises a recurrent neural network (RNN) comprising an output that is coupled to an input of the RNN to provide the feedback connection.\n3. The system of claim 1, wherein a fourth pipeline structure of the plurality of pipeline structures comprises a pre-processing module configured to receive the input stream and output the plurality of input images, and wherein the fourth pipeline structure comprises:\na fourth pre-rule component configured to determine whether the input stream includes a plurality of captured images comprising pixels,\na fourth pipeline component configured to perform pose estimation on each of the plurality of captured images, and\na fourth post-rule component configured to overlay pose estimation pixels onto the plurality of captured images to generate the plurality of input images.\n4. The system of claim 3, wherein the pose estimation identifies the subject's body, fingers and face.\n5. The system of claim 3, wherein the fourth pipeline component is further configured, prior to performing the pose estimation, to:\nprocess the plurality of captured images to extract pixels corresponding to a subject, a foreground and a background in each of the plurality of captured images; and\nperform spatial filtering, upon determining that the background or the foreground includes no information that pertains to the gesture being identified, to remove the pixels corresponding to the background or the foreground.\n6. The system of claim 1, wherein the first pipeline structure is operable to use an optical flow algorithm to detect the gesture.\n7. The system of claim 6, wherein the optical flow algorithm comprises sharpening, line, edge, corner and shape enhancements.\n8. The system of claim 1, wherein the second pipeline component is operable to use 32 reference points on a face of a subject performing the gesture to generate the first result.\n9. The system of claim 1, wherein the third pipeline component comprises one or more long short-term memory (LTSM) units.\n10. The system of claim 1, wherein a fifth pipeline structure of the plurality of pipeline structures comprises an audio recognition module, and wherein the fifth pipeline structure comprises:\na fifth pre-rule component configured to determine whether an input includes audio samples,\na fifth pipeline component configured to perform audio recognition on the audio samples, and\na fifth post-rule component configured to determine whether the audio samples are associated with the gesture and generate a second result, and\nwherein the third pre-rule of the third pipeline structure is further configured to determine whether the second result is compatible with the recognition information generated by the first pipeline structure.",
    "status": "Active",
    "citations_own": [
        "US5481454A",
        "US5544050A",
        "US5659764A",
        "US5704012A",
        "US5887069A",
        "US20020069067A1",
        "US6477239B1",
        "US6628244B1",
        "US20030191779A1",
        "US20040210603A1",
        "US20050258319A1",
        "US7027054B1",
        "US20060134585A1",
        "US20060139348A1",
        "US20060204033A1",
        "US20080013793A1",
        "US20080013826A1",
        "US20080024388A1",
        "US20080201144A1",
        "US20100044121A1",
        "US20100194679A1",
        "US20110221974A1",
        "US20110228463A1",
        "US20110274311A1",
        "US20110301934A1",
        "US8553037B2",
        "US20130318525A1",
        "US20140101578A1",
        "US20140253429A1",
        "US20140309870A1",
        "USD719472S1",
        "USD721290S1",
        "USD722315S1",
        "US20150092008A1",
        "US20150244940A1",
        "US20150317304A1",
        "US20150324002A1",
        "WO2015191468A1",
        "USD752460S1",
        "US9305229B2",
        "US20160196672A1",
        "US20160267349A1",
        "US20160320852A1",
        "US20160379082A1",
        "US20170090995A1",
        "JP2017111660A",
        "US20170206405A1",
        "US9715252B2",
        "US20170236450A1",
        "US20170255832A1",
        "US20170351910A1",
        "US20180018529A1",
        "US20180032846A1",
        "US20180047208A1",
        "US20180101520A1",
        "US20180137644A1",
        "US20180181809A1",
        "US20180189974A1",
        "US10037458B1",
        "US20180268601A1",
        "US20180374236A1",
        "US20180373985A1",
        "US20190026956A1",
        "US20190043472A1",
        "US20190066733A1",
        "US20190064851A1",
        "US10289903B1",
        "US10304208B1",
        "US10346198B1",
        "US20190251343A1",
        "US10924651B2"
    ],
    "citations_ftf": [
        "WO2008150918A1"
    ],
    "citedby_own": [
        "US20220026992A1"
    ],
    "citedby_ftf": [
        "WO2016077797A1",
        "US10489639B2",
        "US10289903B1",
        "CN110147702B",
        "US11062476B1",
        "CN110223322B",
        "CN110399482B",
        "CN110427945A",
        "CN110200624A",
        "CN110458046B",
        "CN110399850B",
        "US11488320B2",
        "CN110532932B",
        "CN110595401A",
        "US10956724B1",
        "CN110705390A",
        "CN110610210B",
        "CN111091045B",
        "CN110865650B",
        "CN111160164B",
        "CN111145255B",
        "CN111626135A",
        "CN111695457B",
        "CN111723779B",
        "CN112329974A",
        "CN111833351A",
        "CN112329867B",
        "CN112487951B",
        "US11587362B2",
        "CN112560757B",
        "US11625938B2",
        "CN112818011B",
        "CN113052112B",
        "CN113326781B",
        "TWI797956B",
        "CN115527045A",
        "CN115690853B",
        "CN116343342B"
    ]
}