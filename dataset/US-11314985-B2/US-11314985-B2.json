{
    "patent_id": "US-11314985-B2",
    "title": "System and method for procedurally synthesizing datasets of objects of interest for training machine-learning models ",
    "assignee": "Nvidia Corporation",
    "publication_date": "2022-04-26",
    "patent_link": "https://patents.google.com/patent/US11314985B2/en",
    "inventors": [
        "Jesse Clayton",
        "Vladimir Glavtchev"
    ],
    "classifications": [
        "G06K9/6256",
        "G06N20/00",
        "G06F18/214",
        "G06F18/254",
        "G06F18/28",
        "G06K9/00221",
        "G06K9/00335",
        "G06K9/00369",
        "G06K9/00805",
        "G06K9/00825",
        "G06K9/6255",
        "G06K9/6292",
        "G06K9/66",
        "G06N5/046",
        "G06T15/005",
        "G06V10/772",
        "G06V10/774",
        "G06V20/58",
        "G06V20/584",
        "G06V40/103",
        "G06V40/16",
        "G06V40/20"
    ],
    "abstract": "The disclosure provides method of training a machine-learning model employing a procedurally synthesized training dataset, a machine that includes a trained machine-learning model, and a method of operating a machine. In one example, the method of training includes: (1) generating training image definitions in accordance with variations in content of training images to be included in a training dataset, (2) rendering the training images corresponding to the training image definitions, (3) generating, at least partially in parallel with the rendering, ground truth data corresponding to the training images, the training images and the ground truth comprising the training dataset, and (4) training a machine-learning model using the training dataset and the ground truth data.",
    "claims": "\n1. A method of training a machine-learning model employing a procedurally synthesized training dataset, comprising:\ngenerating training image definitions in accordance with variations in content of training images to be included in a training dataset;\nrendering the training images corresponding to the training image definitions;\ngenerating, at least partially in parallel with the rendering, ground truth data corresponding to the training images, the training images and the ground truth comprising the training dataset; and\ntraining a machine-learning model using the training dataset and the ground truth data.\n2. The method as recited in claim 1, wherein the machine-learning model is for machine vision employed to at least partially operate some functionality of a motor vehicle.\n3. The method as recited in claim 1, wherein the machine-learning model is a pedestrian detector or a vehicle detector.\n4. The method as recited in claim 1, wherein the machine-learning model is for machine vision employed to at least partially operate some functionality of a robot.\n5. The method as recited in claim 1, wherein the variations are uniformly distributed according to different scenarios depicted in the training images.\n6. The method as recited in claim 1, wherein the variations in the content are variations in characteristics of the content.\n7. The method as recited in claim 6, wherein the training image definitions include limits on the characteristics based on user input.\n8. The method as recited in claim 1, wherein the training images are virtual images that correspond to real-world images.\n9. The method as recited in claim 8, wherein the rendering of the virtual images employs raytracing.\n10. The method as recited in claim 1, wherein the training images include a distribution of common and rare real-world images.\n11. A method of operating a machine, comprising:\nreceiving data corresponding to a plurality of objects; and\nrecognizing the plurality of objects using a machine-learning model that has been trained via a training dataset that has been procedurally synthesized by:\ngenerating training image definitions in accordance with variations in content of training images to be included in the training dataset;\nrendering the training images corresponding to the training image definitions; and\ngenerating, at least partially in parallel with the rendering, ground truth corresponding to the training images, the training images and the ground truth comprising the training dataset.\n12. The method as recited in claim 11, wherein the machine is a robot.\n13. The method as recited in claim 11, wherein the machine is a vehicle.\n14. The method as recited in claim 11, wherein the rendering is performed by a 3D graphics engine and the training images are virtual images that correspond to real-world images.\n15. The method as recited in claim 14, wherein the 3D graphics engine employs raytracing for rendering the virtual images.\n16. A machine, comprising:\na machine-learning model; and\na machine vision processor configured to identify objects employing the machine-learning model, wherein the machine-learning model has been trained via a training dataset that has been procedurally synthesized by:\ngenerating training image definitions in accordance with variations in content of training images to be included in the training dataset;\nrendering the training images corresponding to the training image definitions; and\ngenerating, at least partially in parallel with the rendering, ground truth corresponding to the training images, the training images and the ground truth comprising the training dataset.\n17. The machine as recited in claim 16, wherein the machine is a vehicle.\n18. The machine as recited in claim 17, wherein the machine-learning model is a pedestrian detector.\n19. The machine as recited in claim 16, wherein the machine is a robot.\n20. The machine as recited in claim 16, wherein the training images are virtual images that correspond to real-world images.",
    "status": "Active",
    "citations_own": [
        "GB2357412A",
        "US6421458B2",
        "US6578017B1",
        "US6947597B2",
        "US20070031028A1",
        "US7212665B2",
        "US7558772B2",
        "US7853072B2",
        "US7924146B2",
        "US8019702B1",
        "US20110249023A1",
        "US8224127B2",
        "US20120207371A1",
        "US8379994B2",
        "US8422797B2",
        "US8422994B2",
        "US8649606B2",
        "US20140079314A1",
        "US8774515B2",
        "US8813111B2",
        "US8861870B2",
        "US8971581B2",
        "US9111349B2",
        "US9183560B2",
        "US9183466B2",
        "US20150339570A1",
        "US9208405B2",
        "US9275308B2",
        "US9373033B2",
        "US9594983B2",
        "US20170109611A1",
        "US9665802B2",
        "US9870624B1",
        "US9996771B2",
        "US10078727B2",
        "US10139279B2"
    ],
    "citations_ftf": [
        "US9213892B2",
        "US10489691B2"
    ],
    "citedby_own": [
        "US20220261595A1"
    ],
    "citedby_ftf": [
        "EP3156942A1",
        "CN107958197A",
        "EP3343432A1",
        "US11392133B2",
        "US11042155B2",
        "US11573573B2",
        "US10235601B1",
        "US11334762B1",
        "WO2019059011A1",
        "US10970564B2",
        "EP3495771A1",
        "US10748247B2",
        "US10755115B2",
        "WO2019136375A1",
        "WO2019152888A1",
        "US10867214B2",
        "US10997433B2",
        "JP7028333B2",
        "US11537139B2",
        "US11080590B2",
        "US11436484B2",
        "WO2019189661A1",
        "US11550841B2",
        "US11256958B1",
        "US20200089999A1",
        "EP3647994A1",
        "CN113039563A",
        "CN113454636A",
        "US11170299B2",
        "WO2020140049A1",
        "WO2020163390A1",
        "WO2020164841A1",
        "CN113811886A",
        "US11165954B1",
        "US11042758B2",
        "US11713978B2",
        "US10997469B2",
        "CN111523469B",
        "US11328117B2",
        "US11335008B2",
        "DE102021126965A1",
        "US20230139772A1"
    ]
}