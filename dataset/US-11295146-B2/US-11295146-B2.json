{
    "patent_id": "US-11295146-B2",
    "title": "System and method for online real-time multi-object tracking ",
    "assignee": "Tusimple, Inc.",
    "publication_date": "2022-04-05",
    "patent_link": "https://patents.google.com/patent/US11295146B2/en",
    "inventors": [
        "Lingting GE",
        "Pengfei Chen",
        "Panqu Wang"
    ],
    "classifications": [
        "G06T7/277",
        "G06K9/00805",
        "G06N3/04",
        "G06N3/045",
        "G06N3/08",
        "G06T7/248",
        "G06T7/74",
        "G06V10/764",
        "G06V10/82",
        "G06V20/58",
        "B60Y2400/3015",
        "G06N20/10",
        "G06N20/20",
        "G06T2207/10016",
        "G06T2207/20081",
        "G06T2207/20084",
        "G06T2207/30248",
        "G06T2207/30261",
        "G06V10/62"
    ],
    "abstract": "A system and method for online real-time multi-object tracking is disclosed. A particular embodiment can be configured to: receive image frame data from at least one camera associated with an autonomous vehicle; generate similarity data corresponding to a similarity between object data in a previous image frame compared with object detection results from a current image frame; use the similarity data to generate data association results corresponding to a best matching between the object data in the previous image frame and the object detection results from the current image frame; cause state transitions in finite state machines for each object according to the data association results; and provide as an output object tracking output data corresponding to the states of the finite state machines for each object.",
    "claims": "\n1. A system comprising:\na data processor; and\na memory for storing an online real-time multi-object tracking system, executable by the data processor, the online real-time multi-object tracking system being configured to:\nreceive image frame data from at least one camera;\ndetect objects in the image frame data;\nextract an appearance feature from each of the objects;\ngenerate similarity data corresponding to a similarity between objects detected in a first image frame compared with objects detected in a second image frame;\nmaintain a template for each of the objects detected in the image frame data, wherein the template corresponds to the appearance feature extracted from each of the objects;\nupdate the template based on the similarity data; and\nmatch the objects detected in the first image frame with the objects detected in the second image frame based on the similarity data.\n2. The system of claim 1 being further configured to create a finite state machine for each of the objects detected, wherein the finite state machine for each of the objects represents a current object detection state of a plurality of possible object detection states for a corresponding object.\n3. The system of claim 1 being further configured to generate the similarity data using a motion feature similarity and an appearance feature similarity.\n4. The system of claim 1 being further configured to generate the similarity data using a motion feature similarity based on a prediction of a Kalman filter and a bounding box position of a detected object.\n5. The system claim 1 wherein a number of the appearance feature extracted from each of the objects is at least two, wherein a number of the template for each of the objects is at least two.\n6. The system claim 1 wherein the at least one camera is associated with an autonomous vehicle.\n7. The system claim 1 wherein the first image frame is earlier than the second image frame on a timeline.\n8. A method comprising:\nreceiving image frame data from at least one camera;\ndetecting objects in the image frame data;\nextracting an appearance feature from each of the objects;\ngenerating similarity data corresponding to a similarity between objects detected in a first image frame compared with objects detected in a second image frame;\nmaintaining a template for each of the objects detected in the image frame data, wherein the template corresponds to the appearance feature extracted from each of the objects;\nupdating the template based on the similarity data; and\nmatching the objects detected in the first image frame with the objects detected in the second image frame based on the similarity data.\n9. The method of claim 8 further comprising using the similarity data to generate data association results corresponding to a best match between the objects in the first image frame and the objects in the second image frame.\n10. The method of claim 9 further comprising causing state transitions in finite state machines for each of the objects according to the data association results.\n11. The method of claim 10 further comprising outputting output data corresponding to states of the finite state machines for each of the objects.\n12. The method of claim 11 further comprising smoothing the output data.\n13. The method of claim 11 wherein the output data is adjusted based on a weighted average calculation for data of the objects detected, or a prediction of a Kalman filter.\n14. The method of claim 10 wherein the finite state machines for each of the objects comprise states from the group consisting of: initialized, tracked, lost, and removed.\n15. A non-transitory machine-useable storage medium embodying instructions which, when executed by a machine, cause the machine to:\nreceive image frame data from at least one camera;\ndetect objects in the image frame data;\nextract an appearance feature from each of the objects;\ngenerate similarity data corresponding to a similarity between objects detected in a first image frame compared with objects detected in a second image frame;\nmaintain a template for each of the objects detected in the image frame data, wherein the template corresponds to the appearance feature extracted from each of the objects;\nupdate the template based on the similarity data; and\nmatch the objects detected in the first image frame with the objects detected in the second image frame based on the similarity data.\n16. The non-transitory machine-useable storage medium of claim 15 being further configured to generate the similarity data based on determining a similarity score between a position of a bounding box predicted by a Kalman filter and a position of a bounding box of each of the objects detected in an image frame.\n17. The non-transitory machine-useable storage medium of claim 15 wherein the appearance feature is extracted by a pre-trained convolutional neural network (CNN).\n18. The non-transitory machine-useable storage medium of claim 15 being further configured to determine a similarity score for a detected object and a templated object based on the appearance feature, wherein the non-transitory machine-useable storage medium is configured to update the template when:\nthe similarity score is less than a first threshold; and\na confidence level of a bounding box of the detected object is higher than a second threshold.\n19. The non-transitory machine-useable storage medium of claim 15 being further configured to use the similarity data to generate data association results corresponding to a best matching between the objects in the first image frame and the objects in the second image frame, wherein the data association results are generated based on results comprising: matched objects detected with object detection results, unmatched objects detected, and unmatched object detection results.\n20. The non-transitory machine-useable storage medium of claim 15 being further configured to generate the similarity data using an appearance feature similarity calculated as a Euclidean distance between the appearance feature of an object defined as the template and the appearance feature of the detected objects."
}