{
    "patent_id": "US-11295199-B2",
    "title": "XAI and XNN conversion ",
    "assignee": "UMNAI Limited",
    "publication_date": "2022-04-05",
    "patent_link": "https://patents.google.com/patent/US11295199B2/en",
    "inventors": [
        "Angelo Dalli",
        "Mauro PIRRONE"
    ],
    "classifications": [
        "G06N3/0454",
        "G06N5/045",
        "G06N3/045",
        "G06N3/08",
        "G06N3/082",
        "G06N3/084",
        "G06N3/10",
        "G06N5/022",
        "G06N3/044"
    ],
    "abstract": "In an exemplary embodiment, a method for extracting a model from an existing machine learning model may be shown and described. In black-box models, transfer learning consists of transferring knowledge with the objective of learning new patterns. However, in an exemplary embodiment, transfer learning presents the concept of converting an explainable neural network into logically equivalent variants, which may not be possible with black-box neural networks, which typically consist of multiple fully-connected layers. The white-box nature of an exemplary XNN or XAI enables new ways of transferring knowledge with intelligent conversions of neural networks in ways that are impossible to do with a black-box model.",
    "claims": "\n1. A computer implemented method for converting from a first system comprising at least one of a black-box, grey-box, or white-box neural network to an explainable neural network, comprising executing on a processor the steps of:\nidentifying one or more system partitions within the system;\nafter identifying the one or more system partitions, wherein the one or more system partitions include one or more overlapping partitions, extracting one or more conditions from the one or more system partitions within the system, wherein extracting the one or more conditions comprises resolving an overlap between the one or more overlapping partitions based on a priority function;\nforming rules from the extracted conditions;\naggregating the rules into a hierarchy of explainable network partitions comprising a plurality of partitions, said hierarchy comprising a structure in which one or more parent partitions contain one or more child partitions;\ntransforming one or more of the conditions;\ncombining one or more of the extracted and transformed conditions and identifying one or more coefficients or one or more coefficient weights related to the conditions and explainable network partitions;\ngenerating linear or non-linear equations from the coefficients, wherein the linear or non-linear equations are local models;\nrefining a set of rules produced by the coefficient weights, said set of rules comprising a causal logic model comprising a combination of factual and counterfactual rules; and\nconverting, based on a predefined conversion algorithm, an architecture formed from the generated linear or non-linear equations into a logically equivalent explainable neural network architecture having a different one or more of density, sparsity, and structure, said converting step further comprising preserving, in a conversion from the first system to the explainable neural network architecture, all information and all functionality from the first system, said explainable neural network architecture configured to provide, as an output, an explanation including a representation of a plurality of activated explainable neural modules provided simultaneously with an answer without performance loss.\n2. The method of claim 1, wherein converting comprises the steps of:\nadding one or more additional features to the system;\nmultiplying the additional features by one or more coefficient weights of zero; and\nrepeating the step of adding one or more additional features until a prediction network of the explainable neural network is fully connected.\n3. The method of claim 1, further comprising implementing the neural network on a GPU.\n4. The method of claim 1, wherein the set of rules are defined via a gradient descent technique.\n5. The method of claim 1, further comprising deleting one or more rules; pruning the one or more rules; and converting the one or more rules into the coefficients or the coefficient weights.\n6. The method of claim 2, wherein the explainable neural network is a dense neural network.\n7. The method of claim 2, further comprising obtaining knowledge via a model induction method, and identifying the one or more additional features based on the obtained knowledge.\n8. The method of claim 2, further comprising: receiving human knowledge and identifying the one or more additional features based on the human knowledge.\n9. The method of claim 8, wherein the human knowledge is represented as a rule in a logic-based format, and wherein converting further comprises directly combining the rule with a plurality of rules formed from the extracted conditions.\n10. The method of claim 1, wherein converting comprises the steps of:\nidentifying one or more zero-valued coefficients; and\neliminating the one or more zero-valued coefficients.\n11. The method of claim 10, further comprising implementing the explainable neural network on at least one of a CPU, an ASIC, neuromorphic hardware, analog/digital circuitry, a quantum enabled hardware circuit, a handheld device, an autonomous air, land, sea, underwater, or space vehicle system, an automated control system, a robot, or an Internet of Things device.\n12. The method of claim 10, further comprising: receiving human knowledge and identifying the one or more zero-valued coefficients based on at least one of the human knowledge and a machine learnt knowledge.\n13. The method of claim 10, wherein the system is a dense neural network, and the explainable neural network is a sparse neural network.\n14. The method of claim 1, further comprising implementing the explainable neural network directly as a hardware circuit using at least one of:\na flexible programmable gate array,\nan application specific integrated circuit,\na neuromorphic computing architecture, and\nquantum computing hardware.\n15. The method of claim 1, wherein the converting comprises the steps of:\nimplementing each of the one or more local models in an explainable neural module, wherein each explainable neural module is self-contained; and\naggregating the plurality of explainable neural modules to form a global model;\nwherein the plurality of explainable neural modules encompasses the plurality of activated explainable neural modules.\n16. The method of claim 15, further comprising training each of the plurality of explainable neural modules independently.\n17. The method of claim 15, wherein the plurality of explainable neural modules comprises at least one of a sparse neural network and a dense neural network.\n18. A system for providing an explainable neural network, comprising:\na hardware circuit or a computer processor other than a graphics processing unit, wherein the hardware circuit or computer processor is configured to implement a plurality of explainable neural modules, wherein each explainable neural module is configured to interface with another explainable neural module, wherein an output of each of the plurality of explainable neural modules is interpretable by at least one of the plurality of explainable neural modules, and wherein each of the plurality of explainable neural modules comprises a local model;\nwherein at least one of the plurality of explainable neural modules comprises a conditional layer configured to model an input based on a partition hierarchy comprising one or more partitions including one or more overlapping partitions, wherein each of the one or more partitions comprises a rule, and wherein said hierarchy comprises a structure in which one or more parent partitions contain one or more child partitions;\nwherein the conditional layer is further configured to resolve an overlap between the one or more overlapping partitions based on a priority function; and\nwherein the system is configured to provide, as a system output, an explanation including a representation of a plurality of activated explainable neural modules selected from the plurality of explainable neural modules, said plurality of activated explainable neural modules provided simultaneously with an answer without performance loss.\n19. The system of claim 18, wherein each of the plurality of explainable neural modules is independently implemented on a hardware resource comprising at least one of a CPU, an ASIC, neuromorphic hardware, analog/digital circuitry, a quantum enabled hardware circuit, a handheld device, an autonomous air, land, sea, underwater, or space vehicle system, an automated control system, a robot, or an Internet of Things device.\n20. The system of claim 18, wherein the plurality of explainable neural modules are deployed on quantum processing hardware.\n21. The system of claim 18, wherein each of the plurality of explainable neural modules is configurable to a user defined level of precision.\n22. The system of claim 18, wherein each of the plurality of explainable neural modules is trained independently.\n23. The system of claim 18, wherein the plurality of explainable neural modules comprises at least one of a dense neural network and a sparse neural network.\n24. The system of claim 18, wherein at least one of the plurality of explainable neural modules comprises one or more of:\nan aggregation layer configured to aggregate one or more rules into one or more of the partitions; and\na switch output layer configured to selectively pool the aggregated partitions from the aggregation layer with the remaining partitions from the conditional layer;\na feature generation and transformation network comprising one or more transformation neurons configured to apply one or more transformations to the input;\na fit layer configured to combine features which have been transformed by the feature generation and transformation network to identify one or more coefficients or one or more coefficient weights related to at least one of: one or more features and one or more partitions; and\na value output layer configured to output a value related to at least one of: one or more features, one or more partitions, as applied to the one or more coefficients; and\nan output layer configured to present the output which is interpretable and explainable by at least one of a machine program or a human.\n25. The system of claim 24, wherein at least one of the plurality of explainable neural modules forms a conditional network, the conditional network comprising the conditional layer, aggregation layer, and switch output layer.\n26. The system of claim 25, wherein the conditional network is one of a sparse neural network and a dense neural network.\n27. The system of claim 24, wherein one explainable neural module forms a prediction network, the prediction network comprising the feature generation and transformation network, the fit layer, and the value output layer.\n28. The system of claim 27, wherein the prediction network is one of a sparse neural network and a dense neural network.\n29. A system for converting from a first system comprising at least one of a black-box, grey-box, or white-box neural network to an explainable neural network, comprising a processor configured to execute the steps of:\nidentifying one or more system partitions within the system;\nafter identifying the one or more system partitions, wherein the one or more system partitions include one or more overlapping partitions, extracting one or more conditions from the one or more system partitions within the system, wherein extracting the one or more conditions comprises resolving an overlap between the one or more overlapping partitions based on a priority function;\nforming rules from the extracted conditions;\naggregating the rules into a hierarchy of explainable network partitions comprising a plurality of partitions, said hierarchy comprising a structure in which one or more parent partitions contain one or more child partitions;\ntransforming one or more of the conditions;\ncombining one or more of the extracted and transformed conditions and identifying one or more coefficients or one or more coefficient weights related to the conditions and explainable network partitions;\ngenerating linear or non-linear equations from the coefficients, wherein the linear or non-linear equations are local models;\nrefining a set of rules produced by the coefficient weights, said set of rules comprising a causal logic model comprising a combination of factual and counterfactual rules; and\nconverting, based on a predefined conversion algorithm, an architecture formed from the generated linear or non-linear equations into a logically equivalent explainable neural network architecture having a different density."
}