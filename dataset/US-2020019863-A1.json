{
    "patent_id": "US-2020019863-A1",
    "title": "Generative Adversarial Network Based Modeling of Text for Natural Language Processing ",
    "assignee": "International Business Machines Corporation",
    "publication_date": "2020-01-16",
    "patent_link": "https://patents.google.com/patent/US20200019863A1/en",
    "inventors": [
        "Dheeru Dua",
        "Cicero Nogueira dos Santos",
        "Bowen Zhou"
    ],
    "classifications": [
        "G06F40/216",
        "G06F16/313",
        "G06F17/2715",
        "G06F17/30616",
        "G06F18/2178",
        "G06F40/30",
        "G06K9/6263",
        "G06N3/042",
        "G06N3/045",
        "G06N3/047",
        "G06N3/088",
        "G06N5/041",
        "G06V10/464",
        "G06V10/7784",
        "G10L15/02",
        "G10L15/197",
        "G06N3/044",
        "G06N3/048",
        "G06N5/02"
    ],
    "abstract": "Mechanisms are provided to implement a generative adversarial network (GAN) for natural language processing. With these mechanisms, a generator neural network of the GAN is configured to generate a bag-of-ngrams (BoN) output based on a noise vector input and a discriminator neural network of the GAN is configured to receive a BoN input, where the BoN input is either the BoN output from the generator neural network or a BoN input associated with an actual portion of natural language text. The mechanisms further configure the discriminator neural network of the GAN to output an indication of a probability as to whether the input BoN is from the actual portion of natural language text or is the BoN output of the generator neural network. Moreover, the mechanisms train the generator neural network and discriminator neural network based on a feedback mechanism that compares the output indication from the discriminator neural network to an indicator of whether the input BoN is from the actual portion of natural language text of the BoN output of the generator neural network.",
    "claims": "\n1. A method, in a data processing system comprising at least one processor and at least one memory, the at least one memory comprising instructions executed by the at least one processor to configure the processor to implement a generative adversarial network (GAN) for natural language processing, the method comprising:\nconfiguring a generator neural network of the GAN to generate a bag-of-ngrams (BoN) output based on a noise vector input;\nconfiguring a discriminator neural network of the GAN to receive a BoN input, where the BoN input is either the BoN output from the generator neural network or a BoN input associated with an actual portion of natural language text;\nconfiguring the discriminator neural network of the GAN to output an indication of a probability as to whether the input BoN is from the actual portion of natural language text or is the BoN output of the generator neural network; and\ntraining the generator neural network and discriminator neural network based on a feedback mechanism that compares the output indication from the discriminator neural network to an indicator of whether the input BoN is from the actual portion of natural language text of the BoN output of the generator neural network.\n2. The method of claim 1, wherein the generator neural network produces the BoN output as a vector output, and wherein each vector slot in the vector output of the BoN output is set to a value indicative of a probability of whether a corresponding ngram is in the BoN.\n3. The method of claim 1, wherein the discriminator neural network performs one or more statistical value analysis operations or feature extraction analysis operations on the BoN output of the generator neural network to score the BoN output and generate the indication of the probability as to whether the BoN output is from an actual portion of natural language text.\n4. The method of claim 3, wherein the one or more statistical value analysis operations or feature extraction analysis operations performed on the BoN output comprises at least one of term frequency analysis or inverse document frequency analysis.\n5. The method of claim 1, wherein the generator neural network, during training of the GAN:\nreceives a noise vector input;\nprojects and replicates the noise vector input to form a first matrix data structure;\nretrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN;\ngenerates a second matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the second matrix;\nconcatenates the first matrix and the second matrix to generate a concatenation matrix;\ninputs each row of the concatenation matrix into a neural network; and\nprocesses each row of the concatenation matrix through the neural network to generate the BoN output.\n6. The method of claim 5, wherein each row of the concatenation matrix comprises a first portion corresponding to the first matrix, and a second portion corresponding to the second matrix.\n7. The method of claim 5, wherein the neural network is a multi-layer perceptron that uses rectified linear unit as an activation function of an output layer of the neural network, and wherein the neural network outputs a numerical value that indicates a probability that a corresponding ngram is present in the BoN based on the noise vector input.\n8. The method of claim 1, wherein the discriminator neural network, during training of the GAN:\nreceives the BoN input;\nretrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN;\ngenerates a first matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the first matrix;\nmultiplies the BoN input with the first matrix;\nprojects results of the multiplication of the BoN input with the first matrix to generate a second matrix;\nperforms sum pooling on the second matrix to generate a feature vector output; and\nprocesses the feature vector output via a neural network to generate an output indicating whether or not the BoN input is from the actual portion of natural language text or is the BoN output of the generator neural network.\n9. The method of claim 8, wherein the neural network is a multi-layer perceptron with a sigmoid activation function in an output layer of the multi-layer perceptron.\n10. The method of claim 1, further comprising:\ngenerating, by the trained GAN, a BoN output representing a bag-of-ngrams that approximate actual natural language text; and\nperforming natural language processing on a portion of natural language text based on the BoN output generated by the trained GAN.\n11. A computer program product comprising a computer readable storage medium having a computer readable program stored therein, wherein the computer readable program, when executed on a computing device, configures the computing device to implement a generative adversarial network (GAN) for natural language processing, and causes the computing device to:\nconfigure a generator neural network of the GAN to generate a bag-of-ngrams (BoN) output based on a noise vector input;\nconfigure a discriminator neural network of the GAN to receive a BoN input, where the BoN input is either the BoN output from the generator neural network or a BoN input associated with an actual portion of natural language text;\nconfigure the discriminator neural network of the GAN to output an indication of a probability as to whether the input BoN is from the actual portion of natural language text or is the BoN output of the generator neural network; and\ntrain the generator neural network and discriminator neural network based on a feedback mechanism that compares the output indication from the discriminator neural network to an indicator of whether the input BoN is from the actual portion of natural language text of the BoN output of the generator neural network.\n12. The computer program product of claim 11, wherein the generator neural network produces the BoN output as a vector output, and wherein each vector slot in the vector output of the BoN output is set to a value indicative of a probability of whether a corresponding ngram is in the BoN.\n13. The computer program product of claim 11, wherein the discriminator neural network performs one or more statistical value analysis operations or feature extraction analysis operations on the BoN output of the generator neural network to score the BoN output and generate the indication of the probability as to whether the BoN output is from an actual portion of natural language text.\n14. The computer program product of claim 13, wherein the one or more statistical value analysis operations or feature extraction analysis operations performed on the BoN output comprises at least one of term frequency analysis or inverse document frequency analysis.\n15. The computer program product of claim 11, wherein the generator neural network, during training of the GAN:\nreceives a noise vector input;\nprojects and replicates the noise vector input to form a first matrix data structure;\nretrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN;\ngenerates a second matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the second matrix;\nconcatenates the first matrix and the second matrix to generate a concatenation matrix;\ninputs each row of the concatenation matrix into a neural network; and\nprocesses each row of the concatenation matrix through the neural network to generate the BoN output.\n16. The computer program product of claim 15, wherein each row of the concatenation matrix comprises a first portion corresponding to the first matrix, and a second portion corresponding to the second matrix.\n17. The computer program product of claim 15, wherein the neural network is a multi-layer perceptron that uses rectified linear unit as an activation function of an output layer of the neural network, and wherein the neural network outputs a numerical value that indicates a probability that a corresponding ngram is present in the BoN based on the noise vector input.\n18. The computer program product of claim 11, wherein the discriminator neural network, during training of the GAN:\nreceives the BoN input;\nretrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN;\ngenerates a first matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the first matrix;\nmultiplies the BoN input with the first matrix;\nprojects results of the multiplication of the BoN input with the first matrix to generate a second matrix;\nperforms sum pooling on the second matrix to generate a feature vector output; and\nprocesses the feature vector output via a neural network to generate an output indicating whether or not the BoN input is from the actual portion of natural language text or is the BoN output of the generator neural network.\n19. The computer program product of claim 18, wherein the neural network is a multi-layer perceptron with a sigmoid activation function in an output layer of the multi-layer perceptron.\n20. An apparatus comprising:\nat least one processor; and\nat least one memory coupled to the at least one processor, wherein the at least one memory comprises instructions which, when executed by the at least one processor, configures the at least one processor to implement a generative adversarial network (GAN) for natural language processing, and causes the at least one processor to:\nconfigure a generator neural network of the GAN to generate a bag-of-ngrams (BoN) output based on a noise vector input;\nconfigure a discriminator neural network of the GAN to receive a BoN input, where the BoN input is either the BoN output from the generator neural network or a BoN input associated with an actual portion of natural language text;\nconfigure the discriminator neural network of the GAN to output an indication of a probability as to whether the input BoN is from the actual portion of natural language text or is the BoN output of the generator neural network; and\ntrain the generator neural network and discriminator neural network based on a feedback mechanism that compares the output indication from the discriminator neural network to an indicator of whether the input BoN is from the actual portion of natural language text of the BoN output of the generator neural network."
}