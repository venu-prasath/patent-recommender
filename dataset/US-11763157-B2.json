{
    "patent_id": "US-11763157-B2",
    "title": "Protecting deep learned models ",
    "assignee": "Microsoft Technology Licensing, Llc",
    "publication_date": "2023-09-19",
    "patent_link": "https://patents.google.com/patent/US11763157B2/en",
    "inventors": [
        "Sriram Srinivasan",
        "David Yuheng Zhao",
        "Ming-Chieh Lee",
        "Mu Han"
    ],
    "classifications": [
        "G06N3/082",
        "G06F21/62",
        "G06F17/16",
        "G06N3/045",
        "G06N3/063",
        "G06N3/08",
        "G06N5/04",
        "G06N20/00",
        "G06N3/044",
        "G06N5/01",
        "G06N7/01"
    ],
    "abstract": "Apparatus and methods are disclosed for using machine learning models with private and public domains. Operations can be applied to transform input to a machine learning model in a private domain that is kept secret or otherwise made unavailable to third parties. In one example of the disclosed technology, a method includes applying a private transform to produce transformed input, providing the transformed input to a machine learning model that was trained using a training set modified by the private transform, and generating inferences with the machine learning model using the transformed input. Examples of suitable transforms that can be employed include matrix multiplication, time or spatial domain to frequency domains, and partitioning a neural network model such that an input and at least one hidden layer form part of the private domain, while the remaining layers form part of the public domain.",
    "claims": "\n1. A computing system adapted to provide access to computing system resources having a plurality of access domains, the plurality of access domains comprising a public domain and a private domain, the computer system being configured to provide access to the public domain with at least one permissions configuration that does not provide access to the private domain, the computing system comprising:\nat least one processor; and\ncomputer-readable storage devices and/or memory storing computer-readable instructions that when executed by the at least one processor, cause the computing system to execute a machine learning model using at least the public domain computing system resources, the instructions comprising:\ninstructions that cause the processor to provide a scrambler by accessing computing system resources that are accessible to the private domain but not to the public domain,\ninstructions that cause the processor to perform a transform operation on input using the provided scrambler to produce transformed input suitable for use with the machine learning model, the transform operation being performed with a matrix provided by the scrambler using the private domain computing resources, access to the matrix being restricted from the public domain computing resources, and\ninstructions that cause the processor to, with computing system resources of the public domain of the processor, perform execution of the machine learning model, the execution comprising propagating the transformed input provided by the scrambler to outputs of the machine learning model.\n2. The computing system of claim 1, the instructions further comprising:\ninstructions that cause the processor to perform training reconfiguration of the machine learning model by:\nforward propagating the transformed input to outputs of the machine learning model,\ndetermining errors between expected values of the outputs and the machine learning model outputs, and\nadjusting at least one parameter of the machine learning model based on the determined errors; and\ninstructions that cause the processor to repeatedly perform the training reconfiguration until a metric of the determined errors exceeds a predetermined threshold.\n3. The computing system of claim 1, wherein the computer-readable storage devices and/or memory further comprise:\ninstructions that cause the processor to generate a prediction for the input based on at least one of the outputs, the outputs being generated by forward propagating the transformed input to outputs of the machine learning model.\n4. The computing system of claim 1, wherein:\nthe input is expressed in a physical domain and the transformed input is in a frequency domain; and\nthe transformed input comprises rearranged samples of the frequency domain, the samples being rearranged using the scrambler.\n5. The computing system of claim 1, wherein the instructions that cause the processor to provide the scrambler include:\ninstructions that cause the processor to obtain a decryption key from the private domain compute resources; and\ninstructions that cause the processor to perform the transform by decrypting data and/or instructions to provide the scrambler from a computer-readable storage device or memory with the decryption key.\n6. The computing system of claim 1, wherein the machine learning model is an artificial neural network, and wherein the scrambler provides an input layer and nodes for a next layer of the artificial neural network that are accessible to computing system resources accessible to the private domain but not accessible to the public domain, and wherein at least some of the remaining nodes of the artificial neural network are accessible to computing system resources of the public domain.\n7. A method of operating a computing system having private compute resources and public compute resources, the method being performed with a processor of the computing system executing a machine learning model, the method comprising:\nby the computing system:\nobtaining scrambler data from a private compute resource, the scrambler data being restricted from being accessed by the public compute resources, the scrambler data comprising a decryption key;\nscrambling input using the obtained scrambler data to produce transformed input suitable for use with the public compute resources of the machine learning model, the scrambling comprising using the decryption key to move, transpose, and/or substitute data for at least a portion of the input; and\npropagating the transformed input produced using the obtained scrambler data to outputs of the machine learning model using the public compute resources.\n8. The method of claim 7, wherein the machine learning model is an artificial neural network, and wherein the method further comprises training the artificial neural network by:\nrepeatedly adjusting weights and/or activation functions for at least one node of the artificial neural network, the adjusted weights and/or activation functions being determined by calculating loss for the outputs produced by the propagating using the transformed input; and\nstoring data representing the adjusted weights and/or activation functions in a computer-readable storage device or medium.\n9. The method of claim 7, further comprising generating inferences with the machine learning model by:\noutputting at least one prediction for the input based on the outputs produced by the propagating using the transformed input.\n10. The method of claim 7, wherein the scrambler data comprises data, computer-executable instructions, or data and computer-executable instructions used to perform scrambling the input, the method further comprising:\nencrypting the scrambler data; and\nstoring the encrypted scrambler data in a computer-readable storage device or medium.\n11. The method of claim 7, wherein the scrambling the input comprises multiplying the input by a matrix produced from the obtained scrambler data.\n12. The method of claim 7, wherein:\nthe machine learning model is an artificial neural network;\nthe scrambling the input uses the private compute resource to provide nodes for an input layer and nodes for at least a next layer of the artificial neural network; and\nthe propagating the transformed input comprises using the public compute resources, but not the private compute resource, to process node values for the remaining nodes of the artificial neural network.\n13. The method of claim 12, further comprising:\nstoring data representing at least a portion of the input layer nodes or the next layer nodes in a proprietary format in a computer-readable storage device or medium that has access to the public compute resources restricted; and\nstoring data representing at least a portion of the remaining nodes in an open source format in a computer-readable storage device or medium accessible by the public compute resources.\n14. The method of claim 12, wherein:\nthe input comprises audio data, image data, and/or video data; and\nthe scrambling the input comprises transforming the audio data, image data, and/or video data to a proprietary feature domain.\n15. The method of claim 12, wherein the scrambling the input comprises:\ntransforming physical domain samples in the input to frequency domain samples; and\nmoving at least one of the frequency domain samples to a different position in the transformed input.\n16. One or more computer-readable storage media storing computer-readable instructions, which when executed, cause a computer system to perform a method of operating the computing system, the computing system having private compute resources and public compute resources, the method being performed with a processor of the computing system executing a machine learning model, the machine learning model being an artificial neural network, the method comprising:\n17. One or more computer-readable storage media storing data and/or instructions for an artificial neural network having a public domain and a private domain, the data and/or instructions comprising at least one of:\nprivate domain computer-executable instructions, which when executed cause a computer system to transform input into transformed input suitable for propagating through the artificial neural network;\nprivate domain data, which when applied as the input, is rendered suitable for propagating through the artificial neural network by the private domain computer-executable instructions, the private domain data comprising a tensor, which when applied to input, renders the input suitable for propagating through the artificial neural network;\npublic domain computer-executable instructions, which when executed cause a computer system to propagate transformed input through the artificial neural network to generate predictions for the input; or\npublic domain data representing at least a portion of nodes of the artificial neural network, which, when the public domain of the artificial neural network receives input rendered suitable for the public domain, generates output used to generate predictions for the input.\n18. The computer-readable storage media of claim 17, wherein the private domain data comprises:\nthe private domain computer-executable instructions stored in an encrypted format, in a container secured by a license key, in object code, or in a dynamically-linked library (DLL).\n19. The computing system of claim 1, wherein the instructions that cause the processor to perform a transform operation comprise instructions for performing a Fourier transform to the input used to provided the transformed input.\n20. The computing system of claim 1, wherein the instructions that cause the processor to perform a transform operation comprise instructions for performing at least one of the following transforms to the input used to provided the transformed input: a discrete cosine transform (DCT), a Haar transform, a Taylor series transform, a wavelet transform, or a Hadamard transform."
}