{
    "patent_id": "US-11373405-B2",
    "title": "Methods and systems of combining video content with one or more augmentations to produce augmented video ",
    "assignee": "Second Spectrum, Inc.",
    "publication_date": "2022-06-28",
    "patent_link": "https://patents.google.com/patent/US11373405B2/en",
    "inventors": [
        "Yu-Han Chang",
        "Rajiv Tharmeswaran Maheswaran",
        "Jeffrey Wayne Su",
        "Emil Dotchevski",
        "Jason Kent Simon"
    ],
    "classifications": [
        "A63F13/60",
        "A63F13/65",
        "G06F3/011",
        "G06F3/012",
        "G06F3/013",
        "G06N20/00",
        "G06N20/10",
        "G06N7/01",
        "G06V20/42",
        "G06V20/46",
        "G11B27/031",
        "G11B27/11",
        "G11B27/28",
        "G11B27/34",
        "H04N13/204",
        "H04N21/2187",
        "H04N21/23418",
        "H04N21/251",
        "H04N21/26603",
        "H04N21/4223",
        "H04N21/4345",
        "H04N21/44008",
        "H04N21/4532",
        "H04N21/4662",
        "H04N21/8455",
        "H04N21/8456",
        "H04N21/8549",
        "H04N5/2224",
        "G06N3/006",
        "G06T2207/20081",
        "G06T2207/30221",
        "H04N13/117",
        "H04N13/243"
    ],
    "abstract": "Data processing systems and methods are disclosed for combining video content with one or more augmentations to produce augmented video. Objects within video content may have associated bounding boxes that may each be associated with respective RGB values. Upon user selection of a pixel, the RGBA value of the pixel may be used to determine a bounding box associated with the RGBA value. The client may transmit an indicator of the determined bounding box to an augmentation system to request augmentation data for the object associated with the bounding box. The system then uses the indicator to determine the augmentation data and transmits the augmentation data to the client device.",
    "claims": "\n1. A computer-implemented data processing method for displaying augmented content on a client device, the method comprising:\nreceiving, from an external server by the client device, video data corresponding to an event, the video data comprising video content and a plurality of definitions of a plurality of bounding areas;\npresenting, on a graphical user interface displayed via the client device, the video content;\ndetecting, by the client device, a user selection of a portion of the graphical user interface;\ndetermining, by the client device, a value associated with one or more pixels associated with the portion of the graphical user interface;\ndetermining, by the client device, a particular bounding area from the plurality of bounding areas based at least in part on a bounding area value associated with the particular bounding area that corresponds to the value associated with the one or more pixels associated with the portion of the graphical user interface;\ntransmitting, by the client device, an indication of the particular bounding area to a renderer;\nreceiving, by the client device from the renderer, augmentation data associated with the particular bounding area;\ngenerating, by the client device, augmented video content based on the video data and the augmentation data associated with the particular bounding area; and\npresenting, by the client device on the graphical user interface, the augmented video content.\n2. The computer-implemented data processing method of claim 1, wherein each definition of the plurality of definitions is associated with a respective object represented in the video content.\n3. The computer-implemented data processing method of claim 2, wherein the augmentation data associated with the particular bounding area comprises an augmentation image and video frame data and auxiliary data associated with the augmentation image.\n4. The computer-implemented data processing method of claim 3, wherein transmitting the indication of the particular bounding area to the renderer causes the renderer to select the augmentation image from among a plurality of augmentation images associated with the respective object.\n5. The computer-implemented data processing method of claim 1, wherein the renderer comprises one of an external rendering engine and a client-side rendering engine.\n6. The computer-implemented data processing method of claim 1, wherein detecting the user selection of the portion of the graphical user interface comprises detecting a user tap or a user click of a display on the client device displaying the graphical user interface.\n7. A client device, the client device comprising:\none or more computer processors;\nmemory storing computer-executable instructions that, when executed by the one or more computer processors, cause the one or more computer processors to perform operations comprising:\nreceiving, from an external server, video data corresponding to an event, the video data comprising video content and a plurality of transparent bounding areas;\npresenting, on a graphical user interface, the video content;\ndetecting a user selection of a portion of the graphical user interface;\ndetermining a color value associated with the portion of the graphical user interface;\nselecting a particular transparent bounding area from among the plurality of transparent bounding areas by determining that a transparent bounding area color value associated with the particular transparent bounding area corresponds to the color value associated with the portion of the graphical user interface;\ntransmitting an indicator of the particular transparent bounding area to a renderer;\nreceiving, from the renderer, one or more augmentation images associated with the particular transparent bounding area;\nreceiving, from the renderer, video frame data and location data associated with the one or more augmentation images; and\npresenting the one or more augmentation images on the graphical user interface based on the video frame data and the location data.\n8. The client device of claim 7, wherein each of the one or more augmentation images is a Portable Network Graphics (PNG) image.\n9. The client device of claim 7, wherein the video data further comprises a respective pre-defined transparent bounding area color value for each transparent bounding area of the plurality of transparent bounding areas.\n10. The client device of claim 7, wherein each transparent bounding area of the plurality of transparent bounding areas is associated with a respective object represented in the video content.\n11. The client device of claim 10, wherein the event is a sporting event, and\nthe respective object associated with at least one transparent bounding area of the plurality of transparent bounding areas corresponds to a player in the sporting event.\n12. The client device of claim 10, wherein the event is a sporting event, and\nthe respective object associated with at least one transparent bounding area of the plurality of transparent bounding areas corresponds to a non-player object in the sporting event.\n13. The client device of claim 7, wherein presenting the one or more augmentation images on the graphical user interface based on the video frame data and the location data comprises presenting the one or more augmentation images on the graphical user interface in conjunction with the video content so that, when the one or more augmentation images are presented, the one or more augmentation images remain in a substantially fixed orientation relative to an object associated with the one or more augmentation images as the video content is presented on the graphical user interface.\n14. The client device of claim 7, wherein the color value associated with the portion of the graphical user interface comprises a red, green, blue, alpha (RGBA) value.\n15. A non-transitory computer-readable medium storing computer-executable instructions, the computer-executable instructions executable by one or more processors for performing operations comprising\ndetermining a plurality of objects within a plurality of video frames of video content;\nassigning a plurality of bounding areas to the plurality of objects, wherein each bounding area is associated with a respective object represented in one or more frames of the plurality of video frames;\nassigning a respective color value to each bounding area of the plurality of bounding areas;\ntransmitting, to a client device, video data corresponding to an event, wherein the video data comprises the video content, the plurality of bounding areas, and the assigned color values;\nreceiving, from the client device, an indicator of a particular bounding area identified from among the plurality of bounding areas, wherein the particular bounding area is identified based on the respective color value assigned to the particular bounding area;\ndetermining a particular object within a frame of the plurality of video frames of the video content based on the indicator of the particular bounding area;\ndetermining a current augmentation state for the particular object;\nselecting an augmentation image from among a plurality of augmentation images associated with the particular object based on the current augmentation state for the particular object;\ndetermining video frame data and location data associated with the particular object; and\ntransmitting the augmentation image, the video frame data, and the location data to the client device.\n16. The non-transitory computer-readable medium of claim 15, wherein each of the respective color values assigned to each bounding area of the plurality of bounding areas comprises an opacity value.\n17. The non-transitory computer-readable medium of claim 16, wherein the operations further comprise:\nassigning an opacity value of zero to each bounding area of the plurality of bounding areas.\n18. The non-transitory computer-readable medium of claim 15, wherein determining a current augmentation state for the particular object comprises determining that there is no current augmentation image associated with the particular object.\n19. The non-transitory computer-readable medium of claim 15, wherein selecting the augmentation image from among the plurality of augmentation images associated with the particular object based on the current augmentation state for the particular object comprises:\nselecting a next sequential augmentation image from among a sequence of augmentation images, wherein the current augmentation state is associated with a current augmentation image, and the next sequential augmentation image follows the current augmentation image in the sequence of augmentation images.\n20. The non-transitory computer-readable medium of claim 15, wherein each of the respective color values assigned to each bounding area of the plurality of bounding areas comprises a red, green, blue, alpha (RGBA) value.",
    "status": "Active",
    "citations_own": [
        "US5060171A",
        "WO1996031047A2",
        "WO2002071334A2",
        "US20030079224A1",
        "US20030093810A1",
        "US20030172346A1",
        "US6631522B1",
        "US20050107159A1",
        "US20050160458A1",
        "US6959253B2",
        "US7085322B2",
        "US20060256210A1",
        "US7143083B2",
        "US20080193016A1",
        "US20080225130A1",
        "CN101271527A",
        "US20090183103A1",
        "US7657836B2",
        "US7699707B2",
        "US7753794B2",
        "US7796155B1",
        "US20110013087A1",
        "US7932923B2",
        "US20110202397A1",
        "US20110275045A1",
        "US8238662B2",
        "US8275672B1",
        "US20120265758A1",
        "US8295683B2",
        "CN102750695A",
        "US8316303B2",
        "US20130027757A1",
        "CN102985926A",
        "US8413188B2",
        "US8453027B2",
        "CN103294716A",
        "WO2013166456A2",
        "US8597133B2",
        "US8620077B1",
        "US20140029921A1",
        "US8645991B2",
        "US20140037140A1",
        "US20140085443A1",
        "US8718439B2",
        "US8810408B2",
        "US8856030B2",
        "US8965172B2",
        "WO2015060691A1",
        "US9110988B1",
        "US9124856B2",
        "US20150248917A1",
        "US9141258B2",
        "US9171369B2",
        "US9168452B2",
        "US20160080830A1",
        "US9294710B2",
        "WO2016057844A1",
        "US9339710B2",
        "US9342785B2",
        "US20160148650A1",
        "US20160234566A1",
        "US9440152B2",
        "US20160378861A1",
        "US9596399B2",
        "US9609373B2",
        "US9613661B2",
        "US9740977B1",
        "US9740984B2",
        "US9744457B2",
        "US9750433B2",
        "US20170255828A1",
        "US9814977B2",
        "US20180025078A1",
        "US9881221B2",
        "WO2018053257A1",
        "US20180225827A1",
        "US20190075176A1",
        "US10231787B2",
        "US10248812B2",
        "US10360685B2",
        "WO2019183235A1",
        "US20190294631A1",
        "US20190311254A1",
        "US20190354765A1",
        "US20200074181A1",
        "US20200394824A1",
        "US11074447B1"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US11645841B2"
    ],
    "citedby_ftf": [
        "US10521671B2",
        "US10460765B2",
        "US10969748B1",
        "US11524242B2",
        "US11050809B2",
        "US10678244B2",
        "US11409692B2",
        "US10671349B2",
        "US11157441B2",
        "US10257578B1",
        "US10970560B2",
        "US11561791B2",
        "US11601721B2",
        "US11215999B2",
        "US11636333B2",
        "US11562231B2",
        "US11196678B2",
        "US11537811B2",
        "US11610117B2",
        "US10997461B2",
        "US11094130B2",
        "US11567514B2",
        "US10956755B2",
        "US20220053245A1",
        "US11023729B1",
        "US11113535B2",
        "US11076276B1",
        "US10972809B1",
        "US11475249B2",
        "US11450111B2",
        "JP7157781B2",
        "EP4210847A1",
        "CN112132054A",
        "US11776273B1",
        "US11665373B2",
        "US11748988B1",
        "US20230010078A1",
        "US20230013988A1",
        "CN113316021B",
        "CN113657478B",
        "CN113673541B"
    ]
}