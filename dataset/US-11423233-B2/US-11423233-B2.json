{
    "patent_id": "US-11423233-B2",
    "title": "On-device projection neural networks for natural language understanding ",
    "assignee": "Google Llc",
    "publication_date": "2022-08-23",
    "patent_link": "https://patents.google.com/patent/US11423233B2/en",
    "inventors": [
        "Sujith Ravi",
        "Zornitsa KOZAREVA"
    ],
    "classifications": [
        "G06F40/30",
        "G06F40/253",
        "G06N3/04",
        "G06N3/044",
        "G06N3/063",
        "G06N3/082",
        "G06N3/084",
        "G06N3/045",
        "G06N5/022",
        "G06N7/01"
    ],
    "abstract": "The present disclosure provides projection neural networks and example applications thereof. In particular, the present disclosure provides a number of different architectures for projection neural networks, including two example architectures which can be referred to as: Self-Governing Neural Networks (SGNNs) and Projection Sequence Networks (ProSeqoNets). Each projection neural network can include one or more projection layers that project an input into a different space. For example, each projection layer can use a set of projection functions to project the input into a bit-space, thereby greatly reducing the dimensionality of the input and enabling computation with lower resource usage. As such, the projection neural networks provided herein are highly useful for on-device inference in resource-constrained devices. For example, the provided SGNN and ProSeqoNet architectures are particularly beneficial for on-device inference such as, for example, solving natural language understanding tasks on-device.",
    "claims": "\n1. A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to implement:\na machine-learned multi-layered projection model configured to receive a projection model input and to generate a projection model output from the projection model input, the machine-learned multi-layered projection model comprising:\na sequence of one or more projection layers, wherein each projection layer has a plurality of projection layer parameters, wherein each projection layer is configured to:\nreceive a layer input;\napply a plurality of projection layer functions to the layer input, each projection layer function generating a respective projection function output that projects the layer input to a different space, and\ngenerate a layer output by applying the projection layer parameters for the projection layer to the projection function outputs; and\na sequence of one or more additional model layers configured to receive a layer output generated by a highest projection layer in the sequence of one or more projection layers and to generate one or more additional model layer outputs;\nwherein execution of the instructions causes the one or more computers to perform operations comprising:\nobtaining the projection model input;\ninputting the projection model input into the machine-learned multi-layered projection model; and\nreceiving the projection model output generated by the machine-learned multi-layered projection model.\n2. The system of claim 1, wherein the machine-learned multi-layered projection model further comprises:\nan output layer configured to receive the additional model layer output generated by a highest additional model layer in the sequence of one or more additional model layers and to generate the projection model output.\n3. The system of claim 1, wherein the sequence of one or more projection layers comprises a plurality of projection layers one after the other.\n4. The system of claim 1, wherein the sequence of one or more additional model layers comprises one or more neural model layers.\n5. The system of claim 1, wherein the sequence of one or more additional model layers comprises one or more additional projection layers.\n6. The system of claim 1, wherein the sequence of one or more additional model layers comprises one or more projection sequence layers.\n7. The system of claim 1, wherein the machine-learned multi-layered projection model is trained to perform on-device text classification.\n8. The system of claim 7, wherein execution of the instructions by the one or more computers causes the one or more computers to:\nreceive an input text;\nconvert the input text into an intermediate feature vector; and\ninput the intermediate feature vector as the projection model input to the machine-learned multi-layered projection model.\n9. The system of claim 8, wherein the intermediate feature vector comprises one or more of the following intermediate features that have been generated from or associated with the input text:\nskip-grams;\nn-grams;\npart of speech tags;\ndependency relationships;\nknowledge graph information; or\ncontextual information.\n10. The system of claim 1, wherein, for each projection layer, the plurality of projection layer functions are precomputed and held static.\n11. The system of claim 1, wherein, for each projection layer, the plurality of projection layer functions are modeled using locality sensitive hashing.\n12. The system of claim 1, wherein execution of the instructions by the one or more computers causes the one or more computers to:\ndynamically compute the plurality of projection layer functions at inference time using one or more seeds.\n13. The system of claim 1, wherein the machine-learned multi-layered projection model comprises a self-governing neural model that performs natural language processing without initializing, loading, or storing any feature or vocabulary weight matrices.\n14. The system of claim 1, wherein the machine-learned multi-layered projection model has been trained based solely on its own performance relative to training data.\n15. The system of claim 1, wherein, for each projection layer, each projection function is associated with a respective set of projection vectors, and wherein applying each projection function to the layer input comprises:\nfor each projection vector:\ndetermining a dot product between the layer input and the projection vector;\nwhen the dot product is negative, assigning a first value to a corresponding position in the projection function output; and\nwhen the dot product is positive, assigning a second value to the corresponding position in the projection function output.\n16. The system of claim 1, wherein, for each projection layer, the projection functions are each encoded as sparse matrices and are used to generate a binary representation from the layer input.\n17. The system of claim 1, wherein the projection layer parameters include a parameter matrix and a bias vector, and wherein generating the layer output by applying the projection layer parameters for the projection layer to the projection function outputs comprises:\napplying the parameter matrix to the projection function outputs and then adding the bias vector.\n18. A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to implement:\na projection sequence model configured to receive an input and to generate an output from the input, the projection sequence model comprising:\na sequence of one or more projection layers, wherein each projection layer has a plurality of projection layer parameters, wherein each projection layer is configured to:\nreceive a layer input;\napply a plurality of projection layer functions to the layer input, each projection layer function generating a respective projection function output that projects the layer input to a different space, and\ngenerate a layer output by applying the projection layer parameters for the projection layer to the projection function outputs; and\none or more projection sequence layers positioned after the sequence of one or more projection layers, wherein each of the one or more projection sequence layers is configured to provide first internal state data to a subsequent iteration of such projection sequence layer in a subsequent iteration of the projection sequence model and to receive second internal state data from the subsequent iteration of such projection sequence layer in the subsequent iteration of the projection sequence model;\nwherein execution of the instructions causes the one or more computers to perform operations comprising:\nobtaining the input;\ninputting the input into the projection sequence model; and\nreceiving the output generated by the projection sequence model.\n19. The system of claim 18, wherein the input comprises text and the output comprises segments of the text that have been identified and classified into respective ones of a plurality of classes by the projection sequence model.\n20. The system of claim 18, wherein each projection sequence layer comprises:\na first set of nodes configured to provide the first internal state data of the first set of nodes to a subsequent iteration of the first set of nodes in the subsequent iteration of such projection sequence layer in the subsequent iteration of the projection sequence model; and\na second set of nodes that are distinct from the first set of nodes, the second set of nodes configured to receive the second internal state data from a subsequent iteration of the second set of nodes in the subsequent iteration of such projection sequence layer in the subsequent iteration of the projection sequence model.",
    "status": "Active",
    "citations_own": [
        "WO1993013487A1",
        "US20140067735A1",
        "US20140156575A1",
        "US20150074027A1",
        "CN104538028A",
        "US20160078339A1",
        "US20160307564A1",
        "US20160307566A1",
        "EP3144859A2",
        "US20170132528A1",
        "US20170139913A1",
        "US9842106B2",
        "US20180121799A1",
        "US20180150744A1",
        "US9990687B1",
        "US20180356771A1"
    ],
    "citations_ftf": [],
    "citedby_own": [],
    "citedby_ftf": [
        "US8977255B2",
        "US10417037B2",
        "WO2014124332A2",
        "US10170123B2",
        "US10747498B2",
        "US9946437B2",
        "US10586535B2",
        "DK201670540A1",
        "DK180048B1",
        "DK201770429A1",
        "DK179496B1",
        "CN107908635B",
        "US10878817B2",
        "US11080485B2",
        "US10928918B2",
        "DK180639B1",
        "US10872601B1",
        "US11741356B2",
        "CN111368996A",
        "US11444845B1",
        "US11348573B2",
        "US11704573B2",
        "US11494615B2",
        "US11307752B2",
        "US11217251B2",
        "US11468890B2",
        "US11354501B2",
        "US11231703B2",
        "US11620435B2",
        "US11338199B2",
        "CN113554145A",
        "US11783812B2",
        "US11183193B1",
        "US11610065B2",
        "CN111832316A",
        "US11438683B2",
        "CN112906662B",
        "US11797270B2",
        "US11734013B2",
        "US11675592B2",
        "US11669331B2",
        "US11693692B2",
        "US11269632B1"
    ]
}