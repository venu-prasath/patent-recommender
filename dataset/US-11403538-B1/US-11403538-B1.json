{
    "patent_id": "US-11403538-B1",
    "title": "Methods and apparatus for generating fast counterfactual explanations for black-box models using reinforcement learning ",
    "assignee": "Arthur AI, Inc.",
    "publication_date": "2022-08-02",
    "patent_link": "https://patents.google.com/patent/US11403538B1/en",
    "inventors": [
        "Sahil Verma",
        "John Dickerson",
        "Keegan Hines"
    ],
    "classifications": [
        "G06N5/045",
        "G06N20/00",
        "G06F18/214",
        "G06F18/2185",
        "G06F18/295",
        "G06K9/6264",
        "G06K9/6297",
        "G06N7/01",
        "G06N3/006",
        "G06N5/01"
    ],
    "abstract": "In an embodiment, the systems and methods discussed herein are related to generating, via a processor, a Markov Decision Process (MDP), the MDP including a state space, an action space, a transition function, a reward function, and a discount factor. A reinforcement learning (RL) model is applied, via the processor, to the MDP to generate a RL agent. An input data associated with a first user is received at the RL agent. At least one counterfactual explanation (CFE) is generated via the processor and by the RL agent and based on the input data. A representation of the at least one CFE and at least one recommended remedial action is caused to transmit, via the processor, to at least one of a compute device of the first user or a compute device of a second user different from and associated with the first user.",
    "claims": "\n1. A method, comprising:\ngenerating, via a processor, a Markov Decision Process (MDP), based on (1) output data from a classifier, (2) training data associated with the classifier, and (3) at least one actionable feature, the MDP including a state space, an action space, a transition function, a reward function, and a discount factor;\napplying, via the processor, a reinforcement learning (RL) model to the MDP to generate a RL agent;\nreceiving, via the processor and at the RL agent, first data associated with a first user;\ngenerating, via the processor and by the RL agent, at least one first counterfactual explanation (CFE) based on the first data;\nreceiving, at the RL agent, second data associated with a second user different from the first user;\ngenerating, via the processor and by the RL agent, at least one second CFE based on the second data;\ncausing transmission, via the processor, of a representation of the at least one first CFE to a compute device of the first user; and\ncausing transmission, via the processor, of a representation of the at least one second CFE and at least one recommended remedial action to a compute device of the second user.\n2. The method of claim 1, wherein the at least one actionable feature includes a numerical actionable feature.\n3. The method of claim 1, wherein the at least one actionable feature includes a categorical actionable feature.\n4. The method of claim 1, wherein the at least one actionable feature includes at least two actionable features, and the generating the MDP is further based on at least one causal constraint, defining a relationship between the at least two actionable features, that is to be maintained.\n5. The method of claim 4, wherein the at least one causal constraint includes a unary constraint.\n6. The method of claim 4, wherein the at least one causal constraint includes a binary constraint.\n7. The method of claim 1, wherein the RL agent is configured to maximize the reward function.\n8. The method of claim 1, wherein the generating of the MDP is further based on a tunable parameter associated with the reward function, the tunable parameter defining a pre-selectable similarity between at least one of (1) the at least one first CFE and the training data, or (2) the at least one second CFE and the training data.\n9. A method, comprising:\ngenerating, via a processor, a Markov Decision Process (MDP), based on (1) output data from a classifier, (2) training data associated with the classifier, and (3) at least one actionable feature, the MDP including a state space, an action space, a transition function, and a reward function,\nthe state space including a terminal state, a dummy state associated with the terminal state and at least one non-terminal state,\nthe reward function defining an association between at least one of:\nthe terminal state and a positive reward,\nthe dummy state and a positive reward,\na transition from the terminal state to the dummy state and a positive reward,\nperforming the at least one actionable feature and a first negative reward, or\nachieving the at least one non-terminal state and at least one of no reward or a second negative reward;\napplying, via the processor, a reinforcement learning (RL) model to the MDP to generate a RL agent;\nreceiving, via the processor and at the RL agent, an input data associated with a user;\ngenerating, via the processor and by the RL agent, at least one counterfactual explanation (CFE) based on the input data; and\ncausing transmission, via the processor, of a representation of the at least one CFE and at least one recommended remedial action to a compute device of the user.\n10. The method of claim 9, wherein the MDP further includes a discount factor representing a relative prioritization by the RL agent of short-term reward relative to long-term reward.\n11. The method of claim 9, wherein the at least one actionable feature includes a plurality of actionable features, and the at least one CFE modifies a single actionable feature from the plurality of actionable features.\n12. The method of claim 9, wherein the MDP further includes a tunable parameter associated with the reward function defining a pre-selectable similarity between the at least one CFE and the training data.\n13. The method of claim 9, wherein the at least one actionable feature includes a plurality of actionable features, and the association between performing the at least one actionable feature and the first negative award further includes:\nperforming a first actionable feature from the plurality of actionable features and the first negative reward.\n14. A method, comprising\ngenerating, via a processor, a Markov Decision Process (MDP), the MDP including a state space, an action space, a transition function, a reward function, and a discount factor;\napplying, via the processor, a reinforcement learning (RL) model to the MDP to generate a RL agent;\nreceiving, at the RL agent, an input data associated with a first user;\ngenerating, via the processor, by the RL agent, and based on the input data, at least one counterfactual explanation (CFE); and\ncausing transmission, via the processor, of a representation of the at least one CFE and at least one recommended remedial action to at least one of a compute device of the first user or a compute device of a second user different from and associated with the first user.\n15. The method of claim 14, wherein the input data is a first input data and the at least one CFE is a first CFE, the method further comprising:\nreceiving, at the RL agent, a second input data associated with a third user different from the first user and the second user; and\ngenerating, by the RL agent, a second CFE based on the second input data,\nthe causing transmission including causing transmission of a representation of the second CFE to at least one of the compute device of the first user, the compute device of the second user, or a compute device of the third user.\n16. The method of claim 14, wherein the RL agent is configured to maximize the reward function.\n17. The method of claim 14, wherein the at least one recommended remedial action is associated with at least one of a financial decision, a medical decision, a hiring decision, a parole decision, or a fraud detection decision.\n18. The method of claim 14, wherein the action space includes a representation of a plurality of actionable features, and the at least one CFE modifies a single actionable feature from the plurality of actionable features.\n19. The method of claim 14, wherein the state space includes a terminal state, dummy state, and at least one non-terminal state, and the reward function defines an association between at least one of:\nthe terminal state and a positive reward,\nthe dummy state and a positive reward,\na transition from the terminal state to the dummy state and a positive reward,\nperforming an action in the action space and a first negative reward, or\nachieving the at least one non-terminal state and at least one of no reward or a second negative reward.\n20. The method of claim 9, wherein the at least one actionable feature includes a plurality of actionable features, and the association between performing the at least one actionable feature and the first negative award further includes:\nperforming a second actionable feature from the plurality of actionable features and the second negative reward."
}