{
    "patent_id": "US-11663456-B2",
    "title": "On-the-fly deep learning in machine learning at autonomous machines ",
    "assignee": "Intel Corporation",
    "publication_date": "2023-05-30",
    "patent_link": "https://patents.google.com/patent/US11663456B2/en",
    "inventors": [
        "Raanan Yonatan Yehezkel Rohekar"
    ],
    "classifications": [
        "G06N3/063",
        "G06N20/20",
        "G06F18/214",
        "G06F18/2148",
        "G06F18/2411",
        "G06F18/2413",
        "G06F9/46",
        "G06N3/04",
        "G06N3/044",
        "G06N3/045",
        "G06N3/08",
        "G06N3/084",
        "G06V10/454",
        "G06V10/764",
        "G06V10/774",
        "G06V10/82",
        "G06V10/95",
        "G06V10/955",
        "G06V20/00",
        "G06F9/505",
        "G06V2201/06",
        "G06V40/174"
    ],
    "abstract": "A mechanism is described for facilitating the transfer of features learned by a context independent pre-trained deep neural network to a context dependent neural network. The mechanism includes extracting a feature learned by a first deep neural network (DNN) model via the framework, wherein the first DNN model is a pre-trained DNN model for computer vision to enable context-independent classification of an object within an input video frame and training, via the deep learning framework, a second DNN model for computer vision based on the extracted feature, the second DNN model an update of the first DNN model, wherein training the second DNN model includes training the second DNN model based on a dataset including context-dependent data.",
    "claims": "\n1. A data processing system on a computing device, the data processing system comprising:\none or more storage devices comprising a graphics execution environment including instructions to provide a deep learning framework to accelerate deep learning operations via one or more general-purpose graphics processors of the computing device, the deep learning framework to cause the one or more general-purpose graphics processors to perform operations comprising:\nextracting, via the deep learning framework, a feature learned by a first deep neural network (DNN) model via the framework, wherein the first DNN model is a pre-trained DNN model for computer vision to enable context-independent classification of an object within an input video frame; and\ntraining, via the deep learning framework, a second DNN model for computer vision based on the extracted feature and a dataset including context-dependent data, the second DNN model an update of the first DNN model, wherein the deep learning framework is to provide a library of machine learning primitives, the machine learning primitives accelerated via instructions executed by the one or more general-purpose graphics processors, and training the second DNN model includes executing one or more primitives provided by the deep learning framework to cause the one or more general-purpose graphics processors to perform operations to train the second DNN model.\n2. The data processing system as in claim 1, wherein training the second DNN model includes training the second DNN model separately from the first DNN model.\n3. The data processing system as in claim 1, wherein the library of machine learning primitives includes primitives to perform tensor convolution, at least one activation function, and a pooling operation.\n4. The data processing system as in claim 3, wherein the library of machine learning primitives includes primitives to implement basic linear algebra subprograms associated with respective layers of the second DNN model, the respective layers including a fully connected layer.\n5. The data processing system as in claim 1, wherein the graphics execution environment is a virtualized environment.\n6. The data processing system as in claim 5, wherein the one or more general-purpose graphics processors are configurable into partitions and the graphics execution environment is to execute as a virtualized environment by one or more partitions of the general-purpose graphics processors.\n7. The data processing system as in claim 1, further comprising:\na network interface to enable communication with an external system, the external system including one or more general-purpose graphics processors; and\nwherein training the second DNN model for computer vision via the deep learning framework includes interfacing with an instance of the deep learning framework on the external system and training the second DNN model via the one or more general-purpose graphics processors of the external system.\n8. The data processing system as in claim 7, wherein training the second DNN model includes training the second DNN model to perform computer vision operations for autonomous navigation.\n9. The data processing system as in claim 1, wherein the one or more general-purpose graphics processors include multiple general-purpose graphics processors, the multiple general-purpose graphics processors interconnected via peer-to-peer links between the multiple general-purpose graphics processors.\n10. A method comprising:\nextracting, by one or more general-purpose graphics processors of a data processing system, a feature learned by a first deep neural network (DNN) model, wherein the first DNN model is a pre-trained DNN model for computer vision to enable context-independent classification of an object within an input video frame, instructions executed by the general-purpose processor to extract the feature are provided via a deep learning framework, the deep learning framework is provided by a graphics execution environment on a server computing device, and the deep learning framework provides instructions to accelerate deep learning operations; and\ntraining, via the deep learning framework, a second DNN model for computer vision based on the extracted feature and a dataset including context-dependent data, the second DNN model an update of the first DNN model, wherein the deep learning framework is to provide a library of machine learning primitives, the machine learning primitives accelerated via instructions executed by the one or more general-purpose graphics processors and training the second DNN model includes executing one or more primitives provided by the deep learning framework to cause the one or more general-purpose graphics processors to perform operations to train the second DNN model.\n11. The method as in claim 10, wherein training the second DNN model includes training the second DNN model separately from the first DNN model.\n12. The method as in claim 10, additionally comprising:\ndetecting an output associated with the first DNN model;\n13. The method as in claim 12, wherein training the second DNN model includes training the second DNN model to perform computer vision operations for autonomous navigation.\n14. The method as in claim 12, wherein the library of machine learning primitives includes primitives to perform tensor convolution, at least one activation function, and a pooling operation.\n15. The method as in claim 14, wherein the library of machine learning primitives includes primitives to implement basic linear algebra subprograms associated with respective layers of the second DNN model, the respective layers including a fully connected layer.\n16. A non-transitory machine-readable medium storing instructions which, when executed by one or more processors including one or more general-purpose graphics processors, cause the one or more processors to perform operations comprising:\nextracting, by the one or more general-purpose graphics processors, a feature learned by a first deep neural network (DNN) model, wherein the first DNN model is a pre-trained DNN model for computer vision to enable context-independent classification of an object within an input video frame, the instructions executed by the general-purpose processor to extract the feature are provided via a deep learning framework, the deep learning framework is provided by a graphics execution environment on a server computing device, and the deep learning framework provides instructions to accelerate deep learning operations; and\ntraining, via the deep learning framework, a second DNN model for computer vision based on the extracted feature and a dataset including context-dependent data, the second DNN model an update of the first DNN model, wherein the deep learning framework is to provide a library of machine learning primitives, the machine learning primitives accelerated via instructions executed by the one or more general-purpose graphics processors and training the second DNN model includes executing one or more primitives provided by the deep learning framework to cause the one or more general-purpose graphics processors to perform operations to train the second DNN model.\n17. The non-transitory machine-readable medium as in claim 16, wherein training the second DNN model includes training the second DNN model separately from the first DNN model.\n18. The non-transitory machine-readable medium as in claim 17, the operations additionally comprising:\ndetecting an output associated with the first DNN model;\n19. The non-transitory machine-readable medium as in claim 18, wherein the library of machine learning primitives includes primitives to perform tensor convolution, at least one activation function, and a pooling operation.\n20. The non-transitory machine-readable medium as in claim 19, wherein the library of machine learning primitives includes primitives to implement basic linear algebra subprograms associated with respective layers of the second DNN model, the respective layers including a fully connected layer.",
    "status": "Active",
    "citations_own": [
        "US7873812B1",
        "US20160062947A1",
        "US20160110642A1",
        "US20160342888A1",
        "US20160379112A1",
        "US20170024849A1",
        "US20170300767A1",
        "US20170308789A1",
        "US20180046906A1",
        "US20180293691A1",
        "CN108805292A",
        "US20190114541A1",
        "US20200081744A1",
        "US10884761B2",
        "US20210174214A1",
        "US20210275918A1",
        "US11146564B1",
        "US20220398751A1"
    ],
    "citations_ftf": [
        "US7219085B2",
        "US9477925B2",
        "WO2014117096A1",
        "US9620145B2",
        "US9953634B1",
        "CA2951723C",
        "GB2532075A",
        "US9786270B2",
        "US9767565B2",
        "US10884503B2",
        "US10229672B1",
        "US9805255B2",
        "CN106096602A",
        "US11263516B2",
        "US10891538B2",
        "KR102033411B1",
        "US10970768B2",
        "CN106600667B",
        "US10268200B2",
        "US10671840B2",
        "US11232782B2"
    ],
    "citedby_own": [],
    "citedby_ftf": [
        "US11037330B2",
        "US10572773B2",
        "US11392133B2",
        "US11573573B2",
        "US11042155B2",
        "US10409667B2",
        "US11003992B2",
        "US10802489B1",
        "US11321612B2",
        "US10769310B2",
        "US11468291B2",
        "US10891514B2",
        "CN111400021B",
        "CN110147251B",
        "US11507677B2",
        "CN109617845B",
        "CN111680798A",
        "CN110378472A",
        "JP7231511B2",
        "US11636411B2",
        "US11429839B2",
        "US11663814B2",
        "CN110488835B",
        "US11453404B2",
        "US11663523B2",
        "US11562267B2",
        "US11475374B2",
        "CN110674770A",
        "US11727314B2",
        "EP4062333A2",
        "CN112787840B",
        "US11288515B2",
        "JP7363407B2",
        "CN110837896B",
        "US10691133B1",
        "US11367290B2",
        "US10956807B1",
        "US11366434B2",
        "US11687778B2",
        "US11599376B1",
        "CN111341102B",
        "CN110991625B",
        "JP2021142234A",
        "US11500858B2",
        "US11222201B2",
        "US11734576B2",
        "US20210329306A1",
        "CN111523676A",
        "CN111698327B",
        "US11574175B2",
        "CN111767059B",
        "US11742901B2",
        "EP3958182A1",
        "TWI757999B",
        "WO2022140650A2",
        "CN114697206A",
        "CN112732591B",
        "CN114202027B"
    ]
}