{
    "patent_id": "US-2020019642-A1",
    "title": "Question Answering Using Trained Generative Adversarial Network Based Modeling of Text ",
    "assignee": "International Business Machines Corporation",
    "publication_date": "2020-01-16",
    "patent_link": "https://patents.google.com/patent/US20200019642A1/en",
    "inventors": [
        "Dheeru Dua",
        "Cicero Nogueira dos Santos",
        "Bowen Zhou"
    ],
    "classifications": [
        "G06F17/30654",
        "G06F16/3329",
        "G06F16/3347",
        "G06F17/2715",
        "G06F17/3069",
        "G06F40/216",
        "G06F40/30",
        "G06N3/042",
        "G06N3/045",
        "G06N3/047",
        "G06N3/08",
        "G06N3/088",
        "G06N5/02",
        "G06N5/041",
        "G10L15/16",
        "G10L15/197",
        "G10L15/22",
        "G06N3/048",
        "G10L2015/225"
    ],
    "abstract": "Mechanisms are provided for implementing a Question Answering (QA) system utilizing a trained generator of a generative adversarial network (GAN) that generates a bag-of-ngrams (BoN) output representing unlabeled data for performing a natural language processing operation. The QA system obtains a plurality of candidate answers to a natural language question, where each candidate answer comprises one or more ngrams. For each candidate answer, a confidence score is generated based on a comparison of the one or more ngrams in the candidate answer to ngrams in the BoN output of the generator neural network of the GAN. A final answer to the input natural language question is selected from the plurality of candidate answers based on the confidence scores associated with the candidate answers, and is output.",
    "claims": "\n1. A method, in a data processing system comprising at least one processor and at least one memory, the at least one memory comprising instructions executed by the at least one processor to configure the processor to implement a Question Answering (QA) system, the method comprising:\ntraining a generator neural network of a generative adversarial network (GAN) to generate a bag-of-ngrams (BoN) output representing unlabeled data for performing a natural language processing operation;\nobtaining, by the QA system, a plurality of candidate answers to a natural language question, wherein each candidate answer comprises one or more ngrams;\ngenerating, by the QA system, for each candidate answer in the plurality of candidate answers, a confidence score associated with the candidate answer based on a comparison of the one or more ngrams in the candidate answer to ngrams in the BoN output of the generator neural network of the GAN, wherein the confidence score represents a confidence that the candidate answer is a correct answer to the input natural language question;\nselecting, by the QA system, at least one final answer to the input natural language question from the plurality of candidate answers based on the confidence scores associated with the candidate answers; and\noutputting, by the QA system, the selected at least one final answer to the source computing device.\n2. The method of claim 1, wherein the generator neural network is trained to generate a BoN that represents ngrams present in a correct answer to the input question.\n3. The method of claim 1, wherein generating, for each candidate answer in the plurality of candidate answers, the confidence score associated with the candidate answer comprises scoring the candidate answer based on a degree of matching of the one or more ngrams in the candidate answers to ngrams in the BoN.\n4. The method of claim 3, wherein a candidate answer having a highest degree of matching with the BoN is selected as the at least one final answer for the input natural language question.\n5. The method of claim 1, wherein training the generator neural network of a GAN to generate a bag-of-ngrams (BoN) output representing unlabeled data for performing a natural language processing operation comprises:\nconfiguring the generator neural network of the GAN to generate a bag-of-ngrams (BoN) output based on a noise vector input;\nconfiguring a discriminator neural network of the GAN to receive a BoN input, where the BoN input is either the BoN output from the generator neural network or a BoN input associated with an actual portion of natural language text;\nconfiguring the discriminator neural network of the GAN to output an indication of a probability as to whether the input BoN is from the actual portion of natural language text or is the BoN output of the generator neural network; and\ntraining the generator neural network and discriminator neural network based on a feedback mechanism that compares the output indication from the discriminator neural network to an indicator of whether the input BoN is from the actual portion of natural language text of the BoN output of the generator neural network.\n6. The method of claim 5, wherein:\nthe generator neural network produces the BoN output as a vector output, and wherein each vector slot in the vector output of the BoN output is set to a value indicative of a probability of whether a corresponding ngram is in the BoN, and\nthe discriminator neural network performs one or more statistical value analysis operations or feature extraction analysis operations on the BoN output of the generator neural network to score the BoN output and generate the indication of the probability as to whether the BoN output is from an actual portion of natural language text.\n7. The method of claim 5, wherein the generator neural network, during training of the generator neural network and discriminator neural network:\nreceives a noise vector input;\nprojects and replicates the noise vector input to form a first matrix data structure;\nretrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN;\ngenerates a second matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the second matrix;\nconcatenates the first matrix and the second matrix to generate a concatenation matrix;\ninputs each row of the concatenation matrix into a neural network; and\nprocesses each row of the concatenation matrix through the neural network to generate the BoN output.\n8. The method of claim 5, wherein each row of the concatenation matrix comprises a first portion corresponding to the first matrix, and a second portion corresponding to the second matrix.\n9. The method of claim 5, wherein:\nthe neural network is a multi-layer perceptron that uses rectified linear unit as an activation function of an output layer of the neural network, and\nthe neural network outputs a numerical value that indicates a probability that a corresponding ngram is present in the BoN based on the noise vector input.\n10. The method of claim 5, wherein the discriminator neural network, during training of the generator neural network and discriminator neural network:\nreceives the BoN input;\nretrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN;\ngenerates a first matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the first matrix;\nmultiplies the BoN input with the first matrix;\nprojects results of the multiplication of the BoN input with the first matrix to generate a second matrix;\nperforms sum pooling on the second matrix to generate a feature vector output; and\nprocesses the feature vector output via a neural network to generate an output indicating whether or not the BoN input is from the actual portion of natural language text or is the BoN output of the generator neural network.\n11. A computer program product comprising a computer readable storage medium having a computer readable program stored therein, wherein the computer readable program, when executed on a computing device, configures the computing device to implement a Question Answering (QA) system, and causes the computing device to:\ntrain a generator neural network of a generative adversarial network (GAN) to generate a bag-of-ngrams (BoN) output representing unlabeled data for performing a natural language processing operation;\nobtain, by the QA system, a plurality of candidate answers to a natural language question, wherein each candidate answer comprises one or more ngrams;\ngenerate, by the QA system, for each candidate answer in the plurality of candidate answers, a confidence score associated with the candidate answer based on a comparison of the one or more ngrams in the candidate answer to ngrams in the BoN output of the generator neural network of the GAN, wherein the confidence score represents a confidence that the candidate answer is a correct answer to the input natural language question;\nselect, by the QA system, at least one final answer to the input natural language question from the plurality of candidate answers based on the confidence scores associated with the candidate answers; and\noutput, by the QA system, the selected at least one final answer to the source computing device.\n12. The computer program product of claim 11, wherein the generator neural network is trained to generate a BoN that represents ngrams present in a correct answer to the input question.\n13. The computer program product of claim 11, wherein generating, for each candidate answer in the plurality of candidate answers, the confidence score associated with the candidate answer comprises scoring the candidate answer based on a degree of matching of the one or more ngrams in the candidate answers to ngrams in the BoN.\n14. The computer program product of claim 13, wherein a candidate answer having a highest degree of matching with the BoN is selected as the at least one final answer for the input natural language question.\n15. The computer program product of claim 11, wherein training the generator neural network of a GAN to generate a bag-of-ngrams (BoN) output representing unlabeled data for performing a natural language processing operation comprises:\nconfiguring the generator neural network of the GAN to generate a bag-of-ngrams (BoN) output based on a noise vector input;\nconfiguring a discriminator neural network of the GAN to receive a BoN input, where the BoN input is either the BoN output from the generator neural network or a BoN input associated with an actual portion of natural language text;\nconfiguring the discriminator neural network of the GAN to output an indication of a probability as to whether the input BoN is from the actual portion of natural language text or is the BoN output of the generator neural network; and\ntraining the generator neural network and discriminator neural network based on a feedback mechanism that compares the output indication from the discriminator neural network to an indicator of whether the input BoN is from the actual portion of natural language text of the BoN output of the generator neural network.\n16. The computer program product of claim 15, wherein:\nthe generator neural network produces the BoN output as a vector output, and wherein each vector slot in the vector output of the BoN output is set to a value indicative of a probability of whether a corresponding ngram is in the BoN, and\nthe discriminator neural network performs one or more statistical value analysis operations or feature extraction analysis operations on the BoN output of the generator neural network to score the BoN output and generate the indication of the probability as to whether the BoN output is from an actual portion of natural language text.\n17. The computer program product of claim 15, wherein the generator neural network, during training of the generator neural network and discriminator neural network:\nreceives a noise vector input;\nprojects and replicates the noise vector input to form a first matrix data structure;\nretrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN;\ngenerates a second matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the second matrix;\nconcatenates the first matrix and the second matrix to generate a concatenation matrix;\ninputs each row of the concatenation matrix into a neural network; and\nprocesses each row of the concatenation matrix through the neural network to generate the BoN output.\n18. The computer program product of claim 15, wherein each row of the concatenation matrix comprises a first portion corresponding to the first matrix, and a second portion corresponding to the second matrix.\n19. The computer program product of claim 15, wherein the discriminator neural network, during training of the generator neural network and discriminator neural network:\nreceives the BoN input;\nretrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN;\ngenerates a first matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the first matrix;\nmultiplies the BoN input with the first matrix;\nprojects results of the multiplication of the BoN input with the first matrix to generate a second matrix;\nperforms sum pooling on the second matrix to generate a feature vector output; and\nprocesses the feature vector output via a neural network to generate an output indicating whether or not the BoN input is from the actual portion of natural language text or is the BoN output of the generator neural network.\n20. An apparatus comprising:\nat least one processor; and\nat least one memory coupled to the at least one processor, wherein the at least one memory comprises instructions which, when executed by the at least one processor, configures the at least one processor to implement a Question Answering (QA) system, and causes the at least one processor to:\ntrain a generator neural network of a generative adversarial network (GAN) to generate a bag-of-ngrams (BoN) output representing unlabeled data for performing a natural language processing operation;\nobtain, by the QA system, a plurality of candidate answers to a natural language question, wherein each candidate answer comprises one or more ngrams;\ngenerate, by the QA system, for each candidate answer in the plurality of candidate answers, a confidence score associated with the candidate answer based on a comparison of the one or more ngrams in the candidate answer to ngrams in the BoN output of the generator neural network of the GAN, wherein the confidence score represents a confidence that the candidate answer is a correct answer to the input natural language question;\nselect, by the QA system, at least one final answer to the input natural language question from the plurality of candidate answers based on the confidence scores associated with the candidate answers; and\noutput, by the QA system, the selected at least one final answer to the source computing device."
}