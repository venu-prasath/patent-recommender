{
    "patent_id": "US-11120266-B2",
    "title": "Augmented reality display device with deep learning sensors ",
    "assignee": "Magic Leap, Inc.",
    "publication_date": "2021-09-14",
    "patent_link": "https://patents.google.com/patent/US11120266B2/en",
    "inventors": [
        "Andrew Rabinovich",
        "Tomasz Jan Malisiewicz",
        "Daniel DeTone"
    ],
    "classifications": [
        "G06K9/00671",
        "G06F3/011",
        "A63F13/00",
        "A63F13/211",
        "A63F13/212",
        "A63F13/213",
        "A63F13/428",
        "G02B27/017",
        "G06F1/163",
        "G06F18/214",
        "G06F18/2413",
        "G06F3/0338",
        "G06F3/0346",
        "G06F3/04842",
        "G06K9/00255",
        "G06K9/00288",
        "G06K9/4628",
        "G06K9/627",
        "G06N3/006",
        "G06N3/04",
        "G06N3/044",
        "G06N3/0445",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/08",
        "G06N5/01",
        "G06N7/01",
        "G06T19/006",
        "G06V10/454",
        "G06V10/764",
        "G06V10/82",
        "G06V20/20",
        "G06V40/166",
        "G06V40/172",
        "G06F2203/012",
        "G06K9/6256",
        "G06N7/005"
    ],
    "abstract": "A head-mounted augmented reality (AR) device can include a hardware processor programmed to receive different types of sensor data from a plurality of sensors (e.g., an inertial measurement unit, an outward-facing camera, a depth sensing camera, an eye imaging camera, or a microphone); and determining an event of a plurality of events using the different types of sensor data and a hydra neural network (e.g., face recognition, visual search, gesture identification, semantic segmentation, object detection, lighting detection, simultaneous localization and mapping, relocalization).",
    "claims": "\n1. A system comprising:\nnon-transitory computer-readable memory storing executable instructions; and\none or more processors programmed by the executable instructions to at least:\ntrain a neural network based on a training set comprising a plurality of sensor data as input data and a plurality of corresponding types of events as output data;\nwherein the neural network comprises:\nan input layer for receiving input of the neural network,\na plurality of intermediate layers including a first intermediate layer of the plurality of intermediate layers coupled to the input layer, and\na plurality of head components including a head output node coupled to a last intermediate layer of the plurality of intermediate layers through a plurality of head component layers.\n2. The system of claim 1, wherein the neural network is configured to detect events based on the sensor data.\n3. The system of claim 2, wherein the events comprise one or more of: face recognition, visual search, gesture identification, semantic segmentation, object detection, lighting detection, simultaneous localization and mapping, or relocalization.\n4. The system of claim 1, wherein the plurality of intermediate layers comprises a plurality of lower layers and a plurality of middle layers,\nwherein the plurality of lower layers is trained to extract lower level features from the sensor data, and\nwherein the plurality of middle layers is trained to extract higher level features from the lower level features extracted.\n5. The system of claim 1, wherein the plurality of intermediate layers or the plurality of head component layers comprises one or more of: a convolution layer, a brightness normalization layer, a batch normalization layer, a rectified linear layer, an upsampling layer, a pooling layer, a concatenation layer, a fully connected layer, a linear fully connected layer, a softsign layer, or a recurrent layer.\n6. The system of claim 1, wherein the plurality of sensor data is received from a plurality of different types of sensors.\n7. The system of claim 6, wherein the plurality of different types of sensors include one or more of: an inertial measurement unit, a depth sensing camera, a microphone, or an eye imaging camera.\n8. The system of claim 4, wherein the plurality of middle layers or the plurality of head component layers comprises a pooling layer.\n9. The system of claim 1, wherein the one or more processors is further programmed by the executable instructions to at least:\nretrain the neural network based on a second training set comprising a second plurality of sensor data;\nwherein a second head component of the plurality of head components comprises a second head output node for outputting results associated with a second different type of event, and\nwherein the head output node is connected to the last intermediate layer through a plurality of second head component layers.\n10. The system of claim 9, wherein said retraining the neural network comprises updating weights associated with the plurality of second head component layers.\n11. The system of claim 9, wherein the neural network is retrained without updating weights associated with the plurality of intermediate layers.\n12. The system of claim 9, wherein the second plurality of sensor data is of a second type different than a first type of the sensor data used in training the neural network.\n13. A computerized method, performed by a computing system having one or more hardware computer processors and one or more non-transitory computer readable storage device storing software instructions executable by the computing system to perform the computerized method comprising:\ntraining a neural network based on a training set comprising a plurality of sensor data as input data and a plurality of corresponding types of events as output data;\nwherein the neural network comprises:\nan input layer for receiving input of the neural network,\na plurality of intermediate layers including a first intermediate layer of the plurality of intermediate layers coupled to the input layer, and\na plurality of head components including a head output node coupled to a last intermediate layer of the plurality of intermediate layers through a plurality of head component layers.\n14. The computerized method of claim 13, wherein the neural network is configured to detect events based on the sensor data.\n15. The computerized method of claim 14, wherein the events comprise one or more of: face recognition, visual search, gesture identification, semantic segmentation, object detection, lighting detection, simultaneous localization and mapping, or relocalization.\n16. The computerized method of claim 13, wherein the plurality of intermediate layers comprises a plurality of lower layers and a plurality of middle layers,\nwherein the plurality of lower layers is trained to extract lower level features from the sensor data, and\nwherein the plurality of middle layers is trained to extract higher level features from the lower level features extracted.\n17. The computerized method of claim 13, wherein the plurality of intermediate layers or the plurality of head component layers comprises one or more of: a convolution layer, a brightness normalization layer, a batch normalization layer, a rectified linear layer, an upsampling layer, a pooling layer, a concatenation layer, a fully connected layer, a linear fully connected layer, a softsign layer, or a recurrent layer.\n18. The computerized method of claim 13, wherein the plurality of sensor data is received from a plurality of different types of sensors.\n19. The computerized method of claim 18, wherein the plurality of different types of sensors include one or more of: an inertial measurement unit, a depth sensing camera, a microphone, or an eye imaging camera.\n20. The computerized method of claim 16, wherein the plurality of middle layers or the plurality of head component layers comprises a pooling layer."
}