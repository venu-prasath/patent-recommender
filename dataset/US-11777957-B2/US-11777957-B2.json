{
    "patent_id": "US-11777957-B2",
    "title": "Method for detecting malicious attacks based on deep learning in traffic cyber physical system ",
    "assignee": "Hangzhou Dianzi University",
    "publication_date": "2023-10-03",
    "patent_link": "https://patents.google.com/patent/US11777957B2/en",
    "inventors": [
        "Yuanfang CHEN",
        "Ting Wu",
        "Hengli YUE",
        "Chengnan HU"
    ],
    "classifications": [
        "H04L63/1441",
        "H04L63/1416",
        "G06N20/20",
        "G06N3/04",
        "G06N3/044",
        "G06N3/047",
        "G06N3/084",
        "G06N3/088",
        "G06N5/01",
        "H04L63/1425",
        "H04W12/128"
    ],
    "abstract": "Disclosed is a method for detection a malicious attack based on deep learning in a transportation cyber-physical system (TCPS), comprising: extracting original feature data of a malicious data flow and a normal data flow from a TCPS; cleaning and coding original feature data; selecting key features from the feature data; cleaning and coding the key features to establish a deep learning model; finally, inputing unknown behavior data to be identified into the deep learning model to identify whether the data is malicious data, thereby detecting a malicious attack. The present invention uses a deep learning method to extract and learn the behavior of a program in a TCPS, and detect the malicious attack according to the deep learning result, and effectively identify the malicious attack in the TCPS.",
    "claims": "\n1. A method for detecting a malicious attack based on deep learning in a transportation cyber-physical system (TCPS), comprising:\n1) extraction of features of data flow from a TCPS\ncollecting malicious data flow and normal data flow from a TCPS, organizing the collected malicious data flow and the normal data flow into a general network frame format, removing irrelevant data, extracting features related to malicious attacks, and dividing the features, according to a frame structure, into bytes as original feature data;\n2) cleaning and encoding of the original feature data obtained in step 1\ncleaning the original feature data in step 1 to remove meaningless data and process missing data; encoding the cleaned feature data into classification values using one-hot encoding to construct a feature vector table;\n3) selection of key features from the feature data obtained in step 2\naccording to feature importance in a random forest model, selecting key features that distinguish a malicious attack behavior from a normal behavior; dividing the selected key features into labeled data and unlabeled data to serve as training data of a deep learning model;\n4) learning of the key features obtained in step 3 to establish the deep learning model\ni) performing a pre-training process without supervision of a Restriction Boltzmann Machine (RBM): initializing RBM layers according to the number of layers or neurons of the RBM; inputting the unlabeled data obtained in step 3 into the RBM; individually training the RBM layer by layer without supervision; after fully trained, outputting a trained RBM layer as an input of a next RBM layer, and then training the next RBM layer; repeating the training until all training data are fully learned;\nwherein the training process without supervision is as follows:\nfeature vectors are sampled using Contrastive Divergence with k steps (CD-k) without supervision, and RBM parameters are updated; an RBM training error is evaluated using mean square error (MSE); gradient descent algorithm (SGD) is used to perform multiple iteration until RBM training meets a requirement or a training period ends; and\nii) performing a fine-tuning process using back propagation (BP) algorithm with supervision: inputting an output of a last layer of RBM to a BP fine-tuning network; inputting the labeled data obtained in step 3, and fine-tuning the deep learning model using the BP algorithm, reversely adjusting a weight of the BP fine-tuning network by a feedback mechanism of the BP fine-tuning network until an optimal model is obtained; wherein\neach of hidden layers is calculated using the BP algorithm to obtain an output of each of the hidden layers; an error of the BP algorithm is calculated by taking a Softmax layer as an output layer and Cross-Entropy as a cost function, if there is an error, the error is reversely transmitted from the output layer to an input layer, and a weight of the neurons is adjusted; a total error is repeatedly iterated using gradient descent algorithm (SGD) until the total error meets a requirement or a training period ends;\n5) inputting unknown behavior data to be identified into the deep learning model trained in step 4 to perform feature recognition, finally mapping the output of the deep learning model into an interval of 0-1 using a Softmax classifier of the output layer to obtain various types of probabilities, determining whether a category with a maximum probability is a malicious sample or a normal sample, and if the number of the malicious samples predicted is greater than 1, indicating that the malicious attack exists in the unknown behavior data.\n1) extraction of features of data flow from a TCPS\n2) cleaning and encoding of the original feature data obtained in step 1\n3) selection of key features from the feature data obtained in step 2\n4) learning of the key features obtained in step 3 to establish the deep learning model\n5) inputting unknown behavior data to be identified into the deep learning model trained in step 4 to perform feature recognition, finally mapping the output of the deep learning model into an interval of 0-1 using a Softmax classifier of the output layer to obtain various types of probabilities, determining whether a category with a maximum probability is a malicious sample or a normal sample, and if the number of the malicious samples predicted is greater than 1, indicating that the malicious attack exists in the unknown behavior data.",
    "status": "Active",
    "citations_own": [
        "US20050228666A1",
        "CN101582813A",
        "CN103778432A",
        "CN104732237A",
        "KR101561651B1",
        "CN106453416A",
        "CN106656981A",
        "CN106769048A",
        "CN106911669A",
        "CN107256393A",
        "US20190138731A1",
        "US20190215330A1"
    ],
    "citations_ftf": [],
    "citedby_own": [],
    "citedby_ftf": [
        "CN108718310B",
        "CN108958217A",
        "CN108965340B",
        "CN111209998B",
        "CN110659720A",
        "CN112688901A",
        "CN110912909A",
        "CN111064724B",
        "CN111107082A",
        "CN111144470B",
        "CN111191823B",
        "CN111385145B",
        "CN111427541B",
        "CN111507385B",
        "CN111582440A",
        "CN111585997B",
        "US11424941B2",
        "CN113673635B",
        "CN111723846A",
        "CN111597551A",
        "CN111935127B",
        "CN111935153B",
        "CN112039903B",
        "CN112084185B",
        "CN112235264B",
        "CN112307472A",
        "CN112330632B",
        "CN112417451B",
        "CN112395810A",
        "CN112883373A",
        "CN112989354A",
        "CN112905717A",
        "CN113079158B",
        "CN113206859B",
        "CN113378990B",
        "CN113656798B",
        "CN113572783A",
        "CN113612786B",
        "CN113746813B",
        "CN113660273B",
        "CN113691562A",
        "CN113885330B",
        "CN114070635A",
        "CN114095284B",
        "CN114760098A",
        "CN115189939A",
        "CN115801471B"
    ]
}