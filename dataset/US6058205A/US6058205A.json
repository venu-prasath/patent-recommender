{
    "patent_link": "https://patents.google.com/patent/US6058205A/en",
    "patent_id": "US6058205A",
    "title": "System and method for partitioning the feature space of a classifier in a pattern classification system",
    "abstract": "A system and method are provided which partition the feature space of a classifier by using hyperplanes to construct a binary decision tree or hierarchical data structure for obtaining the class probabilities for a particular feature vector. One objective in the construction of the decision tree is to minimize the average entropy of the empirical class distributions at each successive node or subset, such that the average entropy of the class distributions at the terminal nodes is minimized. First, a linear discriminant vector is computed that maximally separates the classes at any particular node. A threshold is then chosen that can be applied on the value of the projection onto the hyperplane such that all feature vectors that have a projection onto the hyperplane that is less than the threshold are assigned to a child node (say, left child node) and the feature vectors that have a projection greater than or equal to the threshold are assigned to a right child node. The above two steps are then repeated for each child node until the data at a node falls below a predetermined threshold and the node is classified as a terminal node (leaf of the decision tree). After all non-terminal nodes have been processed, the final step is to store a class distribution associated with each terminal node. The class probabilities for a particular feature vector can then be obtained by traversing the decision tree in a top-down fashion until a terminal node is identified which corresponds to the particular feature vector. The information provided by the decision tree is that, in computing the class probabilities for the particular feature vector, only the small number of classes associated with that particular terminal node need be considered. Alternatively, the required class probabilities can be obtained simply by taking the stored distribution of the terminal node associated with the particular feature vector.",
    "inventors": [
        "Lalit Rai Bahl",
        "Peter Vincent deSouza",
        "David Nahamoo",
        "Mukund Padmanabhan"
    ],
    "assignee": "International Business Machines Corp",
    "classifications": [],
    "claims": "\n1. A system of pattern recognition using feature vectors representing corresponding portions of a physical pattern, said system comprising:\nmeans for assigning a class to every training feature vector within a set of training feature vectors;\nmeans for nonlinearly mapping the set of feature vectors to a higher dimensional space;\nmeans for separating the classes by computing a projection which maximally separates the classes in the higher dimensional space and projecting each of the training feature vectors;\nmeans for assigning the training feature vectors having projections less than a first threshold to a first memory segment and remaining training feature vectors into a second memory segment;\nmeans for storing the hyperplane and the first threshold within the first and second memory segments;\nmeans for separating the classes within the first and/or second memory segments if the classes are associated with a number of training feature vectors greater than a second threshold;\nmeans for designating in a memory the first and/or second memory segments as terminal memory segments if said first and/or second memory segments contain classes associated with a number of training feature vectors less than the second threshold;\nmeans for storing information about the classes associated with the number of training feature vectors less than the second threshold within corresponding terminal memory segments; and\nmeans for recognizing a portion of the physical pattern by retrieving stored information from one or more terminal memory segments corresponding to one or more feature vectors representing said portion.\n2. The system as in claim 1, wherein the higher dimensional space is determined by applying linear discriminant analysis on the training feature vectors to determine at least one discriminant vector having a correspondence eigenvalue, where the at least one discriminant vector having the largest eigenvalue provides the maximum amount of separation between the classes and specifies the higher dimensional space.\n3. The system as in claim 1, wherein the means for separating the classes obtains an empirical distribution of the classes and calculates the empirical distribution's entropy value, H, using the formula: ##EQU3##\n4. The system as in claim 3, wherein the means for assigning the training feature vectors having projections less than a first threshold to a first memory segment and remaining training feature vectors into a second memory segment comprises: means for choosing a value for the first threshold which minimizes the average entropy value of the empirical distribution of the classes stored within the first and/or second memory segments.\n5. The system of claim 1, wherein the means for recognizing a portion of the physical pattern computes a probability distribution of only the classes stored within the one or more terminal memory segments which their stored class distribution has a non-zero probability.\n6. The system of claim 1, wherein the means for recognizing a portion of the physical pattern by retrieving information relating to one or more feature vectors corresponding to said portion comprises:\nmeans for identifying the one or more terminal memory segments from where the stored information is to be retrieved from comprising:\nmeans for projecting the one or more feature vectors onto at least one hyperplane to obtain at least one projection;\nmeans for comparing the at least one projection with at least one threshold;\nmeans for choosing a memory segment if the at least one projection is less than the at least one threshold, else choosing another memory segment;\nmeans for determining whether the chosen memory segment has been designated as a terminal memory segment; and\nmeans for repeating the function of the above elements if the chosen memory segment has not been designated as a terminal memory segment.\n7. A system of pattern recognition using feature vectors representing corresponding portions of a physical pattern, comprising:\na first storage unit for storing a plurality of training feature vectors where a class is assigned to each training feature vector;\na processor for nonlinearly mapping the training feature vectors to a higher dimensional space, separating the classes by computing at least one hyperplane and by projecting each of the training feature vectors stored in the first storage unit onto the at least one hyperplane, said processor stores the at least one hyperplane and the training feature vectors having projections less than a first threshold to a first memory segment and the at least one hyperplane and the remaining training feature vectors into a second memory segment, said processor separates the classed within the first and/or second memory segments if the classes correspond to a number of training feature vectors greater than a second threshold, said processor designates the first and/or second memory segments as terminal memory segments if the first and/or second memory segments contain classes corresponding to a number of training feature vectors less than the second threshold, said processor continues to separate and store the at least one hyperplane and the classes within memory segments based upon projecting the training feature vectors assigned to classes not stored in a terminal memory segment until all classes are stored within memory segments designated as terminal memory segments;\na second storage unit for storing the information including a probability distribution about the classes which is stored within the terminal memory segments and the at least one hyperplane and the first threshold corresponding to every memory segment; and\nmeans for recognizing a portion of the physical pattern by retrieving information which represents said portion of the physical pattern from the second storage unit.\n8. The system as in claim 7, wherein the hyperplane is determined by applying linear discriminant analysis on the training feature vectors to determine at least one discriminant vector having a corresponding eigenvalue, where the at least one discriminant vector having the largest eigenvalue provides the maximum amount of separation between the classes and specifies the hyperplane.\n9. The system of claim 7, wherein the means for recognizing a portion of the physical pattern computes a probability distribution of only the classes stored within the one or more terminal memory segments which in a class distribution have a non-zero probability.\n10. The system of claim 7, wherein the means for recognizing a portion of the physical pattern by retrieving information which represents said portion of the physical pattern from the second storage unit comprises:\nmeans for identifying the one or more terminal memory segments from where the stored information is to be retrieved from comprising:\nmeans for projecting the one or more feature vectors onto at least one hyperplane to obtain at least one projection;\nmeans for comparing the at least one projection with at least one threshold;\nmeans for choosing a memory segment if the at least one projection is less than the at least one threshold, else choosing another memory segment;\nmeans for determining whether the chosen memory segment has been designated as a terminal memory segment; and\nmeans for repeating the function of the above elements if the chosen memory segment has not been designated as a terminal memory segment.\n11. A method of pattern recognition by building a decision tree to partition feature vector of a classifier, said method comprising of the steps of:\na. assigning a class to each training feature vector within a set of training feature vectors;\nb. nonlinearly mapping the set of feature vectors to a higher dimensional space;\nc. determining a projection operation that maximally separates the classes in the higher dimensional space, according to a defined measure of separation;\nd. applying the projection on the training feature vectors, and selecting a threshold on the projection;\ne. partitioning the training feature vectors based on whether the respective projection is less than the threshold; and\nf. repeating steps (b-e) for each partition with the nonlinear mapping and projection operation based on the training feature vector in the partition.\n12. The method of claim 11, wherein the defined measure is the ratio of total covariance to between class covariance, where the covariance matrices are computed on a subset of training feature vector at a current node of the decision tree.\n13. The method of claim 11, wherein the step of selecting the threshold includes: i) dividing range of values of the projected feature vectors into a fixed number of intervals and for each value of the interval, partitioning the training feature vectors based on whether the projection of a training feature vector is greater than or less than the interval; and ii) evaluating each partitioning by measuring the reduction in entropy of a class distribution after partitioning.\n14. The method of claim 11, wherein the step of selecting the threshold is by finding the mean value of the projections of the feature vectors.\n15. The method of claim 11, wherein a probability distribution over the set of classes is estimated for each node of the decision tree including non-terminal nodes.\n16. The method of claim 11 wherein for a new feature vector, the decision tree is traversed and a distribution over the classes for the new feature vector is obtained as a linear combination of the distributions at all intermediate nodes that are visited in the process of traversing the decision tree.",
    "status": "Expired - Fee Related",
    "citations_own": [
        "US5522011A",
        "US5680509A"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US6243695B1",
        "US6253179B1",
        "US6347297B1",
        "GB2369697A",
        "US6418412B1",
        "US20020120754A1",
        "US20020174095A1",
        "US6513004B1",
        "US20030045953A1",
        "US6532305B1",
        "US6562077B2",
        "US20030097263A1",
        "US20030158853A1",
        "US20030192765A1",
        "US20030225763A1",
        "US20040044528A1",
        "US20040158461A1",
        "US20050100209A1",
        "US20050105795A1",
        "US6938025B1",
        "US20050278363A1",
        "US6985950B1",
        "US20060242610A1",
        "US20070245400A1",
        "US20080077404A1",
        "US20080147852A1",
        "US20080181308A1",
        "US20080195654A1",
        "US20080303942A1",
        "US20110025710A1",
        "US20110064136A1",
        "US20110081082A1",
        "US20110188715A1",
        "US8364673B2",
        "US8671069B2",
        "US20160123904A1",
        "US10303985B2",
        "US20210294840A1",
        "US11215711B2",
        "US11710309B2"
    ],
    "citedby_ftf": []
}