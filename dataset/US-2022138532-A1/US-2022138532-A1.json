{
    "patent_id": "US-2022138532-A1",
    "title": "Interpretable neural network ",
    "assignee": "UMNAI Limited",
    "publication_date": "2022-05-05",
    "patent_link": "https://patents.google.com/patent/US20220138532A1/en",
    "inventors": [
        "Angelo Dalli",
        "Mauro PIRRONE"
    ],
    "classifications": [
        "G06N3/0427",
        "G06N3/084",
        "G06N3/042",
        "G06N3/045",
        "G06N3/048",
        "G06N3/0481",
        "G06N5/045",
        "G06N5/041",
        "G06N5/046"
    ],
    "abstract": "An exemplary embodiment may provide an interpretable neural network with hierarchical conditions and partitions. A local function f(x) may model the feature attribution within a specific partition. The combination of all the local functions creates a globally interpretable model. Further, INNs may utilize an external process to identify suitable partitions during their initialization and may support training using back-propagation and related techniques.",
    "claims": "\n1. A system for providing an interpretable neural network embodied on a non-transitory computer readable medium, comprising:\nan input;\na feature generation network configured to identify a plurality of features from the input;\none or more relevance estimators, each relevance estimator configured to calculate a coefficient associated with one or more features of the input;\na conditional network configured to evaluate a plurality of rules, each rule comprising at least an IF-condition, wherein each IF-condition activates one or more partitions, each partition comprising one or more features;\na feature attribution layer configured to calculate an attribution value of the features associated with the partitions activated by the conditional network;\nan aggregation layer configured to aggregate a plurality of predictive results for each of the activated partitions; and\nan output layer configured to provide an output comprising an answer and explanation based on the predictive results and activated partitions.\n2. The system for providing an interpretable neural network of claim 1, wherein the relevance estimator and/or the feature generation network are formed from a black-box model.\n3. The system for providing an interpretable neural network of claim 1, wherein the partitions are static or dynamic and discovered through an external partitioning process or through a connected neural network.\n4. The system for providing an interpretable neural network of claim 1, wherein the feature generation network further comprises a transformation network configured to apply one or more transforms to the input vector to identify the features.\n5. The system for providing an interpretable neural network of claim 1, wherein the system comprises one partition, wherein the transformation function and relevance estimator each comprise a deep neural network, and wherein the one partition models non-linear data.\n6. The system for providing an interpretable neural network of claim 1, wherein the aggregation layer is further configured to weight the plurality of predictive results from each of the activated partitions.\n7. The system for providing an interpretable neural network of claim 1, wherein the partitions comprise one or more of linear partitions, Bayesian partitions, curvilinear partitions, continuous partitions, non-continuous segmented partitions, Bezier curve segments, graph-based partitions, hypergraph-based partitions, and simplicial complex partitions.\n8. The system for providing an interpretable neural network of claim 1, wherein the partitions comprise one or more static partitions and one or more dynamic partitions.\n9. The system for providing an interpretable neural network of claim 1, wherein the rules comprise causal logic and one or more of abductive logic, inductive logic, and deductive logic.\n10. The system for providing an interpretable neural network of claim 9, wherein the output is in at least one of a computer-readable programming language and a machine-readable hardware circuit specification.\n11. The system for providing an interpretable neural network of claim 1, wherein one or more of the feature generation network, conditional network, and/or feature attribution layer are implemented on one or more of: a flexible architecture or field-programmable gate array, a static architecture or an application-specific integrated circuit, analogue/digital electronics, discrete components, photo-electronic components, spintronics and neuromorphic architectures, spiking neuromorphic architectures or quantum computing hardware.\n12. The system for providing an interpretable neural network of claim 1, wherein one or more of the feature generation network, conditional network, and/or feature attribution layer are implemented as a spiking network comprising a plurality of spiking neurons.\n13. The system for providing an interpretable neural network of claim 1, further comprising an identify-assess-recommend-resolve framework configured to identify bias, wherein the identify-assess-recommend-resolve framework comprises a goal-plan-action system.\n14. The system for providing an interpretable neural network of claim 1, wherein the output further comprises an explanation structure model, and wherein at least one of the output and the explanation comprises at least one of a human-readable explanation and a machine-readable explanation.\n15. The system for providing an interpretable neural network of claim 1, further comprising at least one data privacy subsystem, the data privacy configured to perform at least one of:\na differential privacy solution comprising introducing, to the input, prior to the input being supplied to the feature generation network for identification of the plurality of features from the input, data noise based on a noise level;\na secure multi-party computation solution comprising performing secure multi-party computation of at least one function of the system;\na federated learning solution comprising retaining, in a plurality of distributed locations, only a portion of data samples provided in the input, said plurality of distributed locations comprising at least a first distributed location having a first data sample and not a second data sample and a second distributed location having the second data sample and not the first data sample; and\nan encryption solution comprising introducing, to the input, prior to the input being supplied to the feature generation network for identification of the plurality of features from the input, homomorphic encryption, and wherein the feature generation network is configured to identify the plurality of features from a homomorphically-encrypted input.\n16. The system for providing an interpretable neural network of claim 1, wherein the rules are in one or more of: disjunctive normal form, conjunctive normal form, first-order logic assertions, non-Boolean logical systems, Type 1 or Type 2 fuzzy logic systems, modal logic, quantum logic, and probabilistic logic.\n17. The system for providing an interpretable neural network of claim 1, wherein the rules further comprise neuro-symbolic constraints comprising one or more of symbolic expressions, polynomial expressions, conditional and non-conditional probability distributions, joint probability distributions, state-space and phase-space transforms, integer/real/complex/quaternion/octonion transforms, Fourier transforms, Walsh functions, Haar and non-Haar wavelets, generalized L2 functions, fractal-based transforms, Hadamard transforms, Type 1 and Type 2 fuzzy logic and difference analysis.\n18. The system for providing an interpretable neural network of claim 17, wherein the neuro-symbolic constraints are linked with past data comprising at least one of: a previous historic rate of activations and a set of dynamically-changing Fast Weights.\n19. A method for providing an interpretable neural network, comprising:\ninputting a set of training data to a black-box predictor model;\nrecording an output of the black-box predictor model corresponding to the set of training data;\naggregating the output and forming one or more hierarchical partitions based on the aggregated output;\napplying at least one linear or non-linear transformation to the partitions to form one or more local models;\nconstructing rules based on the local models; and\naggregating the rules to from a global interpretable model.\n20. The method for providing an interpretable neural network of claim 19, further comprising monitoring for one or more constraints and expressions, wherein the constraints and expressions comprise one or more conditions, events, triggers and actions in the form of one or more of symbolic rules or system of symbolic expressions, polynomial expressions, conditional and non-conditional probability distributions, joint probability distributions, state-space and phase-space transforms, integer/real/complex/quaternion/octonion transforms, Fourier transforms, Walsh functions, Haar and non-Haar wavelets, generalized L2 functions, fractal-based transforms, Hadamard transforms, Type 1 and Type 2 fuzzy logic and difference analysis.\n21. The method for providing an interpretable neural network of claim 20, wherein the monitoring is implemented by a data structure that references one or more features and one or more associated taxonomies, ontologies, causal models, one or more knowledge graph networks, control charts, Nelson rules, Bode plots, or Nyquist plots.\n22. The method for providing an interpretable neural network of claim 19, further comprising inputting the output into an explainable auto encoder or explainable auto decoder, and generating explanations using a generative adversarial network.\n23. The method for providing an interpretable neural network of claim 19, further comprising training global interpretable model using one or more of transfer learning, Genetic Algorithms, Monte Carlo Simulation Techniques, and Bayesian networks.\n24. The method for providing an interpretable neural network of claim 19, further comprising converting global interpretable model to an explainable neural network.\n25. The method for providing an interpretable neural network of claim 19, further comprising detecting bias in the input and/or the global interpretable model or individually detecting bias in one or more partitions.\n26. The method for providing an interpretable neural network of claim 19, further comprising extracting high-level concepts from the input and linking the concepts to a causal model, wherein the causal model is incorporated into the global interpretable model, and wherein the output further comprises causal explanations in a what-if, what-if-not, and but-for forms.\n27. The method for providing an interpretable neural network of claim 19, further comprising abstracting the explanation based on an abstraction transformation function.\n28. The method for providing an interpretable neural network of claim 19, further comprising injecting human knowledge into the rules in a universal representation format wherein the human knowledge is fixed and cannot be updated.\n29. The method for providing an interpretable neural network of claim 19, forming a heatmap, feature attribution graph, or textual explanation based on the attribution values identified by the feature attribution layer.\n30. The system for providing an interpretable neural network of claim 1, further comprising at least one of: an explainable transformer-transducer, a long short-term memory unit, and a gated recurrent unit, configured to provide a recursive output and a recursive explanation."
}