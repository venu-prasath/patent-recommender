{
    "patent_id": "US-2020320435-A1",
    "title": "Multi-level introspection framework for explainable reinforcement learning agents ",
    "assignee": "Sri International",
    "publication_date": "2020-10-08",
    "patent_link": "https://patents.google.com/patent/US20200320435A1/en",
    "inventors": [
        "Pedro Daniel Barbosa Sequeira",
        "Melinda T. Gervasio",
        "Chih-Hung Yeh"
    ],
    "classifications": [
        "G06N3/006",
        "G06N20/00",
        "G06N5/04",
        "G06N3/088",
        "G06N7/01"
    ],
    "abstract": "Techniques are disclosed for applying a multi-level introspection framework to interaction data characterizing a history of interaction of a reinforcement learning agent with an environment. The framework may apply statistical analysis and machine learning methods to interaction data collected during the RL agent's interaction with the environment. The framework may include a first (\u201cenvironment\u201d) level that analyzes characteristics of one or more tasks to be solved by the RL agent to generate elements, a second (\u201cinteraction\u201d) level that analyzes actions of the RL agent when interacting with the environment to generate elements, and a third (\u201cmeta-analysis\u201d) level that generates elements by analyzing combinations of elements generated by the first level and elements generated by the second level.",
    "claims": "\n1. A computing system comprising:\na computation engine comprising processing circuitry,\nwherein the computation engine is configured to obtain interaction data generated by a reinforcement learning agent, the interaction data characterizing one or more tasks in an environment and characterizing one or more interactions of the reinforcement learning agent with the environment, the one or more interactions performed according to trained policies for the reinforcement learning agent,\nwherein the computation engine is configured to process the interaction data to apply a first analysis function to the one or more tasks to generate first elements,\nwherein the computation engine is configured to process the interaction data to apply a second analysis function to the one or more interactions to generate second elements, the first analysis function different than the second analysis function,\nwherein the computation engine is configured to process at least one of the first elements and the second elements to generate third elements denoting one or more characteristics of the one or more interactions, and\nwherein the computation engine is configured to output an indication of the third elements to a user to provide an explanation of the one or more interactions of the reinforcement learning agent with the environment.\n2. The computing system of claim 1,\nwherein the computation engine is configured to output a request for a decision for one or more actions to perform by the reinforcement learning agent within the environment,\nwherein the computation engine is configured to receive, from the user, decision data indicating a decision of the user responsive to the request for the decision, and\nwherein the computation engine is configured to process the decision data to modify the trained policies for the reinforcement learning agent, retrain the reinforcement learning agent, or provide control to the reinforcement learning agent.\n3. The computing system of claim 1,\nwherein the computation engine is configured to execute the reinforcement learning agent to perform the one or more tasks to generate the trained policies.\n4. The computing system of claim 1,\nwherein the first analysis function comprises a transition analysis function,\nwherein to generate the first elements, the computation engine applies the transition analysis function to the one or more interactions to identify a reinforcement learning agent transition having a certainty level that meets a threshold, and\nwherein the first elements comprise an indication of the identified reinforcement learning agent transition.\n5. The computing system of claim 1,\nwherein the first analysis function comprises a reward analysis function,\nwherein to generate the first elements, the computation engine applies the reward analysis function to rewards of the one or more interactions to identify an interaction having a reward value that meets a distribution threshold, and\nwherein the first elements comprise an indication of the identified interaction.\n6. The computing system of claim 1,\nwherein the second analysis function comprises an interaction analysis function,\nwherein the second elements comprise at least one of observation frequency data, outlier data, or certainty data.\n7. The computing system of claim 1,\nwherein the first analysis function is a function included in an environmental analysis level of a multi-level introspection framework,\nwherein the second analysis function is a function included in an interaction analysis level of the multi-level introspection framework, and\nwherein to process the first elements and the second elements the computation engine is configured to apply a meta-analysis function of a meta-analysis level of the multi-level introspection framework.\n8. The computing system of claim 1,\nwherein the first analysis function comprises a value function,\nwherein the first elements comprise respective values indicating expected respective rewards for one or more state of the environment,\nwherein the second analysis function comprises a transition probability function,\nwherein the second elements comprise transition probability values each indicating a probability of a transition to a new state of the environment given a state of the environment and an action,\nwherein to process the first elements and the second elements the computation engine is configured to compute at least one of local minima or maxima, absolute minima or maxima, observation variance outliers, or strict-difference variance outliers based on the values and the transition probability values.\n9. The computing system of claim 1,\nwherein the interaction data comprises counter data indicating respective numbers for at least: one or more states of the environment interacted with by the reinforcement learning agent, one or more actions performed for states of the environment, or one or more transitions of the reinforcement learning agent within the environment,\nwherein the first analysis function comprises a value function,\nwherein the first elements comprise respective values indicating expected respective rewards for one or more state of the environment,\nwherein the second analysis function comprises a transition probability function,\nwherein the second elements comprise transition probability values each indicating a probability of a transition to a new state of the environment given a state of the environment and an action,\nwherein to process the first elements and the second elements the computation engine is configured to:\ncompute local maxima based on the values;\ngenerate a transition graph based on the transition probability values; and\nprocess the transition graph to identify most likely sequences of transitions of the reinforcement learning agent within the environment,\nwherein the third elements comprise the most likely sequences.\n10. The computing system of claim 1,\nwherein the first analysis function comprises a reward analysis function,\nwherein to generate the first elements, the computation engine applies the reward analysis function to rewards of the one or more interactions to identify an interaction having a reward value that meets a distribution threshold, and\nwherein the first elements comprise an indication of the identified interaction,\nwherein the second analysis function comprises a value analysis function,\nwherein the interaction data comprises at least one: value data for one or more actions performed for states of the environment, or prediction error for one or more actions performed for the one or more states of the environment, and\nwherein to generate the second elements, the computation engine applies the value analysis function to at least one of the value data or prediction error to generate outlier data,\nwherein the second elements comprise the outlier data,\nwherein the interaction data comprises counter data indicating respective numbers for at least: one or more states of the environment interacted with by the reinforcement learning agent, one or more actions performed for states of the environment, or one or more transitions of the reinforcement learning agent within the environment,\nwherein to process the first elements and the second elements the computation engine is configured to identify contradiction data comprising at least one of contradictory-value observations, contradictory-count observations, or contradictory-goal observations, and\nwherein the third elements comprise the contradiction data.\n11. The computing system of claim 1, wherein to output the indication of the third elements the computation engine is configured to:\ncompute, based on the third elements, summary data for a plurality of analysis functions, the summary data comprising one or more of: a maxima state, a minima state, a state-action pair with associated certainty, a state with associated frequency value, a most likely sequence from a minima state to a maxima state, or a most likely sequence from a maxima state to a minima state; and\noutput, to a display device, the summary data.\n12. The computing system of claim 1,\nwherein the computation engine is configured to generate, based on the third elements, one or more training scenarios.\n13. The computing system of claim 1, wherein the interaction data is for one of:\nan autonomous vehicle,\na conversational assistant,\na medical system,\na network automation system,\na home automation system, or\nan industrial control system.\n14. The computing system of claim 1,\nwherein the computation engine is configured to receive a query for a most likely sequence for the reinforcement learning agent,\nwherein the third elements comprise the most likely sequence.\n15. A method of explainable reinforcement learning, the method comprising:\nobtaining, by a computing system, interaction data generated by a reinforcement learning agent, the interaction data characterizing one or more tasks in an environment and characterizing one or more interactions of the reinforcement learning agent with the environment, the one or more interactions performed according to trained policies for the reinforcement learning agent;\nprocessing, by the computing system, the interaction data to apply a first analysis function to the one or more tasks to generate first elements;\nprocessing, by the computing system, the interaction data to apply a second analysis function to the one or more interactions to generate second elements, the first analysis different than the second analysis;\nprocessing, by the computing system, at least one of the first elements and the second elements to generate third elements denoting one or more characteristics of the one or more interactions; and\noutputting, by the computing system, an indication of the third elements to a user to provide an explanation of the one or more interactions of the reinforcement learning agent.\n16. The method of claim 15,\nwherein the first analysis comprises one of a transition analysis function or a reward analysis function.\n17. The method of claim 15,\nwherein the second analysis function comprises an interaction analysis function.\n18. The method of claim 15,\nwherein the second analysis function comprise one of an observation frequency analysis function, an observation-action frequency analysis function, or a value analysis function, and\nwherein the second elements comprise at least one of observation frequency data, outlier data, or certainty data.\n19. The method of claim 15,\nwherein processing the first elements and the second elements comprises applying a meta-analysis function.\n20. A non-transitory computer-readable medium comprising instructions for causing one or more programmable processors to:\nobtain interaction data generated by a reinforcement learning agent, the interaction data characterizing one or more tasks in an environment and characterizing one or more interactions of the reinforcement learning agent with the environment, the one or more interactions performed according to trained policies for the reinforcement learning agent;\nprocess the interaction data to apply a first analysis function to the one or more tasks to generate first elements;\nprocess the interaction data to apply a second analysis function to the one or more interactions to generate second elements, the first analysis different than the second analysis;\nprocess at least one of the first elements and the second elements to generate third elements denoting one or more characteristics of the one or more interactions; and\noutput an indication of the third elements to a user to provide an explanation of the one or more interactions of the reinforcement learning agent."
}