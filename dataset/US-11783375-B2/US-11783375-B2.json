{
    "patent_id": "US-11783375-B2",
    "title": "Customer journey management engine ",
    "assignee": "Cerebri AI Inc.",
    "publication_date": "2023-10-10",
    "patent_link": "https://patents.google.com/patent/US11783375B2/en",
    "inventors": [
        "Alain Charles Briancon",
        "Jean Joseph Belanger",
        "James Cvetan Stojanov",
        "Christopher Michael Coovrey",
        "Pranav Mahesh Makhijani",
        "Gregory Klose",
        "Max Changchun Huang",
        "Mounib Mohamad Ismail",
        "Michael Henry Engeling",
        "Hongshi Li"
    ],
    "classifications": [
        "G06Q30/0269",
        "G06F18/2155",
        "G06F18/24",
        "G06F18/295",
        "G06N20/20",
        "G06N3/006",
        "G06N3/044",
        "G06N3/088",
        "G06N5/01",
        "G06N5/022",
        "G06N7/01"
    ],
    "abstract": "Provided is a process, including: obtaining a first training dataset, training a first machine-learning model on the first training dataset, obtaining a set of candidate question sequences, forming virtual subject-entity records, forming a second training dataset, training a second machine-learning model, and storing the adjusted parameters of the second machine-learning model in memory.",
    "claims": "\n1. A tangible, non-transitory, machine-readable medium storing instructions that when executed by one or more processors effectuate operations comprising:\nobtaining, with one or more processors, a first training dataset, wherein:\nthe first training dataset comprises a plurality of subject-entity records, the subject-entity records each describe a different subject entity;\neach subject entity is a different member of a first population of entities that have interacted over time with an actor entity;\neach subject-entity record describes attributes of a respective subject entity among the first population;\neach subject-entity record describes a time-series of events involving a respective subject entity among the first population;\nthe events are distinct from the attributes;\nat least some of the events are question events that are caused by the actor entity; and\nat least some of the events are subject responses that are caused by a respective subject entity among the first population;\ntraining, with one or more processors, a first machine-learning model on the first training dataset by adjusting parameters of the first machine-learning model to optimize a first objective function that indicates an accuracy of the first machine-learning model in predicting subsequent events in the time-series given prior events in the time-series and given attributes of subject entities among the first population;\nobtaining, with one or more processors, a set of candidate question sequences including candidate question events, the set including a plurality of different candidate question sequences, wherein the actor entity asks at least some of the different candidate question events;\nforming, with one or more processors, virtual subject-entity records by appending the set of candidate question sequences to time-series of at least some of the subject-entity records, wherein:\na given subset of the virtual subject-entity records includes a plurality of virtual-subject entity records that each include at least part of the time-series from the same subject-entity record in the first training dataset, wherein the time-series from the same subject entity record comprises at least some questions and corresponding response events of the subject entity; and\nat least some of the plurality of virtual-subject entity records in the given subset each have a different member of the set of candidate question sequences appended to the at least part of the time-series from the same subject-entity record in the first training dataset;\nforming, with one or more processors, a second training dataset by:\npredicting responses of the subject entities to at least some of the appended set of candidate question sequences with the first machine-learning model based on the virtual subject-entity records; and\nassociating subject entities or attributes thereof with corresponding predicted responses in the second training dataset;\ntraining, with one or more processors, a second machine-learning model on the second training dataset by adjusting parameters of the second machine-learning model to optimize a second objective function that indicates an accuracy of the second machine-learning model in predicting the predicted responses in the second training set given attributes of subject entities corresponding to the predicted responses; and\nstoring, with one or more processors, the adjusted parameters of the second machine-learning model in memory.\n2. The medium of claim 1, wherein:\nat least some of subject-entity records of the plurality of subject-entity records have interacted within a time range with the actor entity.\n3. The medium of claim 2, wherein the time range is a trailing time range.\n4. The medium of claim 1, wherein:\nat least some of subject-entity records of the plurality of subject-entity records have interacted within a geolocation range with the actor entity.\n5. The medium of claim 1, wherein:\nthe first machine-learning model is configured to predict responses of the plurality of subject-entity records given previous time-series of events and attributes of the plurality of subject-entity records.\n6. The medium of claim 1, wherein the operation comprise:\niterating training of a plurality of models to increase the accuracy of the model across each iteration responsive to next questions and responses.\n7. The medium of claim 1, wherein:\nthe first machine learning model comprises a Hidden Markov model.\n8. The medium of claim 1, wherein:\nthe first machine learning model comprises a long short-term memory model.\n9. The medium of claim 1, wherein:\nthe first machine learning model comprises a dynamic Bayesian network.\n10. The medium of claim 1, wherein:\nthe first machine learning model comprises a neural network classifier.\n11. The medium of claim 1, wherein:\nthe second machine learning model is an unsupervised model configured to translate inputs into a vector representation that maps to a candidate action.\n12. The medium of claim 1, wherein:\nthe second machine learning model is a random decision forest model that includes a plurality of weighted trained decision trees.\n13. The medium of claim 1, wherein:\nthe second machine learning model is a gradient-boosted trees model that includes a plurality of weighted trained decision trees.\n14. The medium of claim 1, wherein:\nthe events are stored in an ontology of event types that describes interrelatedness or similarity between the events.\n15. The medium of claim 1, wherein the question events comprise:\nan interactive user interface element for which a response within the interactive user interface element is collected, which may be a selection or other user input;\nan advertisement, for which a response may be the interaction with a particular portion of the advertisement; and\nan article about a product, for which a response may be the interaction with a particular portion of the article.\n16. The medium of claim 1, wherein at least some of the events are subject responses that are caused by a respective subject entity among the first population.\n17. The medium of claim 16, wherein the subject responses comprise:\na canceled response;\na delayed response;\na direct response; and\nan indirect response.\n18. The medium of claim 1, wherein at least some of the plurality of virtual-subject entity records in the given subset each have a different member of the set of candidate question sequences appended to the at least part of the time-series from the same subject-entity record in the first training dataset.",
    "status": "Active",
    "citations_own": [
        "US6430539B1",
        "US20070219978A1",
        "US20110054960A1",
        "US20110082824A1",
        "US8370280B1",
        "US20140222506A1",
        "US9015084B2",
        "US20150134413A1",
        "US20150356591A1",
        "US20160048766A1",
        "US20160170997A1",
        "US9569729B1",
        "WO2018075995A1",
        "US20180233130A1",
        "US20180330258A1",
        "US20190020670A1",
        "US20190080347A1",
        "US20190138643A1",
        "US10366346B2",
        "US10373177B2",
        "US10496927B2"
    ],
    "citations_ftf": [
        "US7865427B2",
        "US7584134B2",
        "US7693766B2",
        "US8069101B1",
        "US20070124767A1",
        "US8032480B2",
        "US10410220B2",
        "US20160086222A1",
        "US20100223074A1",
        "US20110246251A1",
        "US8768770B2",
        "US8543454B2",
        "US20120284080A1",
        "US8949164B1",
        "US20130138502A1",
        "US9047558B2",
        "WO2014075108A2",
        "US20140244351A1",
        "US20140358828A1",
        "US20150161529A1",
        "US20150248651A1",
        "US10489707B2",
        "US9886670B2",
        "US20160104091A1",
        "US20160275546A1",
        "US10062034B2",
        "US20170083937A1",
        "US9674362B2",
        "EP3374932B1",
        "US20170227673A1",
        "CN114625076A",
        "US10783535B2",
        "EP3520319B1",
        "US20180129970A1",
        "US10956821B2",
        "US20180165604A1",
        "US11087229B2"
    ],
    "citedby_own": [],
    "citedby_ftf": [
        "US8312033B1",
        "US10102570B1",
        "US10140578B1",
        "US10783535B2",
        "US11615285B2",
        "US11755949B2",
        "US10977149B1",
        "US11663478B2",
        "US11315179B1",
        "JP2020087023A",
        "US20200242250A1",
        "US11003947B2",
        "US11569978B2",
        "US11652603B2",
        "US11657162B2",
        "WO2020227434A1",
        "US11455645B2",
        "US11727265B2",
        "US20210005207A1",
        "US11328205B2",
        "US20210117882A1",
        "US20210136220A1",
        "US11232483B2",
        "US20210174403A1",
        "US11651210B2",
        "US11580401B2",
        "US11797827B2",
        "US11790302B2",
        "US11736615B2",
        "US11392969B2",
        "US20210342847A1",
        "US20210365279A1",
        "US11625633B2",
        "CN111666396B",
        "US20210406935A1",
        "US11611588B2",
        "US11706632B1",
        "CN112052891A",
        "US11762928B2",
        "US11507721B2",
        "US11645664B2",
        "US20220139498A1",
        "US11775984B1",
        "US11677875B2",
        "CN113688217B",
        "US20220191003A1",
        "CN114461787B",
        "US20230342351A1",
        "US11736616B1"
    ]
}