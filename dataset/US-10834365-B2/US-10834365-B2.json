{
    "patent_id": "US-10834365-B2",
    "title": "Audio-visual monitoring using a virtual assistant ",
    "assignee": "Nortek Security & Control Llc",
    "publication_date": "2020-11-10",
    "patent_link": "https://patents.google.com/patent/US10834365B2/en",
    "inventors": [
        "Krishna Khadloya",
        "Vaidhi Nathan",
        "Chandan Gope"
    ],
    "classifications": [
        "H04N7/183",
        "G06K9/00302",
        "G06K9/00369",
        "G06K9/00771",
        "G06N20/00",
        "G06N3/006",
        "G06N3/042",
        "G06V10/764",
        "G06V20/10",
        "G06V20/52",
        "G06V20/70",
        "G06V40/10",
        "G06V40/103",
        "G06V40/174",
        "G10L17/00",
        "G10L17/005",
        "G06N3/045",
        "G10L25/51"
    ],
    "abstract": "A function of a user-controlled virtual assistant (UCVA) device, such as a smart speaker, can be augmented using video or image information about an environment. In an example, a system for augmenting an UCVA device includes an image sensor configured to monitor an environment, a processor circuit configured to receive image information from the image sensor and use artificial intelligence to discern a presence of one or more known individuals in the environment from one or more other features in the environment. The system can include an interface coupled to the processor circuit and configured to provide identification information to the UCVA device about the one or more known human beings in the environment. The UCVA device can be configured by the identification information to update an operating mode of the UCVA device.",
    "claims": "\n1. A system for augmenting one or more functions of a user-controlled virtual assistant (UCVA) device, the system comprising:\na first device, other than the UCVA device, including:\nan image sensor configured to monitor an environment;\na processor circuit configured to receive image information from the image sensor and discern a presence of one or more known human beings in the environment from one or more other features in the environment; and\nan interface coupled to the processor circuit and to the UCVA, wherein the interface is configured to provide identification information to the UCVA device about the one or more known human beings in the environment;\nwherein the UCVA device is configured to use the identification information to change an operating mode of the UCVA device from an environment monitoring mode to an assistant mode.\n2. The system of claim 1, wherein the interface is a hardware layer interface that couples the processor circuit to a different processor circuit in the UCVA device.\n3. The system of claim 1, wherein the interface is a software layer interface that communicates information from the processor circuit to a different processor circuit in the UCVA device.\n4. The system of claim 1, wherein the UCVA device is configured by the identification information to personalize an interaction between the UCVA device and the one or more known human beings in the environment.\n5. The system of claim 1, wherein the processor circuit is configured to apply machine learning to the image information from the image sensor to discern the presence of the one or more known human beings.\n6. The system of claim 5, wherein the processor circuit is configured to use a neural network, configured with a human classification model, to process the image information from the image sensor and discern the presence of the one or more known human beings.\n7. The system of claim 1, further comprising the UCVA device, wherein the UCVA device is a smart speaker device.\n8. The system of claim 7, wherein when the UCVA is configured to operate in the environment monitoring mode, the UCVA is configured to use a microphone to receive audio information about the environment and to use a different second processor circuit to classify the received audio information about the environment, wherein the different second processor circuit is configured to apply deep learning to classify the received audio information about the environment as including one or more of a dog bark, a glass break or other material break, a gun shot, human speech, or an environment alarm.\n9. The system of claim 7, wherein when the UCVA is configured to operate in the environment monitoring mode, the UCVA and/or the image sensor are configured to monitor the environment to identify whether a specified portion of the environment includes a moving object, the specified portion of the environment comprising less than all of the environment.\n10. The system of claim 7, wherein when the UCVA is configured to operate in the assistant mode, the UCVA is configured to:\npersonalize a greeting for the one or more known human beings in the environment; and\npersonalize a calendaring event for the one or more known human beings in the environment; and\noffer a personalized response to a user inquiry when the user is one of the known human beings, wherein the personalized response is based on a contact database that is associated with the user or is based on an enterprise application feature accessible by the user but not accessible by other unauthorized users.\n11. An audio-video personal assistant device comprising:\nan image sensor configured to provide image information about an environment;\nan audio sensor configured to provide audio information about the environment;\na processor circuit configured to receive the image information from the image sensor and to receive the audio information from the audio sensor;\na non-transitory memory circuit coupled to the processor circuit, the non-transitory, memory circuit comprising instructions that, when performed by the processor circuit, configure the processor circuit to:\nanalyze one of the image information or the audio information to identify whether a known individual is present in the environment;\nuse the other one of the image information and the audio information to confirm that the known individual is present in the environment; and\nperform a personalized task associated with the known individual when the known individual is confirmed to be present in the environment.\n12. The assistant device of claim 11, wherein the instructions further configure the processor circuit to:\nanalyze one of the image information or the audio information to identify multiple individuals present at an event in the environment and provide information about an attendance at the event based on the identified individuals.\n13. The assistant device of claim 12, wherein the instructions further configure the processor circuit to:\nlook up an expected attendance for the event;\ndetermine one or more individuals not present at the event by comparing the expected attendance with the identified multiple individuals present; and\nautomatically send a reminder about the event to the one or more individuals determined to be not present at the event.\n14. The assistant device of claim 11, wherein the instructions further configure the processor circuit to:\nanalyze one of the image information or the audio information to identify multiple individuals present at an event in the environment;\nanalyze one of the image information or the audio information to identify a particular individual, from among the multiple individuals, who is speaking at the event; and\nrecord the image information and/or the audio information when the particular individual is speaking.\n15. The assistant device of claim 11, wherein the instructions to perform a personalized task associated with the known individual comprise instructions to:\npersonalize a greeting for the known individual; or\npersonalize a calendaring event for the known individual; or\noffer a personalized response to an inquiry submitted by the known individual; or\nenable or make available to the known individual an enterprise application feature.\n16. A method for incorporating intelligent video monitoring to an audio assistant wherein the audio assistant is provided in an environment, the method comprising:\nreceiving image information from a camera configured to monitor the environment;\nusing a processor circuit:\nanalyzing the image information to identify one or more individuals present in the environment;\ncomparing the identified one or more individuals with a database of enrolled individuals to determine whether the identified one or more individuals is one of the enrolled individuals;\nwhen the comparison indicates the identified one or more individuals is one of the enrolled individuals, receiving a command from the identified one or more individuals to place the audio assistant in a security monitoring mode;\nanalyzing other later-received image information from the camera to determine whether an unauthorized object or individual is present when the audio assistant is in the security monitoring mode; and\ncommunicating an alert to at least one of the enrolled individuals when an unauthorized object or individual is determined to be present.\n17. The method of claim 16, wherein the communicating the alert includes communicating, to the at least one of the enrolled individuals, video and/or audio information about the unauthorized object or individual.\n18. The method of claim 16, wherein the method includes analyzing the image information to identify a look direction, mood, or facial feature of the one or more individuals present in the environment.\n19. The method of claim 16, wherein the analyzing the image information to identify the one or more individuals includes using a neural network to process the image information.\n20. The method of claim 16, wherein receiving the command includes:\nreceiving a spoken command; and\ndetermining whether the command as-received corresponds to the identified one or more individuals based on auditory characteristics of the command and of the one or more individuals.",
    "status": "Active",
    "citations_own": [
        "US6028626A",
        "US20010047264A1",
        "US6594629B1",
        "US6681032B2",
        "US20050267605A1",
        "US20060190419A1",
        "US7113090B1",
        "US7847820B2",
        "US7999857B2",
        "US20120026328A1",
        "US8139098B2",
        "US20120143363A1",
        "US8237571B2",
        "US8527278B2",
        "US20130237240A1",
        "US20130275138A1",
        "US20140046878A1",
        "US8660249B2",
        "US20140222436A1",
        "CN104346607A",
        "US20150221321A1",
        "US9105053B2",
        "US9135797B2",
        "US9208675B2",
        "US9224044B1",
        "US9230560B2",
        "US9304736B1",
        "US9313312B2",
        "US9479354B2",
        "US20160364963A1",
        "US20160378861A1",
        "CN106372576A",
        "US9740940B2",
        "US20170329466A1",
        "CN107609512A",
        "US10073428B2",
        "CN108540762A",
        "US20180342329A1",
        "US20190043525A1",
        "US20190139565A1",
        "US20190259378A1",
        "US20190377325A1"
    ],
    "citations_ftf": [],
    "citedby_own": [],
    "citedby_ftf": [
        "JP6942472B2",
        "WO2019002831A1",
        "GB201713697D0",
        "GB2563953A",
        "GB201801528D0",
        "GB201801526D0",
        "GB201801530D0",
        "GB201801527D0",
        "US11316865B2",
        "US10546655B2",
        "GB201801663D0",
        "GB201801661D0",
        "GB2567503A",
        "GB201801874D0",
        "GB201803570D0",
        "GB201804843D0",
        "GB201801664D0",
        "GB201801659D0",
        "US10861463B2",
        "US11475899B2",
        "US11264037B2",
        "US11735189B2",
        "WO2019152472A1",
        "US10978050B2",
        "US11250383B2",
        "WO2019173331A1",
        "WO2019173333A1",
        "US10733996B2",
        "US10720166B2",
        "US10818296B2",
        "US10692490B2",
        "US11715302B2",
        "US10915614B2",
        "US11037574B2",
        "CN109065058A",
        "US11238294B2",
        "JP2020136899A",
        "US11216480B2",
        "US11227679B2",
        "US11531807B2",
        "CN110196914B",
        "US11670408B2",
        "EP3806015A1",
        "US11023344B2",
        "US11665013B1",
        "US11687778B2",
        "US20220004949A1",
        "TWI721885B",
        "US20210375278A1",
        "US11222103B1",
        "WO2022196921A1",
        "WO2022204338A1",
        "US20220343543A1"
    ]
}