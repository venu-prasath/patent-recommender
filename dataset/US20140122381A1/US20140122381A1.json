{
    "patent_link": "https://patents.google.com/patent/US20140122381A1/en",
    "patent_id": "US20140122381A1",
    "title": "Decision tree training in machine learning",
    "abstract": "Improved decision tree training in machine learning is described, for example, for automated classification of body organs in medical images or for detection of body joint positions in depth images. In various embodiments, improved estimates of uncertainty are used when training random decision forests for machine learning tasks in order to give improved accuracy of predictions and fewer errors. In examples, bias corrected estimates of entropy or Gini index are used or non-parametric estimates of differential entropy. In examples, resulting trained random decision forests are better able to perform classification or regression tasks for a variety of applications without undue increase in computational load.",
    "inventors": [
        "Reinhard Sebastian Bernhard Nowozin"
    ],
    "assignee": "Microsoft Technology Licensing LLC",
    "classifications": [
        "G06N20/00",
        "G06N20/20"
    ],
    "claims": "\n1. A machine learning device comprising:\na communications interface arranged to receive training data;\na tree training logic arranged to train a random decision forest using the received training data and on the basis of uncertainty measures of at least some of the received training data computed using an uncertainty measurement logic;\nthe uncertainty measurement logic arranged to either correct for bias in the uncertainty measurement or to use a non-parametric estimate of uncertainty.\n2. A machine learning device as claimed in claim 1, the uncertainty measurement logic using a non-parametric estimate of uncertainty which is an estimate of a probability distribution of the training data made without assuming that the probability distribution comes from a specified family of parametric distributions.\n3. A machine learning device as claimed in claim 1, the uncertainty measurement logic arranged to correct for bias introduced by estimation, using a limited amount of training data, of a probability distribution from which the training data is generated.\n4. A machine learning device as claimed in claim 1, the uncertainty measurement logic arranged to compute a bias corrected estimate of a Gini index where a Gini index is a measure of inequality of a multinomial distribution.\n5. A machine learning device as claimed in claim 1, the uncertainty measurement logic arranged to compute a bias corrected estimate of a Gini index by using a resampling method that estimates a mean of a probability distribution of an estimated Gini index of a sample from the training data.\n6. A machine learning device as claimed in claim 1, the uncertainty measurement logic arranged to compute a bias corrected estimate of a Gini index by aggregating a plurality of perturbations of a biased estimate of a Gini index.\n7. A machine learning device as claimed in claim 1, the uncertainty measurement logic arranged to compute a bias corrected estimate of an entropy.\n8. A machine learning device as claimed in claim 1 the uncertainty measurement logic arranged to compute a bias corrected estimate of an entropy using a Grassberger estimate.\n9. A machine learning device as claimed in claim 1, the uncertainty measurement logic using a non-parametric estimate of uncertainty which is an estimate of a probability distribution of the training data made without assuming that the probability distribution comes from a specified family of parametric distributions, and where the non-parametric estimate of uncertainty uses a one-nearest neighbor estimator.\n10. A machine learning device as claimed in claim 1, the uncertainty measurement logic using a non-parametric estimate of uncertainty which is an estimate of a probability distribution of the training data made without assuming that the probability distribution comes from a specified family of parametric distributions, and where the non-parametric estimate of uncertainty uses a one-nearest neighbor estimator computed using k-d trees.\n11. A machine learning device as claimed in claim 1, the uncertainty measurement logic using a non-parametric estimate of uncertainty which is an estimate of a probability distribution of the training data made without assuming that the probability distribution comes from a specified family of parametric distributions, and where the non-parametric estimate of uncertainty uses any of: a kernel density estimate, a length of minimum spanning trees, a k-nearest neighbor distance.\n12. A machine learning device as claimed in claim 1 the uncertainty measurement logic being at least partially implemented using hardware logic selected from any one or more of: a field-programmable gate array, a program-specific integrated circuit, a program-specific standard product, a system-on-a-chip, a complex programmable logic device, a graphics processing unit.\n13. A machine learning method comprising:\nreceiving training data at a communications interface;\ntraining, at a processor, a random decision forest using the received training data and on the basis of a measure of uncertainty of at least some of the received training data;\ncomputing, at the processor, the measure of the uncertainty so as to either correct for bias in the measurement of the uncertainty or to use a non-parametric estimate of the uncertainty.\n14. A method as claimed in claim 13 comprising computing the measure of the uncertainty so as to correct for bias introduced by estimation, using a limited amount of training data, of a probability distribution from which the training data is generated.\n15. A method as claimed in claim 13 comprising computing a bias corrected estimate of a Gini index where a Gini index is a measure of inequality of a multinomial distribution.\n16. A method as claimed in claim 13 comprising computing a bias corrected estimate of an entropy.\n17. A method as claimed in claim 13 at least partially carried out using hardware logic.\n18. A machine learning method comprising:\nreceiving training data at a communications interface the training data comprising examples of data to be classified into one of a plurality of possible classes;\ntraining, at a processor, a random decision forest to classify data into the possible classes, the training carried out using the received training data and on the basis of a measure of uncertainty of at least some of the received training data;\ncomputing, at the processor, the measure of the uncertainty so as to either correct for bias in the measurement of the uncertainty or to use a non-parametric estimate of the uncertainty; and where the number of possible classes is such that it is difficult to estimate empirical class frequencies reliably.\n19. A method as claimed in claim 18 comprising computing the measure of the uncertainty so as to correct for bias introduced by estimation, using a limited amount of training data, of a probability distribution from which the training data is generated.\n20. A method as claimed in claim 18 where the number of possible classes is at least ten.",
    "status": "Expired - Fee Related",
    "citations_own": [
        "US5930392A",
        "US20030195831A1",
        "US20030236789A1",
        "US6748341B2",
        "US20050286772A1",
        "US20070185656A1",
        "US20090083206A1",
        "US20110119212A1",
        "US20110188715A1",
        "US20110228997A1",
        "US20110258049A1",
        "US20120166462A1",
        "US20120239174A1",
        "US20120269407A1",
        "US20120321174A1",
        "US20130041623A1"
    ],
    "citations_ftf": [
        "US7310624B1",
        "US6519580B1",
        "GB2376542B",
        "US7251639B2",
        "US20090226872A1",
        "US8131770B2",
        "AU2010274044B2"
    ],
    "citedby_own": [
        "US20140307959A1",
        "US20150067407A1",
        "US20150272546A1",
        "US20150379426A1",
        "US9380224B2",
        "US20170091670A1",
        "US9672474B2",
        "WO2017119997A1",
        "US9760837B1",
        "WO2018006004A1",
        "US9886670B2",
        "US20180062931A1",
        "US9916524B2",
        "US9983859B2",
        "US10013211B2",
        "US10062201B2",
        "CN108564058A",
        "US10102480B2",
        "US10152648B2",
        "CN109001805A",
        "US10169715B2",
        "US10169828B1",
        "CN109165878A",
        "US10204382B2",
        "CN109408583A",
        "US10257275B1",
        "US10318882B2",
        "US10324971B2",
        "US10346927B1",
        "WO2019139759A1",
        "WO2019139760A1",
        "US10387787B1",
        "US20190311526A1",
        "US10452992B2",
        "CN110706192A",
        "US10540606B2",
        "US10602270B1",
        "US10621677B2",
        "US10621597B2",
        "US20200152315A1",
        "CN111259975A",
        "WO2020117611A1",
        "WO2020131499A1",
        "CN111475146A",
        "WO2020197793A1",
        "US20200372400A1",
        "US10861106B1",
        "WO2020247949A1",
        "US10868738B2",
        "US20210042646A1",
        "US10943309B1",
        "US10963810B2",
        "US10970241B1",
        "WO2021071615A1",
        "US11030631B1",
        "US20210201889A1",
        "US11069001B1",
        "US11100420B2",
        "US20210286809A1",
        "CN113674335A",
        "US11182691B1",
        "US11256991B2",
        "US20220101626A1",
        "US20220164973A1",
        "US11348044B2",
        "US20220374724A1",
        "US11600005B2",
        "US20230098255A1",
        "WO2023108086A1",
        "US20230351172A1"
    ],
    "citedby_ftf": [
        "US11475310B1",
        "US11478212B2",
        "US11017324B2",
        "US11521045B2",
        "TWI715903B",
        "JP7308775B2"
    ]
}