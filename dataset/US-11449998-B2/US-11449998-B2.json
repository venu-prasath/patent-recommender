{
    "patent_id": "US-11449998-B2",
    "title": "Processing of histology images with a convolutional neural network to identify tumors ",
    "assignee": "Leica Biosystems Imaging, Inc.",
    "publication_date": "2022-09-20",
    "patent_link": "https://patents.google.com/patent/US11449998B2/en",
    "inventors": [
        "Walter GEORGESCU",
        "Allen Olson",
        "Bharat ANNALDAS",
        "Darragh LAWLER",
        "Kevin Shields",
        "Kiran Saligrama",
        "Mark Gregson"
    ],
    "classifications": [
        "G16H30/40",
        "G06F18/24133",
        "G06K9/00523",
        "G06K9/00536",
        "G06K9/6271",
        "G06N20/00",
        "G06N3/045",
        "G06N3/08",
        "G06N5/04",
        "G06N7/01",
        "G06T7/0012",
        "G06T7/0014",
        "G06V10/454",
        "G06V10/82",
        "G06V20/698",
        "G06V30/19173",
        "G16H10/60",
        "G16H30/20",
        "G16H50/20",
        "G16H50/70",
        "G06F2218/08",
        "G06F2218/12",
        "G06T2207/10056",
        "G06T2207/20084",
        "G06T2207/30024",
        "G06T2207/30096",
        "G06V2201/03",
        "G06V30/18057"
    ],
    "abstract": "A convolutional neural network (CNN) is applied to identifying tumors in a histological image. The CNN has one channel assigned to each of a plurality of tissue classes that are to be identified, there being at least one class for each of non-tumorous and tumorous tissue types. Multi-stage convolution is performed on image patches extracted from the histological image followed by multi-stage transpose convolution to recover a layer matched in size to the input image patch. The output image patch thus has a one-to-one pixel-to-pixel correspondence with the input image patch such that each pixel in the output image patch has assigned to it one of the multiple available classes. The output image patches are then assembled into a probability map that can be co-rendered with the histological image either alongside it or over it as an overlay. The probability map can then be stored linked to the histological image.",
    "claims": "\n1. A method comprising using at least one hardware processor to, for each of one or more histological images of stained tissue samples:\nreceive the histological image;\nextract a plurality of image patches from the histological image, wherein each of the plurality of image patches comprises a partial region of the histological image;\napply a convolutional neural network (CNN) to the plurality of image patches to:\nclassify each pixel of each of the plurality of image patches into one of a plurality of classes, wherein the plurality of classes comprises a non-tumor class representing an absence of tumor tissue and at least one tumor class representing tumor tissue, and\ngenerate a plurality of output image patches based on classifying each pixel of each of the plurality of image patches into one of the plurality of classes, wherein a first pixel of an output image patch of the plurality of output image patches is classified into a first class of the plurality of classes and a second pixel of the output image patch is classified into a second class of the plurality of classes; and\ngenerate a probability heatmap from the plurality of output image patches, wherein the probability heatmap is overlaid over the histological image, wherein the probability heatmap assigns a color to each pixel in the histological image based on the one class into which that pixel was classified.\n2. The method of claim 1, further comprising using the at least one hardware processor to train the CNN using training data comprising histological images that have been annotated according to the plurality of classes.\n3. The method of claim 2, wherein the training uses gradient descent.\n4. The method of claim 1, wherein the at least one tumor class comprises a plurality of tumor classes, wherein each of the plurality of tumor classes represents a different type of tumor.\n5. The method of claim 4, wherein the plurality of tumor classes comprises a first tumor class representing an invasive tumor and a second tumor class representing an in situ tumor.\n6. The method of claim 1, wherein to classify each pixel of each of the plurality of image patches into one of the plurality of classes, the method further comprises using the at least one hardware processor to, for each pixel:\noutput, by the CNN, a vector of probabilities, wherein each probability in the vector is a probability that the pixel belongs to one of the plurality of classes; and\nselect a class of the plurality of classes into which the pixel is classified based on the vector of probabilities.\n7. The method of claim 1, wherein the CNN comprises a plurality of convolutional layers and a plurality of deconvolutional layers.\n8. The method of claim 7, wherein the CNN comprises a maxpool layer following at least one of the plurality of convolutional layers.\n9. The method of claim 8, wherein the CNN comprises a skip connection between at least one maxpool layer and one of the plurality of deconvolutional layers.\n10. The method of claim 9, wherein, for each of the plurality of image patches, the plurality of deconvolutional layers upscales a convolutional feature map of the image patch to a feature map with a same width and height as the image patch.\n11. The method of claim 10, wherein each of the plurality of deconvolutional layers enlarges a width and height of an input feature map by a factor of two.\n12. The method of claim 7, wherein to apply CNN, the method further comprises using the at least one hardware processor to, for each of the plurality of image patches:\nconcatenate two or more features maps, produced by different ones of the plurality of deconvolutional layers, into a combined feature map; and\nconvert the combined feature map into a probability map using a softmax operation, the probability map comprising a vector of probabilities for each pixel in the image patch, wherein each probability in each vector is a probability that the pixel belongs to one of the plurality of classes.\n13. The method of claim 7, wherein an output of one or more of the plurality of convolutional layers or the plurality of deconvolutional layers is transformed by a rectifier function.\n14. The method of claim 1, wherein none of the plurality of images patches overlap.\n15. The method of claim 1, wherein two or more of the plurality of image patches overlap.\n16. The method of claim 1, wherein to generate the probability heatmap, the method further comprises using the at least one hardware processor to, stitch the plurality of output image patches together, wherein each of the plurality of output image patches represents a probability map for one of the plurality of image patches.\n17. The method of claim 1, wherein the plurality of image patches covers an entirety of the histological image.\n18. The method of claim 1, wherein the plurality of image patches only covers a portion of the histological image.\n19. A system comprising:\nat least one hardware processor; and\none or more software modules that are configured to, when executed by the at least one hardware processor, for each of one or more histological images of stained tissue samples,\nreceive the histological image,\nextract a plurality of image patches from the histological image, wherein each of the plurality of image patches comprises a partial region of the histological image;\napply a convolutional neural network (CNN) to the plurality of image patches to:\nclassify each pixel of each of the plurality of image patches into one of a plurality of classes, wherein the plurality of classes comprises a non-tumor class representing an absence of tumor tissue and at least one tumor class representing tumor tissue, and\ngenerate a plurality of output image patches based on classifying each pixel of each of the plurality of image patches into one of the plurality of classes, wherein a first pixel of an output image patch of the plurality of output image patches is classified into a first class of the plurality of classes and a second pixel of the output image patch is classified into a second class of the plurality of classes; and\ngenerate a probability heatmap from the plurality of output image patches, wherein the probability heatmap is overlaid over the histological image, wherein the probability heatmap assigns a color to each pixel in the histological image based on the one class into which that pixel was classified.\n20. A non-transitory computer-readable medium having instructions stored thereon, wherein the instructions, when executed by a processor, cause the processor to, for each of one or more histological images of stained tissue samples:\nreceive the histological image;\nextract a plurality of image patches from the histological image, wherein each of the plurality of image patches comprises a partial region of the histological image;\napply a convolutional neural network (CNN) to the plurality of image patches to:\nclassify each pixel of each of the plurality of image patches into one of a plurality of classes, wherein the plurality of classes comprises a non-tumor class representing an absence of tumor tissue and at least one tumor class representing tumor tissue, and\ngenerate a plurality of output image patches based on classifying each pixel of each of the plurality of image patches into one of the plurality of classes, wherein a first pixel of an output image patch of the plurality of output image patches is classified into a first class of the plurality of classes and a second pixel of the output image patch is classified into a second class of the plurality of classes; and\ngenerate a probability heatmap from the plurality of output image patches, wherein the probability heatmap is overlaid over the histological image, wherein the probability heatmap assigns a color to each pixel in the histological image based on the one class into which that pixel was classified.",
    "status": "Active",
    "citations_own": [
        "US6819790B2",
        "US20150051484A1",
        "US20150213302A1",
        "US20160063724A1",
        "US20160086326A1",
        "US20160253466A1",
        "CN106030608A",
        "US20160321512A1",
        "US20170124415A1",
        "US9739783B1",
        "US20170372117A1",
        "US20180114317A1",
        "US20180130203A1",
        "US10049450B2",
        "US20180232883A1",
        "US20180263568A1",
        "US20190065817A1",
        "US20190147592A1",
        "US20190183429A1",
        "WO2019133538A2",
        "US10460211B2",
        "US20200372635A1"
    ],
    "citations_ftf": [
        "CN107330263B"
    ],
    "citedby_own": [],
    "citedby_ftf": [
        "US10628683B2",
        "EP3625765A4",
        "KR102174379B1",
        "WO2020046986A1",
        "TWI681406B",
        "US10949649B2",
        "CN110084309B",
        "US11320927B2",
        "US11176696B2",
        "JP7100336B2",
        "US11195060B2",
        "CN112288672A",
        "WO2021030270A1",
        "CN110807764A",
        "JP2021056571A",
        "US11367189B2",
        "US11341635B2",
        "CN112905132B",
        "CN110827275A",
        "CN111126187A",
        "EP4078513A1",
        "CN111383328B",
        "WO2021188617A1",
        "EP4147247A1",
        "US11379697B2",
        "US11790533B2",
        "EP3975110A1",
        "CN112233151B",
        "TWI762055B",
        "CN112634208B",
        "RU2764705C1",
        "CN112580748B",
        "CN112801939B",
        "CN112950587B",
        "US11482319B2",
        "WO2022197917A1",
        "CN113077440A",
        "CN113205111B",
        "CN113035334B",
        "TWI790689B",
        "CN113946538A"
    ]
}