{
    "patent_id": "US-2019339087-A1",
    "title": "Deep reinforcement learning for optimizing carpooling policies ",
    "assignee": "Didi Research America, Llc",
    "publication_date": "2019-11-07",
    "patent_link": "https://patents.google.com/patent/US20190339087A1/en",
    "inventors": [
        "Ishan JINDAL",
        "Zhiwei Qin",
        "Xuewen Chen",
        "Matthew NOKLEBY",
        "Jieping Ye"
    ],
    "classifications": [
        "G01C21/3453",
        "G06Q50/30",
        "G01C21/3438",
        "G06F15/18",
        "G06F17/17",
        "G06N20/00",
        "G06N3/006",
        "G06N3/045",
        "G06N3/088",
        "G06N5/046",
        "G06N7/01",
        "G06N3/084"
    ],
    "abstract": "A method for operating a ride-share-enabled vehicle includes determining a target location of the ride-share-enabled vehicle, determining a ride-sharing policy algorithm to determine a behavior of the ride-share-enabled vehicle including whether to accept a multiple shared ride or maintain a single shared ride and a route of the multiple shared ride, if any, based on the determined target location of the ride-share-enabled vehicle, determining a behavior of the ride-share-enabled vehicle based on a current location of the ride-share-enabled vehicle and the determined ride-sharing policy algorithm, and causing the ride-share-enabled vehicle to be operated according to the determined behavior of the ride-share-enabled vehicle.",
    "claims": "\n1. A method for operating a ride-share-enabled vehicle comprising:\ndetermining a target location of the ride-share-enabled vehicle;\ndetermining a ride-sharing policy algorithm to determine a behavior of the ride-share-enabled vehicle including whether to accept a multiple shared ride or maintain a single shared ride and a route of the multiple shared ride, based on the determined target location of the ride-share-enabled vehicle;\ndetermining a behavior of the ride-share-enabled vehicle based on a current location of the ride-share-enabled vehicle and the determined ride-sharing policy algorithm; and\ncausing the ride-share-enabled vehicle to be operated according to the determined behavior of the ride-share-enabled vehicle.\n2. The method of claim 1, wherein the determined ride-sharing policy algorithm is configured based on a deep reinforced learning method of a deep Q-Networks (DQN).\n3. The method of claim 1, further comprising determining a current date or a current time, wherein the ride-sharing policy algorithm is determined also based on the current date or the current time.\n4. The method of claim 1, wherein the determining the ride-sharing policy algorithm comprises:\ndetermining a first ride-sharing policy algorithm as the ride-sharing policy algorithm, when the target location is a first location; and\ndetermining a second ride-sharing policy algorithm different from the first ride-sharing policy algorithm as the ride-sharing policy algorithm, when the target location is a second location different from the first location.\n5. The method of claim 4, wherein the first location is more populated than the second location, and the first ride-sharing policy algorithm is configured to accept more multiple shared rides than the second ride-sharing policy algorithm.\n6. The method of claim 5, wherein the first ride-sharing policy algorithm is not configured based on a deep reinforced learning method of a deep Q-Networks (DQN), and the second ride-sharing policy algorithm is configured based on the deep reinforced learning method of the DQN.\n7. The method of claim 1, further comprising determining a ride request density at the determined target location of the ride-share-enabled vehicle, wherein the ride-sharing policy algorithm is determined based on the determined ride request density.\n8. The method of claim 7, further comprising determining a current date or a current time, wherein the ride request density at the determined target location of the ride-share-enabled vehicle is determined based on the current date or the current time.\n9. The method of claim 7, wherein the determining the ride-sharing policy algorithm comprises:\ndetermining a first ride-sharing policy algorithm as the ride-sharing policy algorithm, when the ride request density is a first density; and\ndetermining a second ride-sharing policy algorithm different from the first ride-sharing policy algorithm as the ride-sharing policy algorithm, when the ride request density is a second density less dense than the first location.\n10. The method of claim 9, wherein the first ride-sharing policy algorithm is configured to accept more multiple shared rides than the second ride-sharing policy algorithm.\n11. The method of claim 10, wherein the first ride-sharing policy algorithm is not configured based on a deep reinforced learning method of a deep Q-Networks (DQN), and the second ride-sharing policy algorithm is configured based on the deep reinforced learning method of the DQN.\n12. The method of claim 1, wherein the target location of the ride-share-enabled vehicle comprises a target service region for a ride share service.\n13. The method of claim 1, wherein the target location of the ride-share-enabled vehicle comprises the current location of the ride-share-enabled vehicle.\n14. A non-transitory computer-readable storage medium storing instructions that, when executed by one or more processors, cause the one or more processors to perform a method for operating a ride-share-enabled vehicle, the method comprising:\ndetermining a target location of the ride-share-enabled vehicle;\ndetermining a ride-sharing policy algorithm to determine a behavior of the ride-share-enabled vehicle including whether to accept a multiple shared ride or maintain a single shared ride and a route of the multiple shared ride, based on the determined target location of the ride-share-enabled vehicle;\ndetermining a behavior of the ride-share-enabled vehicle based on a current location of the ride-share-enabled vehicle and the determined ride-sharing policy algorithm; and\ncausing the ride-share-enabled vehicle to be operated according to the determined behavior of the ride-share-enabled vehicle.\n15. The non-transitory computer-readable storage medium of claim 14, wherein the determined ride-sharing policy algorithm is configured based on a deep reinforced learning method of a deep Q-Networks (DQN).\n16. The non-transitory computer-readable storage medium of claim 14, wherein the method further comprises determining a current date or a current time, wherein the ride-sharing policy algorithm is determined also based on the current date or the current time.\n17. The non-transitory computer-readable storage medium of claim 14, wherein the method further comprises determining a ride request density at the determined target location of the ride-share-enabled vehicle, wherein the ride-sharing policy algorithm is determined based on the determined ride request density.\n18. A system for providing a ride-share service comprising:\na server including one or more processors and memory storing instructions that, when executed by one or more processors, cause the one or more processors to perform a method for operating one or more ride-share-enabled vehicles, wherein the method comprises:\ndetermining a target location of a target vehicle of the one or more ride-share-enabled vehicles;\ndetermining a ride-sharing policy algorithm to determine a behavior of the target vehicle including whether to accept a multiple shared ride or maintain a single shared ride and a route of the multiple shared ride, if any, based on the determined target location of the target vehicle;\ndetermining a behavior of the target vehicle based on a current location of the target vehicle and the determined ride-sharing policy algorithm; and\ncausing the target vehicle to be operated according to the determined behavior of the target vehicle.\n19. The system of claim 18, wherein at least one of the one or more ride-share-enabled vehicles is an autonomous vehicle.\n20. The system of claim 18, wherein the determined ride-sharing policy algorithm is configured based on a deep reinforced learning method of a deep Q-Networks (DQN).",
    "status": "Abandoned",
    "citations_own": [
        "US20100106603A1",
        "US20100332242A1",
        "US20180018568A1"
    ],
    "citations_ftf": [
        "SG11201706602RA",
        "WO2017223031A1",
        "US11599833B2"
    ],
    "citedby_own": [
        "US20200143296A1",
        "CN111191145A",
        "US10769558B2",
        "CN111898310A",
        "CN112287463A",
        "US20210102812A1",
        "US20210133566A1",
        "US11222389B2",
        "US20220036261A1",
        "US20220114690A1",
        "US11321642B1",
        "US11415424B2",
        "CN115131979A",
        "US20220366437A1",
        "US11610165B2",
        "US11616813B2",
        "US11703336B2",
        "US11733049B2"
    ],
    "citedby_ftf": [
        "EP3907661A1"
    ]
}