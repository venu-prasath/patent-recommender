{
    "patent_id": "US-11580394-B2",
    "title": "Accelerated deep learning ",
    "assignee": "Cerebras Systems Inc.",
    "publication_date": "2023-02-14",
    "patent_link": "https://patents.google.com/patent/US11580394B2/en",
    "inventors": [
        "Sean Lie",
        "Michael Morrison",
        "Michael Edwin JAMES",
        "Gary R. Lauterbach",
        "Srikanth Arekapudi"
    ],
    "classifications": [
        "G06N3/08",
        "G06F9/45533",
        "G06F9/5038",
        "G06N3/02",
        "G06N3/04",
        "G06N3/048",
        "G06N3/0481",
        "G06N3/063",
        "G06N3/084",
        "G06N3/10",
        "G06N5/04",
        "G06N3/045",
        "G06N3/0454",
        "Y02D10/00"
    ],
    "abstract": "Techniques in advanced deep learning provide improvements in one or more of accuracy, performance, and energy efficiency, such as accuracy of learning, accuracy of prediction, speed of learning, performance of learning, and energy efficiency of learning. An array of processing elements performs flow-based computations on wavelets of data. Each processing element has a respective compute element and a respective routing element. Each compute element has processing resources and memory resources. Each router enables communication via wavelets with at least nearest neighbors in a 2D mesh. Stochastic gradient descent, mini-batch gradient descent, and continuous propagation gradient descent are techniques usable to train weights of a neural network modeled by the processing elements. Reverse checkpoint is usable to reduce memory usage during the training.",
    "claims": "\n1. A method comprising:\ntraining a neural network comprising a plurality of ordered, connected layers;\nwherein the order identifies for each respective layer which others of the layers are prior to the respective layer and which others of the layers are subsequent to the respective layer;\nwherein each layer comprises one or more neurons, each neuron comprising weights, and connected to at least one of at least one prior neuron of a prior layer and at least one subsequent neuron of a subsequent layer; and\nwherein each neuron is implemented by one or more processing elements, each processing element comprising\nat least one coupling to a fabric, the processing element being enabled to communicate via the fabric via a plurality of virtual channels,\na first memory enabled to store instructions corresponding to at least computations of the neuron,\na second memory enabled to store the weights, and\nhardware execution resources enabled to execute instructions from the respective first memory and access data from the respective second memory.\n2. The method of claim 1, wherein each of the layers is a respective internal layer of the neural network, and the neural network further comprises an input layer and an output layer.\n3. The method of claim 1, wherein the training comprises:\ndetermining a second activation based on a first activation and first weights;\ndetermining and saving second weights based on a first delta and the first weights;\ndetermining a fourth activation based on a third activation and selected weights, wherein the selected weights are dynamically selected from the first weights and the second weights; and\ndetermining and saving third weights based on a second delta and the selected weights.\n4. The method of claim 3, wherein the determining the second activation comprises:\nreceiving the first activation via the fabric from the at least one prior neuron;\ncomputing the second activation based at least in part on the first activation and the first weights by at least executing first instructions stored in the first memory and accessing the first weights in the second memory; and\nselectively transmitting the second activation via the fabric to the at least one subsequent neuron.\n5. The method of claim 4, wherein the determining the fourth activation comprises:\nreceiving the third activation via the fabric from the at least one prior neuron;\ncomputing the fourth activation based at least in part on the third activation and the selected weights by at least executing the first instructions and accessing the selected weights in the second memory; and\nselectively transmitting the fourth activation via the fabric to the at least one subsequent neuron.\n6. The method of claim 5, wherein the determining and saving the second weights comprises:\nreceiving the first delta that is partially based on the second activation via the fabric from the at least one subsequent neuron;\ncomputing a first gradient based at least in part on the first delta and the second activation by at least executing second instructions stored in the first memory;\ncomputing the second weights based at least in part on the first gradient, a learning rule, and the first weights by at least executing third instructions stored in the first memory and accessing the first weights in the second memory; and\nstoring the second weights in the second memory.\n7. The method of claim 6, wherein the determining and saving the third weights comprises:\nreceiving the second delta that is partially based on the fourth activation via the fabric from the at least one subsequent neuron;\ncomputing a second gradient based at least in part on a third delta and the fourth activation by at least executing the second instructions stored in the first memory;\ncomputing the third weights based at least in part on the second gradient, the learning rule and the selected weights by at least executing the third instructions stored in the first memory and accessing the selected weights in the second memory; and\nstoring the third weights in the second memory.\n8. The method of claim 7, wherein the computing the second gradient additionally comprises optionally recomputing the fourth activation based at least in part upon the selected weights.\n9. The method of claim 7, wherein the computing the first gradient additionally comprises optionally recomputing the second activation based at least in part upon the first weights.\n10. The method of claim 5, wherein the selectively transmitting the second activation and the selectively transmitting the fourth activation are selectively based upon the respective values of the second activation and the fourth activation.\n11. The method of claim 5, wherein the selectively transmitting the second activation and the selectively transmitting the fourth activation are selectively based upon the respective absolute values of the second activation and the fourth activation exceeding respective first and second thresholds.\n12. The method of claim 3, wherein the determining and saving the second weights comprises:\nreceiving the first delta that is partially based on the second activation via the fabric from the at least one subsequent neuron;\ncomputing a first gradient based at least in part on the first delta and the second activation by at least executing second instructions stored in the first memory;\ncomputing the second weights based at least in part on the first gradient, a learning rule, and the first weights by at least executing third instructions stored in the first memory and accessing the first weights in the second memory; and\nstoring the second weights in the second memory.\n13. The method of claim 12, wherein the determining and saving the third weights comprises:\nreceiving the second delta that is partially based on the fourth activation via the fabric from the at least one subsequent neuron;\ncomputing a second gradient based at least in part on a third delta and the fourth activation by at least executing the second instructions stored in the first memory;\ncomputing the third weights based at least in part on the second gradient, the learning rule and the selected weights by at least executing the third instructions stored in the first memory and accessing the selected weights in the second memory; and\nstoring the third weights in the second memory.\n14. The method of claim 13, wherein the determining the fourth activation additionally comprises storing the fourth activation in the second memory and the computing the second gradient additionally comprises accessing the fourth activation in the second memory.\n15. The method of claim 12, wherein the selected weights are dynamically selected in accordance with which of the first weights and the second weights were stored most recently.\n16. The method of claim 1, wherein the method is carried out via a substantially whole wafer comprising the processing elements.\n17. An apparatus comprising:\na plurality of processing elements;\na training workload comprising a set of machine codes selected from a predefined native instruction set of codes for performing training of a neural network comprising a plurality of ordered, connected layers;\nwherein the order identifies for each respective layer which others of the layers are prior to the respective layer and which others of the layers are subsequent to the respective layer;\nwherein each layer comprises one or more neurons, each neuron comprising weights, and connected to at least one of at least one prior neuron of a prior layer and at least one subsequent neuron of a subsequent layer; and\nwherein each neuron is implemented by one or more of the processing elements, each processing element comprising\nat least one coupling to a fabric, the processing element being enabled to communicate via the fabric via a plurality of virtual channels,\na first memory enabled to store instructions corresponding to at least computations of the neuron,\na second memory enabled to store the weights, and\na compute engine enabled to perform a predefined set of basic operations in response to receiving a corresponding basic instruction selected from the predefined native instruction set of codes, the performing via execution of instructions from the respective first memory and access of data from the respective second memory.\n18. The apparatus of claim 17, wherein each of the layers is a respective internal layer of the neural network, and the neural network further comprises an input layer and an output layer.\n19. The apparatus of claim 17, wherein the training workload comprises respective sets of the machine codes directed to:\ndetermining a second activation based on a first activation and first weights;\ndetermining and saving second weights based on a first delta and the first weights;\ndetermining a fourth activation based on a third activation and selected weights, wherein the selected weights are dynamically selected from the first weights and the second weights; and\ndetermining and saving third weights based on a second delta and the selected weights.\n20. The apparatus of claim 19, wherein the determining the second activation comprises:\nreceiving the first activation via the fabric from the at least one prior neuron;\ncomputing the second activation based at least in part on the first activation and the first weights by at least executing first instructions stored in the first memory and accessing the first weights in the second memory; and\nselectively transmitting the second activation via the fabric to the at least one subsequent neuron.\n21. The apparatus of claim 20, wherein the determining the fourth activation comprises:\nreceiving the third activation via the fabric from the at least one prior neuron;\ncomputing the fourth activation based at least in part on the third activation and the selected weights by at least executing the first instructions and accessing the selected weights in the second memory; and\nselectively transmitting the fourth activation via the fabric to the at least one subsequent neuron.\n22. The apparatus of claim 21, wherein the determining and saving the second weights comprises:\nreceiving the first delta that is partially based on the second activation via the fabric from the at least one subsequent neuron;\ncomputing a first gradient based at least in part on the first delta and the second activation by at least executing second instructions stored in the first memory;\ncomputing the second weights based at least in part on the first gradient, a learning rule, and the first weights by at least executing third instructions stored in the first memory and accessing the first weights in the second memory; and\nstoring the second weights in the second memory.\n23. The apparatus of claim 22, wherein the determining and saving the third weights comprises:\nreceiving the second delta that is partially based on the fourth activation via the fabric from the at least one subsequent neuron;\ncomputing a second gradient based at least in part on a third delta and the fourth activation by at least executing the second instructions stored in the first memory;\ncomputing the third weights based at least in part on the second gradient, the learning rule and the selected weights by at least executing the third instructions stored in the first memory and accessing the selected weights in the second memory; and\nstoring the third weights in the second memory.\n24. The apparatus of claim 23, wherein the computing the second gradient additionally comprises optionally recomputing the fourth activation based at least in part upon the selected weights.\n25. The apparatus of claim 23, wherein the computing the first gradient additionally comprises optionally recomputing the second activation based at least in part upon the first weights.\n26. The apparatus of claim 19, wherein the determining and saving the second weights comprises:\nreceiving the first delta that is partially based on the second activation via the fabric from the at least one subsequent neuron;\ncomputing a first gradient based at least in part on the first delta and the second activation by at least executing second instructions stored in the first memory;\ncomputing the second weights based at least in part on the first gradient, a learning rule, and the first weights by at least executing third instructions stored in the first memory and accessing the first weights in the second memory; and\nstoring the second weights in the second memory.\n27. The apparatus of claim 26, wherein the determining and saving the third weights comprises:\nreceiving the second delta that is partially based on the fourth activation via the fabric from the at least one subsequent neuron;\ncomputing a second gradient based at least in part on a third delta and the fourth activation by at least executing the second instructions stored in the first memory;\ncomputing the third weights based at least in part on the second gradient, the learning rule and the selected weights by at least executing the third instructions stored in the first memory and accessing the selected weights in the second memory; and\nstoring the third weights in the second memory.\n28. The apparatus of claim 26, wherein the selected weights are dynamically selected in accordance with which of the first weights and the second weights were stored most recently.\n29. The apparatus of claim 17, wherein the apparatus is implemented via a substantially whole wafer comprising the processing elements.\n30. A system comprising:\nmeans for training a neural network comprising a plurality of ordered, connected layers;\nwherein the order identifies for each respective layer which others of the layers are prior to the respective layer and which others of the layers are subsequent to the respective layer;\nwherein each layer comprises one or more neurons, each neuron comprising weights, and connected to at least one of at least one prior neuron of a prior layer and at least one subsequent neuron of a subsequent layer; and\nwherein each neuron is implemented by one or more processing elements, each processing element comprising\nat least one coupling to a fabric, the processing element being enabled to communicate via the fabric via a plurality of virtual channels,\na first memory enabled to store instructions corresponding to at least computations of the neuron,\na second memory enabled to store the weights, and\nhardware execution resources enabled to execute instructions from the respective first memory and access data from the respective second memory.\n31. The system of claim 30, wherein each of the layers is a respective internal layer of the neural network, and the neural network further comprises an input layer and an output layer.\n32. The system of claim 30, wherein the means for training comprises:\nmeans for determining a second activation based on a first activation and first weights;\nmeans for determining and saving second weights based on a first delta and the first weights;\nmeans for determining a fourth activation based on a third activation and selected weights, wherein the selected weights are dynamically selected from the first weights and the second weights; and\nmeans for determining and saving third weights based on a second delta and the selected weights.\n33. The system of claim 32, wherein the means for determining the second activation comprises:\nmeans for receiving the first activation via the fabric from the at least one prior neuron;\nmeans for computing the second activation based at least in part on the first activation and the first weights implemented by at least executing first instructions stored in the first memory and accessing the first weights in the second memory; and\nmeans for selectively transmitting the second activation via the fabric to the at least one subsequent neuron.\n34. The system of claim 33, wherein the means for determining the fourth activation comprises:\nmeans for receiving the third activation via the fabric from the at least one prior neuron;\nmeans for computing the fourth activation based at least in part on the third activation and the selected weights implemented by at least executing the first instructions and accessing the selected weights in the second memory; and\nmeans for selectively transmitting the fourth activation via the fabric to the at least one subsequent neuron.\n35. The system of claim 34, wherein the means for determining and saving the second weights comprises:\nmeans for receiving the first delta that is partially based on the second activation via the fabric from the at least one subsequent neuron;\nmeans for computing a first gradient based at least in part on the first delta and the second activation implemented by at least executing second instructions stored in the first memory;\nmeans for computing the second weights based at least in part on the first gradient, a learning rule, and the first weights implemented by at least executing third instructions stored in the first memory and accessing the first weights in the second memory; and\nmeans for storing the second weights in the second memory.\n36. The system of claim 35, wherein the means for determining and saving the third weights comprises:\nmeans for receiving the second delta that is partially based on the fourth activation via the fabric from the at least one subsequent neuron;\nmeans for computing a second gradient based at least in part on a third delta and the fourth activation implemented by at least executing the second instructions stored in the first memory;\nmeans for computing the third weights based at least in part on the second gradient, the learning rule and the selected weights implemented by at least executing the third instructions stored in the first memory and accessing the selected weights in the second memory; and\nmeans for storing the third weights in the second memory.\n37. The system of claim 36, wherein the means for computing the second gradient additionally comprises means for optionally recomputing the fourth activation based at least in part upon the selected weights.\n38. The system of claim 30, wherein a substantially whole wafer comprises the system."
}