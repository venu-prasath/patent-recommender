{
    "patent_id": "US-11056099-B2",
    "title": "End-to-end speech recognition with policy learning ",
    "assignee": "Salesforce.Com, Inc.",
    "publication_date": "2021-07-06",
    "patent_link": "https://patents.google.com/patent/US11056099B2/en",
    "inventors": [
        "Yingbo Zhou",
        "Caiming Xiong"
    ],
    "classifications": [
        "G10L15/063",
        "G06N3/044",
        "G06N3/0445",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/084",
        "G06N7/005",
        "G06N7/01",
        "G10L15/14",
        "G10L15/16",
        "G10L25/51",
        "G06N3/082"
    ],
    "abstract": "The disclosed technology teaches a deep end-to-end speech recognition model, including using multi-objective learning criteria to train a deep end-to-end speech recognition model on training data comprising speech samples temporally labeled with ground truth transcriptions. The multi-objective learning criteria updates model parameters of the model over one thousand to millions of backpropagation iterations by combining, at each iteration, a maximum likelihood objective function that modifies the model parameters to maximize a probability of outputting a correct transcription and a policy gradient function that modifies the model parameters to maximize a positive reward defined based on a non-differentiable performance metric which penalizes incorrect transcriptions in accordance with their conformity to corresponding ground truth transcriptions; and upon convergence after a final backpropagation iteration, persisting the modified model parameters learned by using the multi-objective learning criteria with the model to be applied to further end-to-end speech recognition.",
    "claims": "\n1. A computer-implemented method of training a deep end-to-end speech recognition model, the method comprising:\nreceiving training data comprising speech samples labeled with ground truth transcriptions;\ngenerating, via the deep end-to-end speech recognition model, a respective distribution of an output transcription corresponding to each speech sample from the training data;\njointly training the deep end-to-end speech recognition model with the received training data to update parameters of the model, wherein the joint training comprises:\ncomputing a maximum likelihood objective function based on the respective distribution of the output transcription;\ncomputing a self-critical sequence training (SCST) policy learning objective function based on the output transcription, a sampled transcription and a ground truth transcription from the training data;\nupdating the deep end-to-end speech recognition model based on a weighted sum of the maximum likelihood objective function and the SCST policy learning objective function; and\npersisting the modified model parameters learned by the joint training with the deep end-to-end speech recognition model to be applied to further end-to-end speech recognition.\n2. The method of claim 1, wherein the maximum likelihood objective function comprises a connectionist temporal classification (CTC) objective function.\n3. The method of claim 1, wherein the self-critical sequence training (SCST) policy learning determines a reward for an output transcription.\n4. The method of claim 1, wherein the model produces a normalized distribution of softmax probabilities over a set of transcription labels.\n5. The method of claim 4, wherein the maximum likelihood objective function maximizes a probability of the model outputting a correct transcription by:\ncombining individual probabilities of a plurality of candidate output transcriptions to produce an output transcription, wherein an individual probability of a candidate output transcription is determined by selecting a most probable label for each of a plurality of timesteps and multiplying softmax probabilities of each of the selected labels; and\nmeasuring differences between the output transcription and a ground truth transcription.\n6. The method of claim 4, wherein the self-critical sequence training (SCST) policy learning determines a reward for an output transcription by:\nindependently sampling a transcription label for each of a plurality of timesteps and concatenating the transcription labels sampled across the timesteps to produce the output transcription; and\nmeasuring differences between the output transcription and a ground truth transcription based on a performance metric.\n7. The method of claim 4, wherein the performance metric is word error rate (WER).\n8. The method of claim 4, wherein the performance metric is character error rate (CER).\n9. A deep end-to-end speech recognition system, comprising:\nan input port that receives digital audio samples of a signal comprising speech labeled with a ground truth transcription; and\na deep end-to-end speech recognition processor comprising hardware and a stack of layers running on the hardware including convolution layers and recurrent layers, coupled to the input port and configurable to process the digital audio samples, recognize speech from the audio samples, and output transcriptions corresponding to recognized speech;\nwherein the deep end-to-end speech recognition processor includes parameters jointly trained on training data to update parameters of the deep end-to-end speech recognition model,\nwherein the joint training comprises:\ncomputing a maximum likelihood objective function based on the respective distribution of the output transcription;\ncomputing a self-critical sequence training (SCST) policy learning objective function based on the output transcription, a sampled transcription and a ground truth transcription from the training data;\nupdating the deep end-to-end speech recognition model based on a weighted sum of the maximum likelihood objective function and the SCST policy learning objective function; and\nwherein the modified model parameters learned by the joint training are persisted with the model to be applied to further end-to-end speech recognition.\n10. The deep end-to-end speech recognition system of claim 9, wherein the maximum likelihood objective function comprises a connectionist temporal classification (CTC) objective function.\n11. The deep end-to-end speech recognition system of claim 9, wherein the self-critical sequence training (SCST) policy learning determines a reward for an output transcription.\n12. The deep end-to-end speech recognition system of claim 9, wherein the processor produces a normalized distribution of softmax probabilities over a set of transcription labels.\n13. The deep end-to-end speech recognition system of claim 12, wherein the maximum likelihood objective function maximizes the probability of the model outputting a correct transcription by:\ncombining individual probabilities of a plurality of candidate output transcriptions to produce an output transcription, wherein an individual probability of a candidate output transcription is determined by selecting a most probable label for each of a plurality of timesteps and multiplying softmax probabilities of each of the selected labels; and\nmeasuring differences between the output transcription and a ground truth transcription.\n14. The deep end-to-end speech recognition system of claim 12, wherein the self-critical sequence training (SCST) policy learning determines a reward for an output transcription by:\nindependently sampling a transcription label for each of a plurality of timesteps and concatenating the transcription labels sampled across the timesteps to produce the output transcription; and\nmeasuring differences between the output transcription and a ground truth transcription based on the performance metric.\n15. The deep end-to-end speech recognition system of claim 14, wherein the performance metric is word error rate (WER).\n16. The deep end-to-end speech recognition system of claim 14, wherein the performance metric is character error rate (CER).\n17. A tangible non-transitory computer readable storage medium having stored thereon program instructions executable by a processor, the instructions, when executed on a processor, implement a method including:\nreceiving training data comprising speech samples labeled with ground truth transcriptions;\ngenerating, via the deep end-to-end speech recognition model, a respective distribution of an output transcription corresponding to each speech sample from the training data;\njointly training a deep end-to-end speech recognition model on training data comprising speech samples to update parameters of the model, wherein the joint training comprises:\ncomputing a maximum likelihood objective function based on the respective distribution of the output transcription;\ncomputing a self-critical sequence training (SCST) policy learning objective function based on the output transcription, a sampled transcription and a ground truth transcription from the training data;\nupdating the deep end-to-end speech recognition model based on a weighted sum of the maximum likelihood objective function and the SCST policy learning objective function; and\npersisting the modified model parameters learned by the joint training with the deep end-to-end speech recognition model to be applied to further end-to-end speech recognition.\n18. The tangible non-transitory computer readable storage medium of claim 17, wherein the maximum likelihood objective function comprises a connectionist temporal classification (CTC) objective function.\n19. The tangible non-transitory computer readable storage medium of claim 17, wherein the self-critical sequence training (SCST) policy learning determines a reward for an output transcription.\n20. The tangible non-transitory computer readable storage medium of claim 17, wherein the model produces a normalized distribution of softmax probabilities over a set of transcription labels.\n21. The tangible non-transitory computer readable storage medium of claim 20, wherein the maximum likelihood objective function maximizes the probability of the model outputting a correct transcription by:\ncombining individual probabilities of a plurality of candidate output transcriptions to produce an output transcription, wherein an individual probability of a candidate output transcription is determined by selecting a most probable label for each of a plurality of timesteps and multiplying softmax probabilities of each of the selected labels; and\nmeasuring differences between the output transcription and a ground truth transcription.\n22. The tangible non-transitory computer readable storage medium of claim 20, wherein the self-critical sequence training (SCST) policy learning determines a reward for an output transcription by:\nindependently sampling a transcription label for each of a plurality of timesteps and concatenating the transcription labels sampled across the timesteps to produce the output transcription; and\nmeasuring differences between the output transcription and a ground truth transcription based on a performance metric.\n23. The tangible non-transitory computer readable storage medium of claim 22, wherein the performance metric is word error rate (WER).\n24. The tangible non-transitory computer readable storage medium of claim 22, wherein the performance metric is character error rate (CER)."
}