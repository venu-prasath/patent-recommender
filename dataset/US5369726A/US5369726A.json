{
    "patent_link": "https://patents.google.com/patent/US5369726A/en",
    "patent_id": "US5369726A",
    "title": "Speech recognition circuitry employing nonlinear processing speech element modeling and phoneme estimation",
    "abstract": "A phoneme estimator in a speech-recognition system includes energy detect circuitry for detecting the segments of a speech signal that should be analyzed for phoneme content. Speech-element processors then process the speech signal segments, calculating nonlinear representations of the segments. The nonlinear representation data is applied to speech-element modeling circuitry which reduces the data through speech element specific modeling. The reduced data are then subjected to further nonlinear processing. The results of the further nonlinear processing are again applied to speech-element modeling circuitry, producing phoneme isotype estimates. The phoneme isotype estimates are rearranged and consolidated, that is, the estimates are uniformly labeled and duplicate estimates are consolidated, forming estimates of words or phrases containing minimal numbers of phonemes. The estimates may then be compared with stored words or phrases to determine what was spoken.",
    "inventors": [
        "John P. Kroeker",
        "Robert L. Powers"
    ],
    "assignee": "Eliza Corp",
    "classifications": [
        "G10L15/02",
        "G10L2015/025"
    ],
    "claims": "\n1. A method of identifying speech elements of interest in a speech signal, said method comprising the steps of:\nA. converting the speech signal to a sequence of digital quantities;\nB. subjecting the digital quantities to a sequence of digital processing steps including a sequence of vector-processing steps whose outputs are multi-element vectors and whose inputs include vector outputs of previous processing steps thereby to generate a first vector with components which are associated with a speech element;\nC. comparing said first vector with a first set of model vectors which are associated with a set of known speech elements, for each comparison of said first vector with one of the model vectors in the first set of model vectors deriving a value representing the degree of similarity between the vectors and generating a second vector with these values as components;\nD. processing said second vector to product a third vector; and\nE. comparing said third vector with a second set of model vectors associated with a set of known speech elements and producing respective speech-element estimate signals that represent the likelihoods that the speech contains the respective known speech elements in the set.\n2. A method of recognizing spoken words or phrases in a speech signal, said method comprising the steps of:\nA. receiving an electrical speech signal and converting a segment of the speech signal to a sequence of digital quantities;\nB. subjecting the digital quantities to a sequence of digital processing steps and generating a first vector with components which are associated with a speech element;\nC. comparing said first vector with each vector in a first set of model vectors, where each model vector is associated with a known speech element, the results of such comparisons, which are values representing the degree of similarity between the first vector and each of the model vectors in said first set of model vectors, being accumulated as the components of a second vector;\nD. comparing a third vector derived from said second vector with a second set of model vectors that are associated with known speech elements and producing respective speech-element estimate signals that represent the likelihoods that the speech contains the respective known speech elements;\nE. repeating steps A through D for successive segments of the speech signal which are associated with a spoken word or phrase; and\nF. combining the speech element estimate signals produced in step D to form combination signals; and\nG. in response to the combination signals identifying words or phrases corresponding to the speech signal.",
    "status": "Expired - Lifetime",
    "citations_own": [
        "US3727193A",
        "US4156868A",
        "US4227177A",
        "US4241329A",
        "US4277644A",
        "US4400788A",
        "US4400828A",
        "US4412098A",
        "US4592085A",
        "US4597098A",
        "US4601054A",
        "US4606069A",
        "US4624010A",
        "US4696041A",
        "US4712243A",
        "US4715004A",
        "US4718092A",
        "US4718093A",
        "US4723290A",
        "US4813074A",
        "US4908865A",
        "US4941178A",
        "US4962535A",
        "US5168524A"
    ],
    "citations_ftf": [
        "GB1569450A",
        "US4489434A",
        "DE3584567D1",
        "US5027408A"
    ],
    "citedby_own": [
        "US5493516A",
        "WO1996010818A1",
        "WO1996013829A1",
        "WO1996013828A1",
        "US5734793A",
        "US5796924A",
        "WO2001073593A1",
        "US20020002463A1",
        "US20020138262A1",
        "US20020143538A1",
        "US6629073B1",
        "US20030187643A1",
        "US6662158B1",
        "US6772117B1",
        "US6868380B2",
        "US20050105779A1",
        "US6963871B1",
        "US20070005586A1",
        "US20090012638A1",
        "US20090216528A1",
        "US8812300B2",
        "US8855998B2"
    ],
    "citedby_ftf": [
        "US5168524A",
        "JPH05134694A",
        "JPH05188994A",
        "FR2696036B1",
        "US5455889A",
        "US5652897A",
        "US6006181A",
        "FR2769117B1",
        "JP3789246B2",
        "US6442520B1",
        "AU2002213338A1",
        "US7554464B1",
        "US20110014981A1",
        "FR2913171A1",
        "WO2012177787A1",
        "WO2015145219A1",
        "US10008201B2",
        "EP3641286B1"
    ]
}