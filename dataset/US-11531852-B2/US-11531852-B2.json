{
    "patent_id": "US-11531852-B2",
    "title": "Machine learning systems and methods for training with noisy labels ",
    "assignee": "D-Wave Systems Inc.",
    "publication_date": "2022-12-20",
    "patent_link": "https://patents.google.com/patent/US11531852B2/en",
    "inventors": [
        "Arash Vahdat"
    ],
    "classifications": [
        "G06N3/08",
        "G06K9/6278",
        "G06F18/213",
        "G06F18/214",
        "G06F18/24155",
        "G06K9/6232",
        "G06K9/6256",
        "G06N3/042",
        "G06N3/0427",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/047",
        "G06N3/0472",
        "G06N5/04",
        "G06N7/005",
        "G06N7/01"
    ],
    "abstract": "Machine learning classification models which are robust against label noise are provided. Noise may be modelled explicitly by modelling \u201clabel flips\u201d, where incorrect binary labels are \u201cflipped\u201d relative to their ground truth value. Distributions of label flips may be modelled as prior and posterior distributions in a flexible architecture for machine learning systems. An arbitrary classification model may be provided within the system. The classification model is made more robust to label noise by operation of the prior and posterior distributions. Particular prior and approximating posterior distributions are disclosed.",
    "claims": "\n1. A method for instantiating a machine learning system for classifying one or more observed elements of an input dataset, the input dataset labelled with one or more observed labels drawn from a plurality of binary labels, the method executed by at least one processor in communication with at least one memory and comprising:\ninstantiating a recognition system in the at least one memory, the recognition system operable to classify elements of the input dataset by generating a classification probability for the at least one observed element;\ngenerating a prior distribution, the prior distribution modelling label noise as a set of stochastic label flips on the plurality of binary labels, each label flip indicating a belief in the correctness of a corresponding label;\ninstantiating an approximating posterior system in the at least one memory, the approximating posterior system being a deep neural network operable to generate a posterior probability for one or more label flips for the observed element given the one or more observed labels;\nmodeling an approximating posterior distribution using the approximating posterior system, the approximating posterior distribution approximating a true posterior distribution corresponding to the prior distribution, wherein at least one of the prior and posterior distribution comprises a spike, the spike defining a high-probability state where no labels are flipped; and\ntraining at least one of the recognition system and the approximating posterior system based on a training dataset, the prior distribution, and the approximating posterior distribution.\n2. The method of claim 1 wherein the method further comprises instantiating a shared transformation system in the at least one memory, the shared transformation system operable to receive the one or more observed elements and to generate a representation of the observed elements based on one or more shared parameters, wherein the recognition system and the approximating posterior system are operable to receive the representation of the observed elements as input.\n3. The method of claim 2 wherein training at least one of the recognition system and the approximating posterior system comprises training the shared transformation system to generate the one or more shared parameters.\n4. The method of claim 1 further comprising selecting the prior distribution, and selecting the approximating posterior distribution, wherein at least one of selecting the prior distribution, and selecting the approximating posterior distribution comprises selecting a predetermined distribution.\n5. The method of claim 1 wherein the approximating posterior distribution comprises a directed graphical model, the directed graphical model operable to generate, given an initial class, a probability of label change for each of one or more remaining classes.\n6. The method of claim 1 wherein the method further comprises selecting one of the prior distribution and the approximating posterior distribution based on a previous selection of the other one of the prior distribution and the approximating posterior distribution.\n7. The method of claim 6 wherein the prior distribution comprises a Boltzmann distribution with a spike and the approximating posterior distribution comprises a factorial distribution with a spike.\n8. A method for training a machine learning system for classifying one or more observed elements of an input dataset, the input dataset labelled with one or more observed labels, the method executed by at least one processor in communication with at least one memory and comprising:\ninstantiating an inference system in the at least one memory;\nmodelling a joint probability distribution over a plurality of variables using the inference system, the plurality of variables comprising the observed labels and one or more true labels, the joint probability distribution conditional on the input dataset;\ninstantiating an auxiliary system in the at least one memory;\nmodelling an auxiliary probability distribution over the plurality of variables independently of the input dataset using the auxiliary system;\ninstantiating a representation system in the at least one memory, the representation system operable to characterize the one or more interactions between the plurality of variables of the joint distribution;\ntraining at least one of the representation system and the inference system based on a first modified lower bound defined over at least a noisy subset of the input dataset, the noisy subset comprising input data and associated observed labels, the first modified lower bound comprising an original lower bound based on the joint probability distribution and an additional term based on the auxiliary probability distribution.\n9. The method of claim 8 wherein training at least one of the representation system and the inference system comprises training at least one of the representation system and the inference system based on a modified clean lower bound defined over a clean subset of the input dataset, the clean subset disjoint from the noisy subset and comprising input data, associated observed labels, and associated true labels, the modified clean lower bound based at least in part on the auxiliary probability distribution.\n10. The method of claim 8 wherein training at least one of the representation system and the inference system comprises determining a gradient over the additional term and training at least one of the representation system and the inference system based on the gradient.\n11. The method of claim 10 wherein determining the gradient over the additional term comprises determining a first gradient over a positive phase of the additional term and a second gradient over a negative phase of the additional term, the first gradient determined analytically and the second gradient determined by approximation.\n12. The method of claim 11 wherein training at least one of the representation system and the inference system comprises optimization of an objective function by expectation maximization.\n13. The method of claim 8 wherein the auxiliary probability distribution is fixed while training at least one of the representation system and the inference system.\n14. The method of claim 13 comprising training the auxiliary probability distribution based on information independent of the training dataset prior to training at least one of the representation system and the inference system.\n15. The method of claim 8 wherein training at least one of the representation system and the inference system comprises training at least one of the representation system and the inference system based on an optimization function comprising a first term based on the joint probability distribution and independent of the auxiliary probability distribution and based on a second term based on the auxiliary probability distribution, wherein at least one of the first and second terms is scaled by a scaling factor.\n16. The method of claim 15 comprising setting the scaling factor to a first value for a first iteration of training and setting the scaling factor to a second value for a second iteration of training.\n17. The method of claim 16 wherein the scaling factor is monotonically decreasing during training.\n18. The method of claim 8 wherein the inference system models the joint probability distribution based on an undirected graphical model, the undirected graphical model comprising one or more undirected edges representing one or more interactions between the plurality of variables of the joint distribution.\n19. The method of claim 18 wherein the inference system models the auxiliary probability distribution based on an auxiliary undirected graphical model, the auxiliary undirected graphical model comprising an undirected subgraph of the undirected graphical model of the inference system."
}