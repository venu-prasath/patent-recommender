{
    "patent_id": "US-11514293-B2",
    "title": "Future object trajectory predictions for autonomous machine applications ",
    "assignee": "Nvidia Corporation",
    "publication_date": "2022-11-29",
    "patent_link": "https://patents.google.com/patent/US11514293B2/en",
    "inventors": [
        "Ruben Villegas",
        "Alejandro Troccoli",
        "Iuri Frosio",
        "Stephen Tyree",
        "Wonmin Byeon",
        "Jan Kautz"
    ],
    "classifications": [
        "G06N3/0445",
        "G06N3/044",
        "G06N3/08",
        "B60W40/02",
        "G06N20/10",
        "G06N20/20",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/047",
        "G06N5/01"
    ],
    "abstract": "In various examples, historical trajectory information of objects in an environment may be tracked by an ego-vehicle and encoded into a state feature. The encoded state features for each of the objects observed by the ego-vehicle may be used\u2014e.g., by a bi-directional long short-term memory (LSTM) network\u2014to encode a spatial feature. The encoded spatial feature and the encoded state feature for an object may be used to predict lateral and/or longitudinal maneuvers for the object, and the combination of this information may be used to determine future locations of the object. The future locations may be used by the ego-vehicle to determine a path through the environment, or may be used by a simulation system to control virtual objects\u2014according to trajectories determined from the future locations\u2014through a simulation environment.",
    "claims": "\n1. A method comprising:\npositioning, within a spatial arrangement, one or more encoded state features, each encoded state feature of the one or more encoded state features corresponding to a respective object in an environment, each encoded state feature being positioned at a location within the spatial arrangement that corresponds to a relative location of the respective object in the environment with respect to an ego-vehicle;\ncomputing, using a long short-term memory (LSTM) network, a first encoded spatial feature by inputting, to the LSTM network, first data representative of the spatial arrangement traversed in a first direction;\ncomputing, using the LSTM network or another LSTM network, a second encoded spatial feature by inputting, to the LSTM network or the another LSTM network, second data representative of the spatial arrangement traversed in a second direction different from the first direction; and\nfor each respective object, computing confidence values corresponding to a likelihood of occurrence of a plurality of object maneuvers based at least in part on the encoded state feature corresponding to the respective object, the first encoded spatial feature, and the second encoded spatial feature.\n2. The method of claim 1, wherein each encoded state feature is computed using a machine learning model and based at least in part on a plurality of prior locations within the environment of the respective object corresponding to the encoded state feature.\n3. The method of claim 1, wherein the plurality of object maneuvers include lateral maneuvers selected from changing to a right adjacent lane, changing to a left adjacent lane, and staying in a current lane, and the plurality of object maneuvers further include longitudinal maneuvers selected from accelerating, decelerating, or maintaining a current speed.\n4. The method of claim 1, further comprising:\nbased at least in part on the confidence values, determining one or more object maneuvers from the plurality of object maneuvers;\nfor each respective object, computing a density function corresponding to a space of future locations for the respective object based at least in part on the encoded state feature corresponding to the respective object, the first encoded spatial feature, the second encoded spatial feature, and the one or more object maneuvers.\n5. The method of claim 4, wherein the density function represents a region of locations within the space of one or more future locations, the region of locations computed using at least one of: a standard deviation of the one or more future locations, a mean of the one or more future locations, or a covariance of the one or more future locations.\n6. The method of claim 4, further comprising:\ninputting, to a machine learning model, third data representative of a future location within the environment, the machine learning model for computing the one or more encoded state features; and\ncomputing, using the machine learning model, an updated encoded state feature for the respective object using the third data.\n7. The method of claim 6, wherein the updated encoded state feature is used to compute another future location after the future location for the respective object.\n8. The method of claim 1, wherein the environment is a simulation environment, and at least an object maneuver from the plurality of object maneuvers is used to determine a trajectory for the respective object within the simulation environment.\n9. The method of claim 1, wherein each encoded state feature of the one or more encoded state features is computed using a respective instantiation of an LSTM network.\n10. A method comprising:\ninputting, at a first time instance and to a long short-term memory (LSTM) network, first data representative of one or more past locations and a current location of an object in an environment;\ncomputing, using the LSTM network and based at least in part on the first data, a first encoded state feature corresponding to the object at the first time instance;\ncomputing, at the first time instance, an encoded spatial feature using the first encoded state feature and one or more additional encoded state features corresponding to one or more additional objects in the environment;\npredicting a future location of the object in the environment using the encoded spatial feature and the first encoded state feature;\ninputting, at a second time instance after the first time instance and to the LSTM network, second data representative of one or more updated past locations, a new current location, and the future location of the object in the environment; and\ncomputing, using the LSTM network and based at least in part on the second data, a second encoded state feature corresponding to the object at the second time instance.\n11. The method of claim 10, wherein the one or more past locations and the current location are represented as values relative to an origin point of an ego-vehicle in the environment.\n12. The method of claim 10, wherein the computing the encoded spatial feature includes:\npopulating a spatial arrangement to include the first encoded state feature and the one or more additional encoded state features, the populating including positioning the first encoded state feature and the one or more additional encoded state features at spatial arrangement locations within the spatial arrangement, the spatial arrangement locations corresponding to relative locations of the object and the one or more additional objects in the environment with respect to an ego-vehicle; and\ninputting, to a machine learning model, third data representative of the spatial arrangement.\n13. The method of claim 10, wherein the first encoded state feature, the second encoded state feature, and the encoded spatial feature are computed based at least in part on sensor data generated by one or more sensors of an ego-vehicle.\n14. The method of claim 10, wherein the encoded spatial feature is computed using a bi-directional LSTM network.\n15. The method of claim 10, further comprising:\ndetermining a predicted maneuver from a plurality of maneuvers based at least in part on the first encoded state feature and the encoded spatial feature,\nwherein the predicting the future location of the object is further based at least in part on the predicted maneuver.\n16. The method of claim 15, wherein the predicted maneuver includes a longitudinal component and a lateral component.\n17. A system comprising:\none or more sensors to generate sensor data of an environment around a vehicle;\na computing device including one or more processing devices and one or more memory devices communicatively coupled to the one or more processing devices storing programmed instructions thereon, which when executed by the one or more processing devices causes instantiation of:\na temporal encoder to generate one or more encoded state features each corresponding to an object in an environment;\na spatial encoder to generate an encoded spatial feature from a spatial arrangement, the spatial arrangement including the one or more encoded state features;\na maneuver predictor to determine, based at least in part on the encoded spatial feature and an encoded state feature of the one or more encoded state features, a likelihood of one or more predicted maneuvers of a respective object corresponding to the encoded state feature; and\na trajectory decoder to determine one or more future locations of the respective object based at least in part on the encoded state feature, the encoded spatial feature, and the one or more predicted maneuvers.\n18. The system of claim 17, wherein the spatial encoder uses a long short-term memory (LSTM) network to generate the encoded spatial feature, wherein the LSTM network uses first data representative of the spatial arrangement traversed in a first direction and second data representative of the spatial arrangement traversed in a second direction different from the first direction as input.\n19. The system of claim 17, wherein, for each object in the environment, the temporal encoder uses an instantiation of a long short-term memory (LSTM) network to generate a respective encoded state feature for the object.\n20. The system of claim 17, wherein the encoded state feature corresponds to a first time instance, and a temporal decoder uses the one or more future locations to generate an updated encoded state feature for a second time instance after the first time instance.",
    "status": "Active",
    "citations_own": [
        "WO2016156236A1",
        "US20180053108A1",
        "US20180336466A1",
        "US20190303759A1",
        "US20190384994A1",
        "US20200082248A1",
        "US20200104641A1",
        "US20200327359A1",
        "US20200324794A1",
        "US20200364554A1",
        "US10885698B2",
        "US20210081715A1",
        "US20210103744A1",
        "US11003189B2",
        "US11017550B2",
        "US20210192748A1",
        "US11062141B2",
        "US20210286371A1",
        "US11195418B1",
        "US11200489B2",
        "US20220011122A1",
        "US11256964B2",
        "US11315421B2",
        "US20220144256A1",
        "US20220169278A1"
    ],
    "citations_ftf": [
        "US9760806B1",
        "US10565305B2",
        "CN108022012A",
        "KR20210061461A"
    ],
    "citedby_own": [
        "US20230074293A1"
    ],
    "citedby_ftf": [
        "US9836895B1",
        "JP6728495B2",
        "US10739775B2",
        "US11514293B2",
        "US11155259B2",
        "US11488374B1",
        "EP3929048A1",
        "US10824913B1",
        "US10997729B2",
        "KR20200094378A",
        "JP7200037B2",
        "US11161500B1",
        "EP3751699B1",
        "US11409304B1",
        "US11380108B1",
        "EP3806065A1",
        "US20210142210A1",
        "US11663726B2",
        "US11429107B2",
        "US11643105B2",
        "CN111681335A",
        "US20210295171A1",
        "US11092688B1",
        "CN111259801B",
        "US11364883B2",
        "US11727690B2",
        "JP2021165080A",
        "EP3916676B1",
        "CN111738337B",
        "US20200324794A1",
        "CN112037506B",
        "US20220076431A1",
        "KR20220039903A",
        "US20220097727A1",
        "US11648959B2",
        "US11475577B2",
        "US11753041B2",
        "US11554794B2",
        "CN112752308B",
        "CN113191539B",
        "WO2023280368A1",
        "US20230057100A1",
        "US11780471B1",
        "EP4148456A1",
        "US20230084262A1",
        "WO2023044356A1",
        "CN113643542A",
        "CN114237237A",
        "EP4202753A1",
        "EP4254267A1",
        "CN114881111B",
        "CN114926772B",
        "CN115293297B",
        "CN115629401B"
    ]
}