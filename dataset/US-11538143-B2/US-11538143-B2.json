{
    "patent_id": "US-11538143-B2",
    "title": "Fully convolutional transformer based generative adversarial networks ",
    "assignee": "Nec Corporation",
    "publication_date": "2022-12-27",
    "patent_link": "https://patents.google.com/patent/US11538143B2/en",
    "inventors": [
        "Dongjin Song",
        "Yuncong Chen",
        "Haifeng Chen",
        "Xinyang Feng"
    ],
    "classifications": [
        "G06N3/08",
        "G06F18/22",
        "G06F18/2433",
        "G06K9/6215",
        "G06N20/10",
        "G06N20/20",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/047",
        "G06N3/084",
        "G06N5/046",
        "G06T7/0002",
        "G06V10/454",
        "G06V10/764",
        "G06V10/82",
        "G06V20/46",
        "G06T2207/20081",
        "G06T2207/20084"
    ],
    "abstract": "Systems and methods for detecting anomaly in video data are provided. The system includes a generator that receives past video frames and extracts spatio-temporal features of the past video frames and generates frames. The generator includes fully convolutional transformer based generative adversarial networks (FCT-GANs). The system includes an image discriminator that discriminates generated frames and real frames. The system also includes a video discriminator that discriminates generated video and real video. The generator trains a fully convolutional transformer network (FCTN) model and determines an anomaly score of at least one test video based on a prediction residual map from the FCTN model.",
    "claims": "\n1. A system for detecting anomaly in video data, comprising:\na generator to receive past video frames and to extract spatio-temporal features of the past video frames and generate at least one frame, wherein the generator includes fully convolutional transformer based generative adversarial networks (FCT-GANs);\nan image discriminator configured to discriminate at least one generated frame and at least one real frame of the past video frames; and\na video discriminator configured to discriminate at least one generated video and at least one real video, wherein the at least one real video includes, the past video frames and at least one future frame,\nWherein the generator is further configured to train a fully convolutional transformer network (FCTN) model and determine an anomaly score of at least one test video based on a prediction residual map from the FCTN model.\n2. The system as recited in claim 1, wherein the system is further configured to:\npre-process the past video frames to obtain an optical flow of each past, video frame.\n3. The system as recited in claim 1, wherein, when training the FCTN model, the generator is further configured to:\ntrain the FCTN model based on a generative adversarial network (GAN) protocol.\n4. The system as recited in claim 1, wherein the image discriminator and the video discriminator are further configured to add adversarial loss on both the at least one generated frame and the at least one generated video.\n5. The system as recited in claim 1, wherein the generator farther comprises at least one convolutional transformer.\n6. The system as recited in claim 5, wherein the convolutional transformer is further configured to:\ntransform at least one feature map to multi-head via a convolutional operation.\n7. The system as recited in claim 6, wherein the convolutional transformer is further configured to:\nfor each head of the multi-head, apply a global average pooling to aggregate over spatial dimension and concatenate a positional encoding (PE) vector.\n8. The system as recited in claim 6, wherein the convolutional transformer is further configured to:\ncompare a similarity between at least one query feature vector and at least one memory vector to generate attention weights by a softmax function.\n9. The system as recited in claim 8, wherein the convolutional transformer is further configured to:\ndetermine at least one attended feature map as a weighted average of the at least one feature map at different time steps; and\ndetermine a final attended map as a concatenation over all heads.\n10. The system as recited in claim 9, wherein the convolutional transformer is further configured to:\ndetermine an attended feature map as a weighted average of the at least one query feature vector and the at least one attended feature map via a spatial selection gate.\n11. A method for detecting anomaly in video data, comprising:\nreceiving, at a generator, past video frames, Wherein the generator includes fully convolutional transformer based generative adversarial networks (FCT-GANs)\nextracting, by the generator, spatio-temporal features of the past video frames and generating at least one frame;\ndiscriminating, by an image discriminator, at least one generated frame and at least one real frame of the past video frames;\ndiscriminating, by a video discriminator, at least one generated video and at least one real video, wherein the at least one real video includes the past video frames and at least one future frame;\ntraining a fully convolutional transformer network (FCTN) model; and\ndetermining an anomaly score of at least one test video based on a prediction residual map from the FCTN model.\n12. The method as recited in claim 11, further comprising:\npre-processing the past video frames to obtain an optical flow of each past video frame.\n13. The method as recited in claim 11, further comprising:\ntraining the FCTN model based on a generative adversarial network (GANs) protocol.\n14. The method as recited in claim 11, further comprising:\nadding adversarial loss on both the at least one generated frame and the at least one generated video.\n15. The method as recited in claim 11, wherein the generator further comprises at least one convolutional transformer, further comprising:\ntransforming at least one feature map to multi-head via a convolutional operation.\n16. The method as recited in claim 15, further comprising;\nfor each head of the multi-head, applying a global average pooling to aggregate over spatial dimension and concatenate a positional encoding (PE) vector.\n17. The method as recited in claim 15, further comprising:\ncomparing a similarity between at least one query feature vector and at least one memory vector to generate attention weights by a softmax function.\n18. The method as recited in claim 17, further comprising:\ndetermining at least one attended feature map as a weighted average of the at least one feature map at different time steps; and\ndetermining a final attended map as a concatenation over heads.\n19. The method as recited in claim 18, further comprising:\ndetermining an attended feature map as a weighted average of the at least one query feature vector and the at least one attended feature map via a spatial selection gate.\n20. A computer program product for detecting anomaly in video data, the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computing device to cause the computing device to perform the method comprising:\nreceiving, at a generator, past video frames, wherein the generator includes fill r convolutional transformer based generative adversarial networks (FCT-GANs);\nextracting, by the generator, spatio-temporal features of the past video frames and generating at least one frame;\ndiscriminating, by an image discriminator, at least one generated frame and at least one real frame of the past video frames;\ndiscriminating, by a video discriminator, at least one generated video and at least one real video, wherein the at least one real video includes the past video frames and at least one future frame;\ntraining a fully convolutional transformer network (FCTN) model; and\ndetermining an anomaly score of at least one test video based on a prediction residual map from the model.",
    "status": "Active",
    "citations_own": [
        "US20170345140A1",
        "US10121104B1",
        "US20180322366A1",
        "US20190057515A1",
        "US20190114748A1",
        "US20190354629A1",
        "US20190377047A1",
        "US20190385302A1",
        "US20200021718A1",
        "US10839506B1",
        "US10846888B2",
        "US10896535B2",
        "US20210249142A1"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US20220132050A1",
        "US20220134546A1"
    ],
    "citedby_ftf": [
        "US11011154B2",
        "US11254331B2",
        "CN111639563B",
        "EP3913539A1",
        "CN111814548B",
        "CN111814622B",
        "CN111836063B",
        "CN111832516B",
        "CN112101543A",
        "CN112016403B",
        "CN112052763A",
        "CN112184548A",
        "CN112215868A",
        "CN112183826B",
        "CN112085735B",
        "US11670072B2",
        "US11636682B2",
        "CN112347362B",
        "WO2022115100A1",
        "CN112309551B",
        "CN112637621B",
        "CN112957052B",
        "CN112784768A",
        "CN112528975A",
        "CN112560827B",
        "CN113011567B",
        "CN113761282B",
        "CN113392728B",
        "CN113313028A",
        "CN113313037A",
        "CN113469331A",
        "CN113505664B",
        "CN113704511B",
        "CN113792862B",
        "CN113793333B",
        "CN114463670B",
        "CN114495958B",
        "CN114795178B",
        "CN114937021A",
        "US11727673B1",
        "CN114760477B",
        "CN115098345B",
        "CN116486464B"
    ]
}