{
    "patent_id": "US-2018357225-A1",
    "title": "Method for generating chatting data based on artificial intelligence, computer device and computer-readable storage medium ",
    "assignee": "Beijing Baidu Netcom Science And Technology Co., Ltd.",
    "publication_date": "2018-12-13",
    "patent_link": "https://patents.google.com/patent/US20180357225A1/en",
    "inventors": [
        "Yi Liu",
        "Daxiang DONG",
        "Dianhai YU"
    ],
    "classifications": [
        "G06F40/56",
        "G06F17/2881",
        "G06F16/3329",
        "G06F17/18",
        "G06F17/2765",
        "G06F17/2845",
        "G06F40/279",
        "G06F40/284",
        "G06F40/289",
        "G06F40/30",
        "G06F40/35",
        "G06F40/49",
        "G06N3/006",
        "G06N3/044",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/08",
        "H04L51/02",
        "G06F16/35",
        "G06F16/5846",
        "G06F2216/03",
        "G06N5/041"
    ],
    "abstract": "Embodiments of the present disclosure relate to a method for generating chatting data based on AI, a computer device and a computer-readable storage medium. The method includes: converting chatting data inputted by a user into an input word sequence; converting a tag of the user into a tag word sequence; based on a preset encoding-decoding model with an attention model, predicting according to the input word sequence and the tag word sequence to obtain a target word sequence; and converting the target word sequence into reply data of the chatting data.",
    "claims": "\n1. A method for generating chatting data based on artificial intelligence, comprising:\nconverting, with one or more processors, chatting data inputted by a user on a device into an input word sequence;\nconverting, with the one or more processors, a tag of the user into a tag word sequence;\nbased on a preset encoding-decoding model with an attention model, predicting, with the one or more processors, according to the input word sequence and the tag word sequence to obtain a target word sequence; and\nconverting, with the one or more processors, the target word sequence into reply data of the chatting data.\n2. The method according to claim 1, wherein based on the preset encoding-decoding model with the attention model, predicting according to the input word sequence and the tag word sequence to obtain the target word sequence, comprises:\nencoding the input word sequence to obtain a first corpus sequence;\nencoding the tag word sequence to obtain a reference corpus sequence;\ndetermining a preset word in the first corpus sequence as an initial word;\ndetermining first context information corresponding to the initial word based on the first corpus sequence;\ndetermining second context information corresponding to the initial word based on the reference corpus sequence;\ndetermining a target word based on the initial word, the first context information and the second context information;\nwhen the target word does not meet a cut-off condition, iterating by using the target word as the initial word to return acts of determining the first context information and determining the second context information.\n3. The method according to claim 2, wherein\ndetermining the first context information corresponding to the initial word based on the first corpus sequence, comprises: determining first weight information corresponding to the initial word, and performing weighted sum on the first corpus sequence based on the first weight information;\ndetermining the second context information corresponding to the initial word based on the reference corpus sequence, comprises: determining second weight information corresponding to the initial word, and performing weighted sum on the reference corpus sequence based on the second weight information.\n4. The method according to claim 2, wherein determining the target word based on the initial word, the first context information and the second context information, comprises:\ninputting the initial word, the first context information and the second context information into the attention model to obtain hidden layer information;\nprojecting the hidden layer information into a word list space by linear transformation;\npredicting a probability that a first word in the word list space is a next word, the first word being any one word in the word list space;\ndetermining the target word based on the probability.\n5. The method according to claim 2, wherein the cut-off condition comprises:\nthe target word being a preset ending word; and/or,\nthe number of the target words that have been obtained reaching a preset length.\n6. The method according to claim 4, further comprising:\nobtaining, with the one or more processors, a plurality of conversation records;\nperforming, with the one or more processors, word segmentation on each of sentences in the plurality of conversation records to obtain a plurality of segmentation words;\nobtaining, with the one or more processors, a probability that each of the plurality of segmentation words appears; and\ndetermining, with the one or more processors, the word list space based on the probability that each of the plurality of segmentation words appears.\n7. A computer device, comprising:\na processor;\na memory; and\ncomputer programs stored in the memory and executable by the processor,\nwherein the processor is configured to execute the computer programs to perform acts of:\nconverting chatting data inputted by a user into an input word sequence;\nconverting a tag of the user into a tag word sequence;\nbased on a preset encoding-decoding model with an attention model, predicting according to the input word sequence and the tag word sequence to obtain a target word sequence; and\nconverting the target word sequence into reply data of the chatting data.\n8. The computer device according to claim 7, wherein the processor is configured to, based on the preset encoding-decoding model with the attention model, predict according to the input word sequence and the tag word sequence to obtain the target word sequence by acts of:\nencoding the input word sequence to obtain a first corpus sequence;\nencoding the tag word sequence to obtain a reference corpus sequence;\ndetermining a preset word in the first corpus sequence as an initial word;\ndetermining first context information corresponding to the initial word based on the first corpus sequence;\ndetermining second context information corresponding to the initial word based on the reference corpus sequence;\ndetermining a target word based on the initial word, the first context information and the second context information;\nwhen the target word does not meet a cut-off condition, iterating by using the target word as the initial word to return acts of determining the first context information and determining the second context information.\n9. The computer device according to claim 8,\nwherein the processor is configured to determine the first context information corresponding to the initial word based on the first corpus sequence by acts of: determining first weight information corresponding to the initial word, and performing weighted sum on the first corpus sequence based on the first weight information;\nwherein the processor is configured to determine the second context information corresponding to the initial word based on the reference corpus sequence by acts of: determining second weight information corresponding to the initial word, and performing weighted sum on the reference corpus sequence based on the second weight information.\n10. The computer device according to claim 8, wherein the processor is configured to determine the target word based on the initial word, the first context information and the second context information by acts of:\ninputting the initial word, the first context information and the second context information into the attention model to obtain hidden layer information;\nprojecting the hidden layer information into a word list space by linear transformation;\npredicting a probability that a first word in the word list space is a next word, the first word being any one word in the word list space;\ndetermining the target word based on the probability.\n11. The computer device according to claim 8, wherein the cut-off condition comprises:\nthe target word being a preset ending word; and/or,\nthe number of the target words that have been obtained reaching a preset length.\n12. The computer device according to claim 10, wherein the processor is further configured to perform acts of:\nobtaining a plurality of conversation records;\nperforming word segmentation on each of sentences in the plurality of conversation records to obtain a plurality of segmentation words;\nobtaining a probability that each of the plurality of segmentation words appears; and\ndetermining the word list space based on the probability that each of the plurality of segmentation words appears.\n13. A non-transitory computer-readable storage medium, having computer programs stored therein, wherein when the computer programs are executed by a processor, a method for generating chatting data based on artificial intelligence is realized, the method comprising:\nconverting chatting data inputted by a user into an input word sequence;\nconverting a tag of the user into a tag word sequence;\nbased on a preset encoding-decoding model with an attention model, predicting according to the input word sequence and the tag word sequence to obtain a target word sequence; and\nconverting the target word sequence into reply data of the chatting data.\n14. The non-transitory computer-readable storage medium according to claim 13, wherein based on the preset encoding-decoding model with the attention model, predicting according to the input word sequence and the tag word sequence to obtain the target word sequence, comprises:\nencoding the input word sequence to obtain a first corpus sequence;\nencoding the tag word sequence to obtain a reference corpus sequence;\ndetermining a preset word in the first corpus sequence as an initial word;\ndetermining first context information corresponding to the initial word based on the first corpus sequence;\ndetermining second context information corresponding to the initial word based on the reference corpus sequence;\ndetermining a target word based on the initial word, the first context information and the second context information;\nwhen the target word does not meet a cut-off condition, iterating by using the target word as the initial word to return acts of determining the first context information and determining the second context information.\n15. The non-transitory computer-readable storage medium according to claim 14, wherein\ndetermining the first context information corresponding to the initial word based on the first corpus sequence, comprises: determining first weight information corresponding to the initial term, and performing weighted sum on the first corpus sequence based on the first weight information;\ndetermining the second context information corresponding to the initial word based on the reference corpus sequence, comprises: determining second weight information corresponding to the initial word, and performing weighted sum on the reference corpus sequence based on the second weight information.\n16. The non-transitory computer-readable storage medium according to claim 14, wherein determining the target word based on the initial word, the first context information and the second context information, comprises:\ninputting the initial word, the first context information and the second context information into the attention model to obtain hidden layer information;\nprojecting the hidden layer information into a word list space by linear transformation;\npredicting a probability that a first word in the word list space is a next word, the first word being any one word in the word list space;\ndetermining the target word based on the probability.\n17. The non-transitory computer-readable storage medium according to claim 14, wherein the cut-off condition comprises:\nthe target word being a preset ending word; and/or,\nthe number of the target words that have been obtained reaching a preset length.\n18. The non-transitory computer-readable storage medium according to claim 16, wherein the method further comprises:\nobtaining a plurality of conversation records;\nperforming word segmentation on each of sentences in the plurality of conversation records to obtain a plurality of segmentation words;\nobtaining a probability that each of the plurality of segmentation words appears; and\ndetermining the word list space based on the probability that each of the plurality of segmentation words appears.",
    "status": "Active",
    "citations_own": [
        "US20030074368A1",
        "US20190341021A1"
    ],
    "citations_ftf": [
        "CN104598445B",
        "US20160350658A1",
        "CN105512228B",
        "CN105955964B",
        "CN106126596B"
    ],
    "citedby_own": [
        "US20190057684A1",
        "CN109726397A",
        "CN109815482A",
        "CN110008476A",
        "CN110147532A",
        "CN110147533A",
        "CN110263131A",
        "CN110737764A",
        "CN111435411A",
        "CN111553152A",
        "CN111625715A",
        "CN113312448A",
        "CN114124860A",
        "CN116188648A"
    ],
    "citedby_ftf": [
        "CN107862058B",
        "CN108364066B",
        "CN108062388B",
        "CN110134971B",
        "CN108491514B",
        "CN109255020B",
        "CN109657890B",
        "CN109299231B",
        "CN110943904A",
        "CN109547323B",
        "CN111125506B",
        "CN110413788B",
        "CN110826340A",
        "CN111125324B",
        "CN111241263A",
        "US11736423B2"
    ]
}