{
    "patent_id": "US-11270190-B2",
    "title": "Method and apparatus for generating target neural network structure, electronic device, and storage medium ",
    "assignee": "Beijing Sensetime Technology Development Co., Ltd.",
    "publication_date": "2022-03-08",
    "patent_link": "https://patents.google.com/patent/US11270190B2/en",
    "inventors": [
        "Zhao ZHONG",
        "Junjie Yan",
        "Chenglin Liu"
    ],
    "classifications": [
        "G06N3/0472",
        "G06N3/082",
        "G06N3/047",
        "G06N3/048",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/084",
        "G06N3/006"
    ],
    "abstract": "Embodiments of the present application disclose a method and apparatus for generating a neural network structure, an electronic device, and a storage medium. The method comprises: sampling a neural network structure to generate a network block, the network block comprising at least one network layer; constructing a sampling neural network based on the network block; training the sampling neural network based on sample data, and obtaining an accuracy corresponding to the sampling neural network; and in response to that the accuracy does not meet a preset condition, regenerating a new network block according to the accuracy until a sampling neural network constructed by the new network block meets the preset condition, and using the sampling neural network meeting the preset condition as a target neural network.",
    "claims": "\n1. A method for generating a target neural network structure, comprising:\nsampling a neural network structure to generate a network block, the network block comprising at least one network layer;\nconstructing a sampling neural network based on the network block;\ntraining the sampling neural network based on sample data, and obtaining an accuracy corresponding to the sampling neural network; and\nin response to that the accuracy does not meet a preset condition, regenerating a new network block according to the accuracy until a sampling neural network constructed by the new network block meets the preset condition, and using the sampling neural network meeting the preset condition as a target neural network.\n2. The method according to claim 1, wherein the sampling the neural network structure to generate a network block comprises:\nsampling the neural network structure based on a probability vector to obtain at least one type of network layers, network parameters corresponding to the at least one type of network layers, and a connection mode between the at least one type of network layers; and\nstacking the at least one type of network layers to obtain a network block.\n3. The method according to claim 1, wherein the at least one network layer comprises at least one of:\na convolution layer, a max pooling layer, an average pooling layer, an identity layer, an elemental add layer, or a concat layer.\n4. The method according to claim 2, wherein the stacking the at least one type of network layers to obtain a network block comprises:\nanalyzing the network layers obtained by sampling to obtain at least one type of the network layers, a number of at least one type of the network layers, and a connection sequence of at least one type of the network layers; and\nconnecting the obtained network layers, according to the connection sequence corresponding to the at least one type of network layers through a connection relationship between the at least one type of network layers to obtain a network block.\n5. The method according to claim 1, wherein the regenerating a new network block according to the accuracy comprises:\nadjusting a probability vector of sampling the neural network structure according to the accuracy corresponding to the sampling neural network, and sampling the neural network structure through the adjusted probability vector, to generate a new network block.\n6. The method according to claim 1, wherein the training the sampling neural network based on sample data comprises:\nprocessing the sample data by using the sampling neural network to obtain a processing result;\ncalculating an error value between supervision information marked in the sample data and the processing result by using a loss function, the sample data being marked with the supervision information; and\ntraining the sampling neural network based on the error value until the sampling neural network meets a convergence condition.\n7. The method according to claim 6, wherein the training the sampling neural network based on the error value until the sampling neural network meets a convergence condition comprises:\nin response to that a convergence rate of the loss function is greater than or equal to a preset value, adjusting parameters in the sampling neural network through a reverse gradient algorithm according to the obtained error value to obtain an adjusted sampling neural network until the convergence rate of the loss function corresponding to the adjusted neural network is smaller than the preset value.\n8. The method according to claim 6, wherein the training the sampling neural network based on the error value until the sampling neural network meets a convergence condition comprises:\nin response to a number of times of calculating the error value using the loss function being smaller than a preset value, adjusting parameters in the sampling neural network through a reverse gradient algorithm according to the obtained error value to obtain an adjusted sampling neural network, and adding the number of times of calculating the error value using the loss function by 1 until the number of times of calculating the error value using the loss function corresponding to the adjusted neural network is greater than or equal to the preset value.\n9. The method according to claim 6, wherein the obtaining an accuracy corresponding to the sampling neural network comprises:\ncalculating a predictive accuracy of the sampling neural network according to the error value of the trained sampling neural network, wherein the calculating further comprises subtracting a preset proportion of a network computing complexity measurement and a preset proportion of a network density measurement based on the predictive accuracy to obtain an accuracy corresponding to the sampling neural network, the network computing complexity and the network density corresponding to the sampling neural network,\nwherein the network computing complexity measurement comprises a number of floating-point operations per second, and the network density measurement comprises a result of dividing a number of edges of a mathematical map by a number of points of the mathematical map.\n10. The method according to claim 1, wherein the sampling the neural network structure to generate a network block comprises:\nsampling the neural network structure to generate N network blocks, N being an integer greater than zero;\nthe constructing a sampling neural network based on the network block comprises:\nrespectively constructing N sampling neural networks based on the N network blocks;\nthe training the sampling neural network based on sample data, and obtaining an accuracy corresponding to the sampling neural network comprise:\nrespectively training the N sampling neural networks based on sample data, and obtaining N accuracies corresponding to the N sampling neural networks; and\nwherein in response to determining that a respective accuracy of the N accuracies does not meet a preset condition, regenerating a new network block according to the respective accuracy until a sampling neural network constructed by the new network block meets the preset condition, and using the sampling neural network meeting the preset condition as a target neural network comprises:\nin response to that there is no accuracy among the N accuracies that meets a preset condition, respectively regenerating N new network blocks according to the N accuracies until there is a neural network among the N sampling neural networks respectively constructed by the N new network blocks that meets the preset condition, and using the sampling neural network meeting the preset condition as a target neural network.\n11. The method according to claim 1, wherein the regenerating a new network block according to the accuracy comprises:\nsampling the neural network structure again to generate the new network block;\nwherein after obtaining an accuracy corresponding to the sampling neural network, the method further comprises:\nin response to that the accuracy meets the preset condition, using the sampling neural network meeting the preset condition as the target neural network;\nor,\nin response to that the accuracy does not meet the preset condition and the sampling for the neural network structure has reached a preset number of times, using a current sampling neural network as the target neural network.\n12. An apparatus for generating a target neural network structure, comprising:\na processor; and\na memory storing instructions, the instructions when executed by the processor, cause the processor to perform operations, the operations comprising:\nsampling a neural network structure to generate a network block, the network block comprising at least one network layer;\nconstructing a sampling neural network based on the network block;\ntraining the sampling neural network based on sample data, and obtaining an accuracy corresponding to the sampling neural network; and\nin response to that the accuracy does not meet a preset condition, regenerating a new network block according to the accuracy until a sampling neural network constructed by the new network block meets the preset condition, and using the sampling neural network meeting the preset condition as a target neural network.\n13. The apparatus according to claim 12, wherein the sampling the neural network structure to generate a network block comprises:\nsampling a neural network structure based on a probability vector to obtain at least one type of network layers, network parameters corresponding to the at least one type of network layers, and a connection mode between the at least one type of network layers; and\nstacking the at least one type of network layers to obtain a network block.\n14. The apparatus according to claim 13, wherein the stacking the at least one type of network layers to obtain a network block comprises:\nanalyzing the network layers obtained by sampling to obtain at least one type of the network layers, a number of at least one type of the network layers, and a connection sequence of at least one type of the network layers; and\nconnecting the obtained network layers according to the connection sequence corresponding to the at least one type of network layers through a connection relationship between the at least one type of network layers to obtain a network block.\n15. The apparatus according to claim 12, wherein the regenerating a new network block according to the accuracy comprises:\nadjusting a probability vector of sampling the neural network structure according to the accuracy corresponding to the sampling neural network, and sampling the neural network structure through the adjusted probability vector, to generate a new network block.\n16. The apparatus according to claim 12, wherein the training the sampling neural network based on sample data comprises:\nprocessing the sample data by using the sampling neural network to obtain a processing result;\ncalculating an error value between supervision information marked in the sample data and the processing result by using a loss function, the sample data being marked with the supervision information; and\ntraining the sampling neural network based on the error value until the sampling neural network meets a convergence condition.\n17. The apparatus according to claim 16, wherein the training the sampling neural network based on the error value until the sampling neural network meets a convergence condition comprises:\nin response to that a convergence rate of the loss function is greater than or equal to a preset value, adjusting parameters in the sampling neural network through a reverse gradient algorithm according to the obtained error value to obtain an adjusted sampling neural network until the convergence rate of the loss function corresponding to the adjusted neural network is smaller than the preset value; or\nin response to a number of times of calculating the error value using the loss function being than a preset value, adjusting parameters in the sampling neural network through a reverse gradient algorithm according to the obtained error value to obtain an adjusted sampling neural network, and add the number of times of calculating the error value using the loss function by 1 until the number of times of calculating the error value using the loss function corresponding to the adjusted neural network is greater than or equal to the preset value.\n18. The apparatus according to claim 16, wherein the obtaining an accuracy corresponding to the sampling neural network comprises:\ncalculating a predictive accuracy of the sampling neural network according to the error value of the trained sampling neural network, wherein the calculating further comprises subtracting a preset proportion of a network computing complexity measurement and a preset proportion of a network density measurement based on the predictive accuracy to obtain an accuracy corresponding to the sampling neural network, the network computing complexity and the network density corresponding to the sampling neural network,\nwherein the network computing complexity measurement comprises a number of floating-point operations per second, and the network density measurement comprises a result of dividing a number of edges of a mathematical map by a number of points of the mathematical map.\n19. The apparatus according to claim 12, wherein the regenerating a new network block according to the accuracy comprises:\nsampling the neural network structure again to generate the new network block;\nwherein the operations further comprising:\nin response to that the accuracy meets the preset condition, using the sampling neural network meeting the preset condition as the target neural network;\nor,\nin response to that the accuracy does not meet the preset condition and the sampling for the neural network structure has reached a preset number of times, using a current sampling neural network as a target neural network.\n20. A non-transitory computer storage medium for storing computer readable instructions, the computer readable instruction when executed by a processor, causes the processor to perform operations, the operations comprising:\nsampling a neural network structure to generate a network block, the network block comprising at least one network layer;\nconstructing a sampling neural network based on the network block;\ntraining the sampling neural network based on sample data, and obtaining an accuracy corresponding to the sampling neural network; and\nin response to that the accuracy does not meet a preset condition, regenerating a new network block according to the accuracy until a sampling neural network constructed by the new network block meets the preset condition, and using the sampling neural network meeting the preset condition as a target neural network.",
    "status": "Active",
    "citations_own": [
        "JPH049566A",
        "JPH04353963A",
        "JPH087483A",
        "CN101726742A",
        "JP2015011510A",
        "US20150100530A1",
        "CN105701540A",
        "CN106203330A",
        "US20170140272A1",
        "CN106778902A",
        "US20180114110A1",
        "CN108229647A",
        "KR20180089769A",
        "CN109902186A"
    ],
    "citations_ftf": [
        "JPH0887483A",
        "JP3315890B2"
    ],
    "citedby_own": [],
    "citedby_ftf": [
        "WO2018033137A1",
        "US10678244B2",
        "US11409692B2",
        "US11157441B2",
        "US10671349B2",
        "CN108229647A",
        "US11561791B2",
        "US11215999B2",
        "US11636333B2",
        "US11562231B2",
        "US11196678B2",
        "CN109635920B",
        "CN109598332B",
        "US11537811B2",
        "CN109359727B",
        "CN111325311A",
        "US11610117B2",
        "CN109800807B",
        "US10997461B2",
        "US11567514B2",
        "US10956755B2",
        "CN109948795B",
        "CN110070120B",
        "CN110070029B",
        "CN110110861B",
        "CN110147883B",
        "CN111684472A",
        "CN110390385B",
        "CN110647990A",
        "CN111105031B",
        "CN112990461B",
        "CN111222637A",
        "CN111325343B",
        "CN111582474B",
        "CN111783937A",
        "CN113723603A",
        "CN113873539A",
        "WO2022126448A1",
        "KR102455681B1",
        "CN112818788B",
        "CN113344181B",
        "CN113869496A"
    ]
}