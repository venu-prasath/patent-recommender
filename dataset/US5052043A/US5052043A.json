{
    "patent_link": "https://patents.google.com/patent/US5052043A/en",
    "patent_id": "US5052043A",
    "title": "Neural network with back propagation controlled through an output confidence measure",
    "abstract": "Apparatus, and an accompanying method, for a neural network, particularly one suited for use in optical character recognition (OCR) systems, which through controlling back propagation and adjustment of neural weight and bias values through an output confidence measure, smoothly, rapidly and accurately adapts its response to actual changing input data (characters). Specifically, the results of appropriate actual unknown input characters, which have been recognized with an output confidence measure that lies within a pre-defined range, are used to adaptively re-train the network during pattern recognition. By limiting the maximum value of the output confidence measure at which this re-training will occur, the network re-trains itself only when the input characters have changed by a sufficient margin from initial training data such that this re-training is likely to produce a subsequent noticeable increase in the recognition accuracy provided by the network. Output confidence is measured as a ratio between the highest and next highest values produced by output neurons in the network. By broadening the entire base of training data to include actual dynamically changing input characters, the inventive neural network provides more robust performance than which heretofore occurs in neural networks known in the art.",
    "inventors": [
        "Roger S. Gaborski"
    ],
    "assignee": "Eastman Kodak Co",
    "classifications": [
        "G06N3/084",
        "G06V30/194"
    ],
    "claims": "\n1. Apparatus for recognizing the existence of a plurality of patterns in unknown input data comprising:\nnetwork means, responsive to unknown input data, for producing a plurality of output values that collectively identifies one of said patterns existing in said unknown input data, said network means having a plurality of numeric weights associated therewith;\nmeans, responsive to a control signal and to said plurality of output values and pre-defined output target values associated therewith for said one pattern, for determining changes in the value of each of said numeric weights and, in response thereto, for adjusting a value of each of said weights while said unknown input data is applied to said network means so as reduce error arising between said output values and said output target values for the unknown input data;\nmeans, responsive to said plurality of output values, for determining a confidence measure associated therewith; and\nmeans, responsive to said confidence measure, for generating said control signal so as to enable said determining and adjusting means during pattern recognition to determine said changes and adjust the values of said weights when said confidence measure has a numeric value lying within a pre-defined numeric range so that network training does not occur during pattern recognition whenenver the confidence measure associated with the output values produced for said one pattern is either greater or less than said numeric range whereby said network means is trained to recognize said pattern in the event the pattern deviates from a pre-defined pattern.\n2. The apparatus in claim 1 wherein said network means comprises a network having an inter-connected hierarchy of neurons responsive to said unknown input data for recognizing the patterns therein and, in response thereto, providing said output values; said hierarchy having input, hidden and output layers formed of different ones of said neurons with pre-defined pairs of said neurons being inter-connected with a corresponding one of said numeric weights associated therewith.\n3. The apparatus in claim 2 wherein each of said neurons has a neural output and a neural input and provides a neural output value as a thresholded function of the neural input, and wherein each of said neurons in said hidden and output layers comprises:\nmeans, responsive to a plurality of neural input signals, for multiplying each of said neural input signals by a corresponding one of said numeric weights to form weighted neural input signals;\nmeans, responsive to said weighted neural input signals, for forming a sum of said neural input signals; and\nmeans for thresholding said weighted sum by a pre-defined continuous threshold function to provide the neural output.\n4. The apparatus in claim 3 wherein said unknown input data comprises a plurality of input values, each of said input values being applied as a neural input signal to an associated and different one of said neurons in said input layer; and said plurality of output values are collectively formed of the neural output values produced by all of said neurons in the output layer.\n5. The apparatus in claim 4 wherein the neural output of each one of said neurons in said input layer is connected to one of the neural inputs of every one of said neurons in said hidden layer and the neural output of each of said neurons in said hidden layer is connected to one of the neural inputs of every one of said neurons in said output layer.\n6. The apparatus in claim 5 wherein said unknown input data is a bit-map having a pre-defined number of pixels, wherein a value of each of said pixels being applied as the neural input to an associated and different one of said neurons in said input layer.\n7. The apparatus in claim 3 wherein said continuous threshold function is a sigmoid function.\n8. The apparatus in claim 3 wherein said sum comprises a bias value in addition to said weighted neural input signals, and wherein said determining and adjusting means, in response to said plurality of said output values and said pre-defined target values associated therewith, determines a change in the bias value for each of said neurons and, in response thereto, changes the bias value while said unknown input data is applied to said network means so as to reduce said error.\n9. The apparatus in claim 1 further comprising training means, responsive to a pre-defined known input pattern and having pre-defined ones of neural output values associated therewith, for simultaneously applying said known pattern as said unknown input data and said pre-defined neural output values as said target output values, and for enabling said determining and adjusting means during a training period to determine said changes and adjust the values of said weights to minimize error arising between said output values and said output target values whereby said network means is trained to initially recognize said known pattern in said unknown input data.\n10. The apparatus in claim 9 wherein said training means successively applies each of a plurality of pre-defined known input patterns as said unknown input data simultaneously with associated pre-defined neural output values as said target output values and enables said determining and adjusting means to determine said changes and adjust the values of said weights to minimize the error arising between said output values and said output target values for each of said known patterns whereby said network means is trained to initially recognize each of said known patterns in said unknown input data.\n11. The apparatus in claim 10 wherein said network means further contains a plurality of numeric bias values associated therewith, and said determining and adjusting means, in response to said plurality of said output values and said pre-defined target values associated therewith, determines changes in the values of the weights and the bias values and, in response thereto, adjusts the values of said weights and the bias values while said unknown input data is applied to said network means so as to reduce said error arising between said output values and said output target values for each of said known input patterns.\n12. The apparatus in claim 11 wherein said network means comprises a network having an inter-connected hierarchy of neurons responsive to said unknown input data for recognizing the patterns therein and, in response thereto, providing said output values; said hierarchy having input, hidden and output layers formed of different ones of said neurons with pre-defined pairs of said neurons being inter-connected with a corresponding one of said numeric weights associated therewith; and wherein each of said neurons has a neural output and a neural input and provides a neural output value as a thresholded function of the neural input, and wherein each of said neurons in said hidden and output layers comprises:\nmeans, responsive to a plurality of neural input signals, for multiplying each of said neural input signals by a corresponding one of said numeric weights to form weighted neural input signals;\nmeans, responsive to said weighted neural input signals and an associated one of bias values for said neuron, for forming a sum of said neural input signals and said associated bias value; and\nmeans for thresholding said weighted sum by a pre-defined sigmoid function to provide the neural output.\n13. In an optical character reader, apparatus for recognizing the existence of a plurality of bit-mapped alphanumeric characters in unknown bit-mapped input data comprising:\nnetwork means, responsive to unknown bit-mapped input data, for producing a plurality of output values that collectively identifies one of said bit-mapped patterns existing in said unknown bit-mapped input data, said network means having a plurality of numeric weights associated therewith;\nmeans, responsive to a control signal and to said plurality of output values and pre-defined output target values associated therewith for said one bit-mapped pattern, for determining changes in the value of each of said numeric weights and, in response thereto, for adjusting a value of each of said weights while said unknown bit-mapped input data is applied to said network means so as reduce error arising between said output values and said output target values for the unknown bit-mapped input data;\nmeans, responsive to said plurality of output values, for determining a confidence measure associated therewith; and\nmeans, responsive to said confidence measure, for generating said control signal so as to enable said determining and adjusting means during pattern recognition to determine said changes and adjust the values of said weights when said confidence measure has a numeric value lying within a pre-defined numeric range so that network training does not occur during pattern recognition whenever the confidence measure associated with the output values produced for said one pattern is either greater or less than said numeric range whereby said network means is trained to recognize said bit-mapped pattern in the event the pattern deviates from a pre-defined bit-mapped pattern for an associated alphanumeric character.\n14. The apparatus in claim 13 wherein said network means comprises a network having an inter-connected hierarchy of neurons responsive to said unknown input data for recognizing the bit-mapped patterns therein and, in response thereto, providing said output values; said hierarchy having input, hidden and output layers formed of different ones of said neurons with pre-defined pairs of said neurons being inter-connected with a corresponding one of said numeric weights associated therewith; and wherein each of said neurons has a neural output and a neural input and provides a neural output value as a thresholded function of the neural input, and wherein each of said neurons in said hidden and output layers comprises:\nmeans, responsive to a plurality of neural input signals, for multiplying each of said neural input signals by a corresponding one of said numeric weights to form weighted neural input signals;\nmeans, responsive to said weighted neural input signals, for forming a sum of said neural input signals; and\nmeans for thresholding said weighted sum by a pre-defined continuous threshold function to provide the neural output.\n15. The apparatus in claim 14 wherein a value of each pixel in an input bit-map is applied as the neural input to an associated and different one of said neurons in said input layer, and said output values are collectively formed of the neural output values produced by all of said neurons in the output layer.\n16. The apparatus in claim 15 further comprising training means, responsive to a pre-defined known bit-mapped pattern for a corresponding known alphanumeric character and having pre-defined ones of neural output values associated therewith, for simultaneously applying said known bit-mapped pattern as said unknown bit-mapped input data and said pre-defined neural output values as said target output values, and for enabling said determining and adjusting means to determine said changes and adjust the values of said weights to minimize error arising between said output values and said output target values whereby said network means is trained to initially recognize said known character in said unknown input data.\n17. The apparatus in claim 16 wherein said training means successively applies each of a plurality of pre-defined bit-mapped patterns for corresponding known alphanumeric characters as said unknown bit-mapped input data simultaneously with associated pre-defined neural output values as said target output values and enables said determining and adjusting means during a training period to determine said changes and adjust the values of said weights to minimize the error arising between said output values and said output target values for each of said bit-mapped known patterns whereby said network means is trained to initially recognize each of said known characters in said unknown input data.\n18. The apparatus in claim 17 wherein said network means further contains a plurality of numeric bias values associated therewith, and said determining and adjusting means, in response to said plurality of said output values and said pre-defined target values associated therewith, determines changes in the values of the weights and the bias values and, in response thereto, adjusts the values of said weights and the bias values while said unknown input bit-mapped data is applied to said network means so as to reduce said error arising between said output values and said output target values for each of said known characters.\n19. A method for recognizing the existence of a plurality of patterns in unknown input data comprising the steps of:\nproducing, in response to unknown input data and through a network, a plurality of output values that collectively identifies one of said patterns existing in said unknown input data, said network having a plurality of numeric weights associated therewith;\ndetermining, in response to a control signal and to said plurality of output values and pre-defined output target values associated therewith for said one pattern, changes in the value of each of said numeric weights and, in response thereto, adjusting a value of each of said weights while said unknown input data is applied to said network so as reduce error arising between said output values and said output target values for the unknown input data;\ndetermining, in response to said plurality of output values, a confidence measure associated therewith; and\ngenerating, in response to said confidence measure, said control signal so as to enable said determining and adjusting steps during pattern recognition to determine said changes and adjust the values of said weights when said confidence measure has a numeric value lying within a pre-defined numeric range so that network training does not occur during pattern recognition whenever the confidence measure associated with the output values produced for said one pattern is either greater or less than said numeric range whereby said network is trained to recognize said pattern in the event the pattern deviates from a pre-defined pattern.\n20. The method in claim 19 wherein said network has a inter-connected hierarchy of neurons responsive to said unknown input data for recognizing the patterns therein and, in response thereto, providing said output values; said hierarchy having input, hidden and output layers formed of different ones of said neurons with pre-defined pairs of said neurons being inter-connected with a corresponding one of said numeric weights associated therewith; wherein each of said neurons has a neural output and a neural input and provides a neural output value as a thresholded function of the neural input, and wherein in each of said neurons in said hidden and output layers:\nmultiplying, in response to a plurality of neural input signals, each of said neural input signals by a corresponding one of said numeric weights to form weighted neural input signals;\nforming, in response to said weighted neural input signals, a sum of said neural input signals; and\nthresholding said weighted sum by a pre-defined sigmoid function to provide the neural output.\n21. The method in claim 20 wherein said unknown input data comprises a plurality of input values, and said method further comprises the steps of:\napplying each of said input values as a neural input signal to an associated and different one of said neurons in said input layer; and\ncollectively forming said plurality of output values as the neural output values produced by all of said neurons in the output layer.\n22. The method in claim 21 wherein said sum comprises a bias value in addition to said weighted neural input signals, and wherein said determining and adjusting steps further comprises the step of determining, in response to said plurality of said output values and said pre-defined target values associated therewith, a change in the bias value for each of said neurons and, in response thereto, adjusting the bias value while said unknown input data is applied to said network so as to reduce said error.\n23. The method in claim 20 further comprising the steps of:\ntraining said network, in response to a pre-defined pattern for a corresponding known input pattern and having pre-defined ones of neural output values associated therewith, comprising:\nsimultaneously applying said pre-defined pattern as said unknown input data and said pre-defined neural output values as said target output values, and\nenabling said determining and adjusting steps during a training period to determine said changes and adjust the values of said weights to minimize error arising between said output values and said output target values whereby said network is trained to initially recognize said known pattern in said unknown input data.\n24. The method in claim 23 wherein said training step further comprises the steps of:\nsuccessively applying each of a plurality of pre-defined patterns for corresponding known input patterns as said unknown input data simultaneously with associated pre-defined neural output values as said target output values; and\nenabling said determining and adjusting steps to determine said changes and adjust the values of said weights to minimize the error arising between said output values and said output target values for each of said training patterns whereby said network is trained to initially recognize each of said known patterns in said unknown input data.\n25. The method in claim 24 wherein said network further contains a plurality of numeric bias values associated therewith, and said determining and adjusting step further comprises: determining, in response to said plurality of said output values and said pre-defined target values associated therewith, changes in the values of the weights and the bias values and, in response thereto, adjusting the values of said weights and the bias values while said unknown input data is applied to said network so as to reduce said error arising between said output values and said output target values for each of said known input patterns.\n26. In an optical character reader, a method for recognizing the existence of a plurality of bit-mapped alphanumeric characters in unknown bit-mapped input data comprising:\nproducing, in response to unknown bit-mapped input data and through a network, a plurality of output values that collectively identifies one of said bit-mapped patterns existing in said unknown bit-mapped input data, said network having a plurality of numeric weights associated therewith;\ndetermining, in response to a control signal and to said plurality of output values and pre-defined output target values associated therewith for said one bit-mapped pattern, changes in the value of each of said numeric weights and, in response thereto, adjusting a value of each of said weights while said unknown bit-mapped input data is applied to said network so as reduce error arising between said output values and said output target values for the unknown bit-mapped input data;\ndetermining, in response to said plurality of output values, a confidence measure associated therewith; and\ngenerating, in response to said confidence measure, said control signal so ass to enable said determining and adjusting steps during character recognition to determine said changes and adjust the values of said weights when said confidence measure has a numeric value lying within a pre-defined numeric range so that network training does not occur during character recognition whenever the confidence measure associated with the output values produced for said one pattern in either greater or less than said numeric range whereby said network is trained to recognize said bit-mapped pattern in the event the pattern deviates from a pre-defined bit-mapped pattern for an associated alphanumeric character.\n27. The method in claim 26 wherein said network has a inter-connected hierarchy of neurons responsive to said unknown input data for recognizing the patterns therein and, in response thereto, providing said output values; said hierarchy having input, hidden and output layers formed of different ones of said neurons with pre-defined pairs of said neurons being inter-connected with a corresponding one of said numeric weights associated therewith; wherein each of said neurons has a neural output and a neural input and provides a neural output value as a thresholded function of the neural input, and wherein in each of said neurons in said hidden and output layers:\nmultiplying, in response to a plurality of neural input signals, each of said neural input signals by a corresponding one of said numeric weights to form weighted neural input signals;\nforming, in response to said weighted neural input signals, a sum of said neural input signals; and\nthresholding said weighted sum by a pre-defined function to provide the neural output.\n28. The method in claim 27 comprising the steps of:\napplying a value of each pixel in an input bit-map as a neural input signal to an associated and different one of said neurons in said input layer; and\ncollectively forming said plurality of output values as the neural output values produced by all of said neurons in the output layer.\n29. The method in claim 28 further comprising the steps of:\ntraining said network, in response to a pre-defined pattern for a known pattern of an alphanumeric character and having pre-defined ones of neural output values associated therewith, comprising:\nsimultaneously applying said pre-defined pattern as said unknown input data and said pre-defined neural output values as said target output values, and\nenabling said determining and adjusting steps during a training period to determine said changes and adjust the values of said weights to minimize error arising between said output values and said output target values whereby said network is trained to initially recognize said known pattern in said unknown bit-mapped input data.\n30. The method in claim 29 wherein said training step further comprises the steps of:\nsuccessively applying each of a plurality of pre-defined training patterns for corresponding known input patterns as said unknown input data simultaneously with associated pre-defined neural output values as said target output values; and\nenabling said determining and adjusting steps to determine said changes and adjust the values of said weights to minimize the error arising between said output values and said output target values for each of said training patterns whereby said network is trained to initially recognize each of said known patterns in said unknown input data.\n31. The method in claim 30 wherein said network further contains a plurality of numeric bias values associated therewith, and said determining and adjusting step further comprises: determining, in response to said plurality of said output values and said pre-defined target values associated therewith, changes in the values of the weights and the bias values and, in response thereto, adjusting the values of said weights and the bias values while said unknown input data is applied to said network so as to reduce said error arising between said output values and said output target values for each of said known input patterns.\n32. In an optical character reader, a method for recognizing the existence of a plurality of bit-mapped alphanumeric characters in unknown bit-mapped input data comprising:\nin a pattern recognition procedure:\nproducing, in response to unknown bit-mapped input data and through a network, a plurality of output values that collectively identifies one of said bit-mapped patterns existing in said unknown bit-mapped input data, said network having a plurality of numeric weights associated therewith;\ndetermining, in response to a control signal and to said plurality of output values and pre-defined output target values associated therewith for said one bit-mapped pattern, changes in the value of each of said numeric weights and, in response thereto, adjusting a value of each of said weights while said unknown bit-mapped input data is applied to said network so as reduce error arising between said output values and said output target values for the unknown bit-mapped input data;\ndetermining, in response to said plurality of output values, a confidence measure associated therewith; and\ngenerating, in response to said confidence measure, said control signal so as to enable said determining and adjusting steps during pattern recognition to determine said changes and adjust the values of said weights when said confidence measure has a numeric value lying within a pre-defined numeric range so that network training does not occur during pattern recognition whenever the confidence measure associated with the output values produced for said one pattern is either greater or less than said numeric range whereby said network is trained to recognize said bit-mapped pattern in the event the pattern deviates from a pre-defined bit-mapped pattern for an associated alphanumeric character; and\nin a training procedure:\napplying the pre-defined bit-mapped pattern for an alphanumeric character as said unknown input data and corresponding pre-defined neural output values associated therewith as said target output values, and\nenabling said determining and adjusting steps during a training period to determine said changes and adjust the values of said weights to minimize error arising between said output values and said output target values whereby said network is trained to initially recognize said known pattern in said unknown bit-mapped input data; and\nexecuting said training procedure followed by said pattern recognition procedure.\n33. The method in claim 32 wherein said network has a inter-connected hierarchy of neurons responsive to said unknown input data for recognizing the patterns therein and, in response thereto, providing said output values; said hierarchy having input, hidden and output layers formed of different ones of said neurons with pre-defined pairs of said neurons being inter-connected with a corresponding one of said numeric weights associated therewith; wherein each of said neurons has a neural output and a neural input and provides a neural output value as a thresholded function of the neural input, and wherein in each of said neurons in said hidden and output layers:\nmultiplying, in response to a plurality of neural input signals, each of said neural input signals by a corresponding one of said numeric weights to form weighted neural input signals;\nforming, in response to said weighted neural input signals, a sum of said neural input signals; and\nthresholding said weighted sum by a pre-defined sigmoid function to provide the neural output.\n34. The method in claim 33 wherein said unknown input data comprises a plurality of input values, and said method further comprises the steps of:\napplying each of said input values as a neural input signal to an associated and different one of said neurons in said input layer; and\ncollectively forming said plurality of output values as the neural output values produced by all of said neurons in the output layer.\n35. The method in claim 34 wherein said sum comprises a bias value in addition to said weighted neural input signals, and wherein said determining and adjusting steps further comprises the step of determining, in response to said plurality of said output values and said pre-defined target values associated therewith, a change in the bias value for each of said neurons and, in response thereto, adjusting the bias value while said unknown input data is applied to said network so as to reduce said error.\n36. The method in claim 32 further comprising successively executing the pattern recognition step twice for each page of an input document being recognized so as to improve recognition accuracy for that page.",
    "status": "Expired - Lifetime",
    "citations_own": [
        "US3192505A",
        "US3267439A",
        "US3275986A",
        "US3275985A",
        "US3646329A",
        "US4479241A",
        "US4504970A",
        "US4682365A",
        "US4742556A",
        "US4748674A",
        "US4873661A",
        "US4876731A",
        "US4884216A",
        "US4885757A",
        "US4912649A",
        "US4912654A",
        "US4912651A",
        "US4912652A",
        "US4912655A",
        "US4918618A",
        "US4921647A",
        "US4933872A",
        "US4951239A"
    ],
    "citations_ftf": [
        "US3548202A",
        "JPS5819109B2"
    ],
    "citedby_own": [
        "US5157275A",
        "WO1992020029A1",
        "US5179596A",
        "US5181256A",
        "WO1993019426A1",
        "US5251286A",
        "US5257328A",
        "US5274747A",
        "EP0586210A2",
        "US5295197A",
        "US5299269A",
        "US5319722A",
        "WO1994012948A1",
        "US5321771A",
        "US5329594A",
        "US5335291A",
        "US5351079A",
        "US5355436A",
        "US5361326A",
        "US5371809A",
        "US5402521A",
        "US5422983A",
        "WO1995015540A1",
        "US5426721A",
        "US5428710A",
        "US5438644A",
        "US5442715A",
        "US5455872A",
        "US5479572A",
        "US5479575A",
        "EP0689154A2",
        "US5485545A",
        "US5485547A",
        "US5493688A",
        "WO1996007992A1",
        "US5528700A",
        "US5542006A",
        "US5555439A",
        "US5586223A",
        "US5590243A",
        "US5594597A",
        "US5621862A",
        "US5630018A",
        "US5631981A",
        "US5633954A",
        "US5634087A",
        "US5642435A",
        "US5644681A",
        "US5649067A",
        "US5671335A",
        "US5675665A",
        "US5677998A",
        "US5684929A",
        "US5696838A",
        "US5706403A",
        "US5708727A",
        "US5712922A",
        "US5737496A",
        "US5742702A",
        "US5751910A",
        "US5796410A",
        "US5812992A",
        "US5825646A",
        "US5826249A",
        "US5832183A",
        "US5835633A",
        "WO1998050880A1",
        "US5930781A",
        "US5966464A",
        "US5971128A",
        "EP0955600A2",
        "US5995953A",
        "US6016384A",
        "US6018728A",
        "US6028956A",
        "US6038555A",
        "US6148106A",
        "US6314414B1",
        "US6324532B1",
        "US6327550B1",
        "US20020025070A1",
        "US6363171B1",
        "US6393395B1",
        "US20030023414A1",
        "US20030115078A1",
        "US20030200075A1",
        "US20030200191A1",
        "US20030200189A1",
        "US20040019574A1",
        "US20040042665A1",
        "US20040117226A1",
        "US20050008227A1",
        "US20050033709A1",
        "US20050059160A1",
        "US20050249273A1",
        "US20050288954A1",
        "US20060034347A1",
        "US20060036529A1",
        "US20060036453A1",
        "US20060036635A1",
        "US20060036452A1",
        "US20060036632A1",
        "US20060198552A1",
        "US20080228761A1",
        "US7457458B1",
        "US20090063189A1",
        "US20090185752A1",
        "US20100054629A1",
        "US20100092075A1",
        "US20100138372A1",
        "US8369967B2",
        "WO2013104938A2",
        "US20130311834A1",
        "EP2793171A1",
        "US8892495B2",
        "US20180005111A1",
        "US20180253645A1",
        "US20180349742A1",
        "WO2019058372A1",
        "US10361802B1",
        "WO2019160975A1",
        "US10410117B2",
        "US10422854B1",
        "WO2019210276A1",
        "WO2020030722A1",
        "US10872290B2",
        "US11024009B2",
        "US11200483B2",
        "US11232330B2",
        "US20220139095A1",
        "US11397579B2",
        "US11437032B2",
        "US11442786B2",
        "US11468332B2",
        "US11513586B2",
        "US11544059B2",
        "US20230004814A1",
        "US11574193B2",
        "US11609760B2",
        "US11630666B2",
        "US11676029B2",
        "US11675676B2",
        "US11703939B2",
        "US11762690B2",
        "US11789847B2"
    ],
    "citedby_ftf": [
        "WO2020196066A1"
    ]
}