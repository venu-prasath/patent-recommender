{
    "patent_link": "https://patents.google.com/patent/US5937384A/en",
    "patent_id": "US5937384A",
    "title": "Method and system for speech recognition using continuous density hidden Markov models",
    "abstract": "A method and system for achieving an improved recognition accuracy in speech recognition systems which utilize continuous density hidden Markov models to represent phonetic units of speech present in spoken speech utterances is provided. An acoustic score which reflects the likelihood that a speech utterance matches a modeled linguistic expression is dependent on the output probability associated with the states of the hidden Markov model. Context-independent and context-dependent continuous density hidden Markov models are generated for each phonetic unit. The output probability associated with a state is determined by weighing the output probabilities of the context-dependent and context-independent states in accordance with a weighting factor. The weighting factor indicates the robustness of the output probability associated with each state of each model, especially in predicting unseen speech utterances.",
    "inventors": [
        "Xuedong D. Huang",
        "Milind V. Mahajan"
    ],
    "assignee": "Microsoft Technology Licensing LLC",
    "classifications": [
        "G10L15/144",
        "G06F18/295",
        "G10L15/187"
    ],
    "claims": "\n1. A method in a computer system for matching an input speech utterance to a linguistic expression, the method comprising the steps of:\nfor each of a plurality of phonetic units of speech, providing a plurality of more-detailed acoustic models and a less-detailed acoustic model to represent the phonetic unit, each acoustic model having a plurality of states followed by a plurality of transitions, each state representing a portion of a speech utterance occurring in the phonetic unit at a certain point in time and having an output probability indicating a likelihood of a portion of an input speech utterance occurring in the phonetic unit at a certain point in time;\nfor each of select sequences of more-detailed acoustic models, determining how close the input speech utterance matches the sequence, the matching further comprising the step of:\nfor each state of the select sequence of more-detailed acoustic models, determining an accumulative output probability as a combination of the output probability of the state and a same state of the less-detailed acoustic model representing the same phonetic unit; and\ndetermining the sequence which best matches the input speech utterance, the sequence representing the linguistic expression.\n2. A method as in claim 1 where each acoustic model is a continuous density hidden Markov model.\n3. A method as in claim 1 wherein the step of determining the output probability further comprises the step of weighing the less-detailed model and more-detailed model output probabilities with separate weighting factors when combined.\n4. A method as in claim 1 wherein the step of providing a plurality of more-detailed acoustic models further comprises the step of training each acoustic model using an amount of training data of speech utterances; and\nwherein the step of determining the output probability further comprises the step of weighing the less-detailed model and more-detailed model output probabilities relative to the amount of training data used to train each acoustic model.\n5. A method in a computer system for determining a likelihood of an input speech utterance matching a linguistic expression, the input speech utterance comprising a plurality of feature vectors indicating acoustic properties of the utterance during a given time interval, the linguistic expression comprising a plurality of senones indicating the output probability of the acoustic properties occurring at a position within the linguistic expression, the method comprising the steps of:\nproviding a plurality of context-dependent senones;\nproviding a context-independent senone associated with the plurality context-dependent senones representing a same position of the linguistic expression;\nproviding a linguistic expression likely to match the input speech utterance;\nfor each feature vector of the input speech utterance, determining the output probability that the feature vector matches the context-dependent senone in the linguistic expression which occurs at the same time interval as the feature vector, the output probability determination utilizing the context-independent senone associated with the context-dependent senone; and\nutilizing the output probabilities to determine the likelihood that the input speech utterance matches the linguistic expression.\n6. A method as in claim 5 wherein the output probability comprises a continuous probability density function.\n7. A method as in claim 5 wherein the step of providing a plurality of context-dependent senones further comprises the step of training the context-dependent senones from an amount of training data representing speech utterances;\nwherein the step of providing a context-independent senone further comprises the step of training the context-independent senones from the amount of training data; and\nwherein the step of determining the output probability further comprises the step of combining the context-independent and context-dependent senones in accord with the amount of training data used to train the senones.\n8. A method as in claim 5 wherein the step of providing a plurality of context-dependent senones further comprises the steps of:\ntraining the context-dependent senones from an amount of training data representing speech utterances;\nproviding a weighting factor for each context-dependent senone representing the amount of training data used to estimate the senone; and\nwherein the step of determining the output probability further comprises the step of combining the context-dependent senone and context-independent senone in accord with the weighing factor.\n9. A method as in claim 8 wherein the step of providing a weighting factor further comprises the step of generating the weighting factor by using a deleted interpolation technique on the amount of training data.\n10. A method as in claim 8 wherein the step of providing a weighting factor further comprises the steps of:\nproducing a parametric representation of the training data; and\ngenerating the weighing factor by applying a deleted interpolation technique to the parametric representation of the amount of training data.\n11. A method as in claim 8 wherein the step of providing a weighting factor further comprises the steps of:\nproducing a parametric representation of the training data;\nproviding a set of data points from the parametric representation of the training data, the data points representing the training data; and\ngenerating the weighing factor from the application of deleted interpolation to the data points.\n12. A method in a computer readable storage medium for recognizing an input speech utterance, said method comprising the steps of:\ntraining a plurality of context-dependent continuous density hidden Markov models to represent a plurality of phonetic units of speech, the training utilizing an amount of training data of speech utterances representing acoustic properties of the utterance during a given time interval, each model having states connected by transitions, each state representing a portion of the phonetic unit and having an output probability indicating a probability of an acoustic property of a speech utterance occurring within a portion of the phonetic unit;\nproviding a context-independent continuous density hidden Markov model for the plurality of context-dependent continuous density hidden Markov models representing the same phonetic unit of speech;\nproviding a plurality of sequences of the context-dependent models, each sequence representing a linguistic expression;\nfor each sequence of the context-dependent models, determining an acoustic probability of the acoustic properties of the input speech utterance matching the states in the sequence of the context-dependent models, the acoustic probability comprising the output probability of each state of each context-dependent model in the sequence and the output probability of the context-independent model corresponding to a same phonetic unit; and\nutilizing the acoustic probability to recognize the linguistic expression which closely matches the input speech utterance.\n13. A method as in claim 12, further comprising the step of providing a weighting factor for each state of the context-dependent models, the weighting factor indicating the amount of training data used to train the output probability associated with each state; and\nwherein the step of determining an acoustic probability further comprises the step of weighing the output probability of the state of the context-dependent model and the state of the context-independent model based on the weighting factor.\n14. A method as in claim 13 wherein the step of providing a weighting factor further comprises the step of deriving the weighting factor from an application of deleted interpolation to the amount of training data.\n15. A method as in claim 13 wherein the step of providing a weighting factor further comprises the steps of:\nproducing a parametric representation of the training data; and\nderiving the weighting factor from an application of deleted interpolation to the parametric representation of the training data.\n16. A method as in claim 13 wherein the step of providing a weighting factor further comprises the steps of:\nproducing a parametric representation of the training data;\ngenerating a set of data points from the parametric representation of the training data; and\nderiving the weighting factor from an application of deleted interpolation to the parametric representation of the training data.\n17. A computer system for matching an input speech utterance to a linguistic expression, comprising:\na storage device for storing a plurality of context-dependent and context-independent acoustic models representing respective ones of phonetic units of speech, the plurality of context-dependent acoustic models which represent each phonetic unit having at least one associated context-independent acoustic model representing the phonetic unit of speech, each acoustic model comprising states having transitions, each state representing a portion of the phonetic unit at a certain point in time and having an output probability indicating a likelihood of a portion of the input speech utterance occurring in the phonetic unit at a certain point in time;\na model sequence generator which provides select sequences of context-dependent acoustic models representing a plurality of linguistic expressions likely to match the input speech utterance;\na processor for determining how well each of the sequence of models matches the input speech utterance, the processor matching a portion of the input speech utterance to a state in the sequence by utilizing an accumulative output probability for each state of the sequence, the accumulative output probability including the output probability of each state of the context-dependent acoustic model combined with the output probability of a same state of the associated context-independent acoustic model; and\na comparator to determine the sequence which best matches the input speech utterance, the sequence representing the linguistic expression.\n18. A system as in claim 17 wherein each acoustic model is a continuous density hidden Markov model.\n19. A system as in claim 17, further comprising:\na training device to receive an amount of training data of speech utterances and to estimate the output probability for each state of each acoustic model with the amount of training data; and\nwherein the processor further comprises a combining element to determine the accumulative output probability of each state, the combining element combining the output probability of each state of the sequence with the output probability of a same state of the associated context-independent acoustic model relative to the amount of training data used to estimate each output probability.\n20. A system as in claim 17, further comprising:\na training device to receive an amount of training data of speech utterances used to estimate the output probability for each state of each acoustic model with the amount of training data, the training device generating a weighting factor for each state of each context-dependent acoustic model indicating a degree to which the output probability can predict speech utterances not present in the training data; and\nwherein the processor further comprises a combining element to determine the accumulative output probability of a state, the combining element combining the output probability of each state of the sequence with the output probability of a same state of the associated context-independent acoustic model relative to the weighting factor for each state.\n21. A system as in claim 20 wherein the weighting factor is derived by applying a deleted interpolation technique to the amount of the training data.\n22. A system as in claim 20 wherein the training device further comprises a parametric generator to generate a parametric representation of the training data; and\nwherein the weighting factor is derived by applying a deleted interpolation technique to the parametric representation of the training data.\n23. A system as in claim 20 wherein the training device further comprises:\na parametric generator to produce a parametric representation of the training data;\na data generator to generate a set of data points from the parametric representation; and\nwherein the weighting factor is derived by applying a deleted interpolation technique to the set of data points.",
    "status": "Expired - Lifetime",
    "citations_own": [
        "US4783803A",
        "US4817156A",
        "US4829577A",
        "US4866778A",
        "US5027406A",
        "US5267345A",
        "US5268990A",
        "US5293584A",
        "US5333236A",
        "US5444617A",
        "US5621859A",
        "US5627939A",
        "US5642519A",
        "US5710866A"
    ],
    "citations_ftf": [
        "US4587670A",
        "JP3581401B2"
    ],
    "citedby_own": [
        "US6336108B1",
        "US6418431B1",
        "US20020091520A1",
        "US20020091521A1",
        "US20020184025A1",
        "US20030004722A1",
        "US6505158B1",
        "US6529866B1",
        "US6539353B1",
        "US6567778B1",
        "US6570964B1",
        "US6571210B2",
        "US6574597B1",
        "US20030115169A1",
        "US20030130843A1",
        "US6606594B1",
        "US6629073B1",
        "US6633845B1",
        "US20030200080A1",
        "US6662158B1",
        "US20040073431A1",
        "US6728674B1",
        "US6751621B1",
        "US20040243419A1",
        "US20050027531A1",
        "US6865528B1",
        "US20050154591A1",
        "US20050216516A1",
        "US20050228666A1",
        "US7031908B1",
        "US7058573B1",
        "US7082397B2",
        "US20060190268A1",
        "US7181399B1",
        "US7200559B2",
        "US20070198261A1",
        "US20070198263A1",
        "US20070271096A1",
        "US20080059185A1",
        "US20080103772A1",
        "US20080243506A1",
        "US7460997B1",
        "US20090106028A1",
        "US7650282B1",
        "US20100057452A1",
        "US20100211387A1",
        "US20100211391A1",
        "US20100211376A1",
        "US20100257128A1",
        "US20100332228A1",
        "US7970613B2",
        "US20120271631A1",
        "US20130006612A1",
        "US8352265B1",
        "US8463610B1",
        "US20130158996A1",
        "US8639510B1",
        "US20140180692A1",
        "US20140180693A1",
        "US20140180694A1",
        "US20150006175A1",
        "US9153235B2",
        "US20150371633A1",
        "US9240184B1",
        "US9484019B2",
        "US9508045B2",
        "US20160350286A1",
        "WO2017061985A1",
        "US9711148B1",
        "US9858922B2",
        "US10014007B2",
        "US10079022B2",
        "US10204619B2",
        "US10229672B1",
        "US10255903B2",
        "US10665243B1",
        "US10706840B2",
        "US20210350798A1",
        "US11211065B2",
        "US11222175B2"
    ],
    "citedby_ftf": [
        "AU5205700A",
        "ES2190342B1",
        "US7050975B2",
        "US7752045B2",
        "US7406416B2",
        "US7478038B2",
        "JP2012108748A",
        "CN102129860B",
        "CN116108391B"
    ]
}