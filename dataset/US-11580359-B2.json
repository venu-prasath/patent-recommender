{
    "patent_id": "US-11580359-B2",
    "title": "Pointer sentinel mixture architecture ",
    "assignee": "Salesforce.Com, Inc.",
    "publication_date": "2023-02-14",
    "patent_link": "https://patents.google.com/patent/US11580359B2/en",
    "inventors": [
        "Stephen Joseph MERITY",
        "Caiming Xiong",
        "James BRADBURY",
        "Richard Socher"
    ],
    "classifications": [
        "G06N3/0445",
        "G06N3/044",
        "G06F40/284",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/047",
        "G06N3/0472",
        "G06N3/08",
        "G06N3/084",
        "G06N7/005",
        "G06N7/01"
    ],
    "abstract": "The technology disclosed provides a so-called \u201cpointer sentinel mixture architecture\u201d for neural network sequence models that has the ability to either reproduce a token from a recent context or produce a token from a predefined vocabulary. In one implementation, a pointer sentinel-LSTM architecture achieves state of the art language modeling performance of 70.9 perplexity on the Penn Treebank dataset, while using far fewer parameters than a standard softmax LSTM.",
    "claims": "\n1. A system for predicting an output token given a context text and a vocabulary text for a natural language processing (NLP) task, the system comprising:\na network interface receiving the vocabulary text and the context text;\na memory storing a plurality of processor-executable instructions for operating a pointer sentinel mixture architecture;\none or more processors reading and executing the plurality of processor-executable instructions to perform operations comprising:\ngenerating, via a pointer network of the pointer sentinel mixture architecture, a pointer distribution, the pointer distribution corresponding to a distribution of attention probability masses over a window of tokens in the context text;\ngenerating, via a vocabulary network of the pointer sentinel mixture architecture, a vocabulary distribution, the vocabulary distribution corresponding to a distribution of vocabulary probability masses over tokens in the vocabulary text;\ngenerating, by a recurrent neural network (RNN), a final output state vector for each position in the window of tokens in the context text;\ncomputing a gate probability mass based on a trained sentinel gate vector and a query vector formulated from the final output state vector;\nmixing, at a pointer vocabulary mixer of the pointer sentinel mixer architecture, the pointer distribution and the vocabulary distribution, as governed by a gate probability mass; and\noutputting the output token according to a mixed distribution in response to the NLP task.\n2. The system of claim 1, wherein the pointer distribution is generated using an unnormalized gate value.\n3. The system of claim 2, wherein the gate probability mass results from exponentially normalizing the unnormalized gate value.\n4. The system of claim 1, wherein a sum of a vector of the attention probability masses and the gate probability mass being a predetermined constant.\n5. The system of claim 1, wherein a sum of the distribution of vocabulary probability masses over the tokens in the vocabulary text being a predetermined constant.\n6. The system of claim 1, wherein the trained sentinel gate vector controls accumulation of information from the vocabulary network and the pointer network.\n7. The system of claim 6, wherein the gate probability mass being unity results in accumulation of information from the vocabulary network.\n8. The system of claim 6, wherein the gate probability mass being zero results in accumulation of information from the pointer network.\n9. The system of claim 1, wherein the query vector is formulated from the final output state vector of the RNN by processing the final output state vector through a linear layer to generate a non-linear projection.\n10. The system of claim 1, wherein the trained sentinel gate vector is trained a priori.\n11. The system of claim 10, wherein a dynamic sentinel gate vector is formulated from a final output state vector of the RNN by concatenating the final output state vector with the trained sentinel gate vector and processing the concatenation through a linear layer to generate a non-linear projection.\n12. The system of claim 1, wherein the output token comprises one of a word, a character, and a phrase.\n13. A method for predicting an output token given a context text and a vocabulary text for a natural language processing (NLP) task, the method comprising:\nreceiving, via a network interface, the vocabulary text and the context text;\ngenerating, via a pointer network of the pointer sentinel mixture architecture implemented on one or more hardware processors, a pointer distribution, the pointer distribution corresponding to a distribution of attention probability masses over a window of tokens in the context text;\ngenerating via a vocabulary network of the pointer sentinel mixture architecture, a vocabulary distribution, the vocabulary distribution corresponding to a distribution of vocabulary probability masses over tokens in the vocabulary text;\ngenerating, by a recurrent neural network (RNN), a final output state vector for each position in the window of tokens in the context text;\ncomputing a gate probability mass based on a trained sentinel gate vector and a query vector formulated from the final output state vector;\nmixing the pointer distribution and the vocabulary distribution, as governed by a gate probability mass; and\noutputting the output token according to a mixed distribution in response to the NLP task.\n14. The method of claim 13, wherein the pointer distribution is generated using an unnormalized gate value.\n15. The method of claim 14, wherein the gate probability mass results from exponentially normalizing the unnormalized gate value.\n16. The method of claim 13, wherein a sum of a vector of the attention probability masses and the gate probability mass being a predetermined constant.\n17. The method of claim 13, wherein a sum of the distribution of vocabulary probability masses over the tokens in the vocabulary text being a predetermined constant.\n18. The method of claim 13, wherein the trained sentinel gate vector controls accumulation of information from the vocabulary network and the pointer network.\n19. The method of claim 18, wherein the gate probability mass being unity results in accumulation of information from the vocabulary network.\n20. The method of claim 18, wherein the gate probability mass being zero results in accumulation of information from the pointer network.\n21. The method of claim 13, wherein the query vector is formulated from the final output state vector of the RNN by processing the final output state vector through a linear layer to generate a non-linear projection.\n22. The method of claim 13, wherein the trained sentinel gate vector is trained a priori.\n23. The method of claim 22, wherein a dynamic sentinel gate vector is formulated from the final output state vector of the RNN by concatenating the final output state vector with the trained sentinel gate vector and processing the concatenation through a linear layer to generate a non-linear projection.\n24. The method of claim 13, wherein the output token comprises one of a word, a character, and a phrase."
}