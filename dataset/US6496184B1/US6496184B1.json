{
    "patent_link": "https://patents.google.com/patent/US6496184B1/en",
    "patent_id": "US6496184B1",
    "title": "Method for inferring scenes from test images and training data using probability propagation in a markov network",
    "abstract": "A method infers a scene from a test image. During a training phase, a plurality of images and corresponding scenes are acquired. Each of the images and corresponding scenes are partitioned respectively into a plurality of image patches and scene patches. Each image patch is represented as an image vector, and each scene patch is represented as a scene vector. The image vectors and scene vectors are modeled as a network. During an inference phase, the test image is acquired. The test image is partitioned into a plurality of test image patches. Each test image patch is represented as a test image vector. Candidate scene vectors corresponding to the test image vectors are located in the network. Compatibility matrices for the candidate scene vectors are determined, and probabilities of the compatibility matrices are propagated in the network until convergence to infer the scene from the test image.",
    "inventors": [
        "William T. Freeman",
        "Egon C. Pasztor",
        "Baback Moghaddam"
    ],
    "assignee": "Mitsubishi Electric Research Laboratories Inc",
    "classifications": [
        "G06T7/277",
        "G06T7/46",
        "G06V20/00",
        "G06T2207/10016",
        "G06T2207/20056",
        "G06T2207/20081"
    ],
    "claims": "\n1. A method for inferring a scene from a test image, comprising the steps of:\n2. The method of claim 1 wherein the images, the scenes, and the test image are visual data.\n3. The method of claim 1 wherein the images, the scenes, and the test image are audio data.\n4. The method of claim 1 wherein the images and the scenes, and the test image are synthetically generated.\n5. The method of claim 1 wherein the images and the scene are measured.\n6. The method of claim 1 wherein there is a one-to-one correspondence between the images and the scenes.\n7. The method of claim 1 wherein each of the image, scene, and test image is respectively partitioned into a plurality of image, scene, and test image patches.\n8. The method of claim 1 wherein the vectors are low-dimensional vectors determined by principal component analysis.\n9. The method of claim 8 wherein the dimensionality of the vectors is less than ten.\n10. The method of claim 1 wherein the network is a Markov network having nodes and edges, and wherein the nodes represent the vectors and the edges represent statistical dependencies between the vectors.\n11. The method of claim 1 wherein the network is multi-resolution.\n12. The method of claim 1 further comprising the steps of determining local likelihood functions for the candidate scene vectors, and determining compatibility functions to indicate how neighboring vectors are probabilistically related to each other.\n13. The method of claim 10 wherein a sum-product rule determines a marginalized mean of a posterior probability for each node of the network.\n14. The method of claim 10 wherein a max-product determines a value of a variable which maximizes the posterior probability for each node.\n15. The method of claim 12 wherein a loss function is used to modify the local likelihood functions.\n16. The method of claim 1 wherein the test image is low-resolution, and the inferred scene is high-resolution.\n17. The method of claim 1 wherein the test image is a picture, and the inferred scene is a set of intrinsic images having reflectance values and surface heights, and a description of overall lighting.\n18. The method of claim 1 wherein the test image is a video of a moving body, and the inferred scene is a set of three-dimensional motion parameters of the moving body.\n19. The method of claim 1 wherein the test image is a first video of a moving object, and the inferred scene is a second video of a second moving object moving according to the first moving object.\n20. The method of claim 1 wherein the image vectors and scene vectors comprise training data arranged as a binary tree.\n21. The method of claim 20 wherein the candidate scene vectors are precomputed at index nodes in the binary tree.\n22. The method of claim 1 wherein the images and scenes are organized as a plurality of classes, each image and scene having a corresponding class label c.\n23. The method of claim 22 wherein a class compatibility function \u03c8(ci, cj) indicates a likelihood that a scene from class c1 borders a scene from class cj such that \u03c8(ci, ci) is 1, and \u03c8(ci, cj) is less than one, for i\u2260j.\n24. The method of claim 23 wherein a scene compatibility function is:\n25. The method of claim 1 wherein the propagating probabilities of the candidate scene vectors are propagated until a termination condition is reached.",
    "status": "Expired - Fee Related",
    "citations_own": [
        "US5784114A",
        "US5940145A",
        "US5963670A",
        "US6097854A",
        "US6141019A",
        "US6151424A"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US20040044634A1",
        "GB2409028A",
        "US6910000B1",
        "US20050286772A1",
        "US20070041639A1",
        "US20080211812A1",
        "US20080262789A1",
        "US20090132436A1",
        "US7583819B2",
        "US20100074549A1",
        "US20100114537A1",
        "US20100110074A1",
        "US20110054853A1",
        "US20110085699A1",
        "US20110187713A1",
        "US20110221966A1",
        "US20120075296A1",
        "US8401222B2",
        "US8731234B1",
        "US8774525B2",
        "US20160239746A1",
        "US9501700B2",
        "US20160341810A1",
        "CN103902989B",
        "US9599466B2",
        "US9679227B2",
        "US9933257B2",
        "US9953370B2",
        "US9959581B2",
        "US10225575B2",
        "US10503843B2",
        "US10663294B2",
        "US10909482B2",
        "US11094113B2",
        "WO2021169209A1",
        "US11164256B2"
    ],
    "citedby_ftf": []
}