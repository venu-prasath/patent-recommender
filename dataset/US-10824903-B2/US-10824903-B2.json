{
    "patent_id": "US-10824903-B2",
    "title": "Deep multi-scale video prediction ",
    "assignee": "Facebook, Inc.",
    "publication_date": "2020-11-03",
    "patent_link": "https://patents.google.com/patent/US10824903B2/en",
    "inventors": [
        "Michael Fabien Mathieu",
        "Camille COUPRIE",
        "Yann Andre Le Cun"
    ],
    "classifications": [
        "G06N3/084",
        "G06K9/6212",
        "G06F18/21",
        "G06K9/00718",
        "G06K9/4628",
        "G06K9/6217",
        "G06N3/044",
        "G06N3/0445",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/047",
        "G06N3/0472",
        "G06N3/088",
        "G06N7/005",
        "G06N7/01",
        "G06V10/454",
        "G06V10/758",
        "G06V10/82",
        "G06V20/41",
        "G06K2009/00738",
        "G06N3/048",
        "G06N3/0481",
        "G06Q50/01",
        "G06V20/44"
    ],
    "abstract": "In one embodiment, a method includes receiving a plurality of input frames of a video sequence associated with a time t, predicting, using a convolutional network, one or more future frames of the video sequence from the plurality of input frames, wherein the convolutional network is trained with randomly selected temporal sequences of a n\u00d7m grid of pixels from the plurality of input frames exhibiting a threshold of optical flow. In addition, the training may comprise randomly selecting temporal sequences of a n\u00d7m grid of pixels from the plurality of input frames exhibiting a threshold of optical flow.",
    "claims": "\n1. A method comprising, by one or more computing devices:\nreceiving a plurality of input frames of a video sequence associated with a time t;\npredicting, using a convolutional network, one or more future frames of the video sequence from the plurality of input frames, wherein the convolutional network is trained with randomly selected temporal sequences of a n\u00d7m grid of pixels from the plurality of input frames exhibiting a threshold of optical flow; and\noutputting a first future frame of the video sequence associated with a time t+1 as predicted by the convolutional network.\n2. The method of claim 1, further comprising by the one or more computing devices:\npredicting, using the convolutional network, a second future frame of the video sequence associated with time t+2 by using the first future frame as an input frame of the video sequence;\nrecursively applying the convolutional network based on the first future frame; and\noutputting the second future frame of the video sequence associated with the time t+2 as predicted by the convolutional network.\n3. The method of claim 1, wherein the threshold of optical flow is determined based on a comparison of movement in the n\u00d7m grid of pixels between a time t\u22121 and the time t.\n4. The method of claim 3, wherein the comparison of movement comprises comparing pixel intensity associated with each pixel of the n\u00d7m grid of pixels between the time t\u22121 and the time t and calculating a similarity measure based on compared pixel intensities.\n5. The method of claim 1, wherein the convolutional network is further trained by evaluating a quality of the predicted one or more future frames of the video sequence by calculating a peak-signal-to-noise ratio between the one or more future frames of the video sequence and a ground truth frame of the video sequence.\n6. The method of claim 1, wherein the convolutional network is further trained by evaluating a quality of the predicted one or more future frames of the video sequence by calculating a structural similarity index measure between the one or more future frames of the video sequence and a ground truth frame of the video sequence.\n7. The method of claim 1, wherein the convolutional network employs an L1 loss to train the network on the input frames.\n8. The method of claim 1, wherein the convolutional network employs an L2 loss to train the network on the input frames.\n9. The method of claim 1, wherein the convolutional network employs a gradient difference loss to train the network on the input frames.\n10. One or more computer-readable non-transitory storage media embodying software that is operable when executed to:\nreceive a plurality of input frames of a video sequence associated with a time t;\npredict, using a convolutional network, one or more future frames of the video sequence from the plurality of input frames, wherein the convolutional network is trained with randomly selected temporal sequences of a n\u00d7m grid of pixels from the plurality of input frames exhibiting a threshold of optical flow; and\noutput a first future frame of the video sequence associated with a time t+1 as predicted by the convolutional network.\n11. The media of claim 10, wherein threshold of optical flow is determined based on a comparison of movement in the n\u00d7m grid of pixels between a time t\u22121 and the time t.\n12. The media of claim 11, wherein the comparison of movement comprises comparing pixel intensity associated with each pixel of the n\u00d7m grid of pixels between the time t\u22121 and the time t and calculating a similarity measure based on compared pixel intensities.\n13. The media of claim 10, wherein the training of the convolutional network may further comprise evaluating a quality of the predicted one or more future frames of the video sequence by calculating a peak-signal-to-noise ratio between the one or more future frames of the video sequence and a ground truth frame of the video sequence.\n14. The media of claim 10, wherein the training of the convolutional network may further comprise evaluating a quality of the predicted one or more future frames of the video sequence by calculating a structural similarity index measure between the one or more future frames of the video sequence and a ground truth frame of the video sequence.\n15. A system comprising: one or more processors; and one or more computer-readable non-transitory storage media coupled to one or more of the processors and comprising instructions operable when executed by one or more of the processors to cause the system to:\nreceive a plurality of input frames of a video sequence associated with a time t;\npredict, using a convolutional network, one or more future frames of the video sequence from the plurality of input frames, wherein the convolutional network is trained with randomly selected temporal sequences of a n\u00d7m grid of pixels from the plurality of input frames exhibiting a threshold of optical flow; and\noutput a first future frame of the video sequence associated with a time t+1 as predicted by the convolutional network.\n16. The system of claim 15, wherein the processors are further operable when executing the instructions to:\npredict a second future frame of the video sequence associated with time t+2 by using the first future frame as an input frame of the video sequence,\nrecursively apply the generative model based on the first future frame, and\noutputting the second future frame of the video sequence associated with the time t+2 as predicted by the generative model.\n17. The system of claim 15, wherein threshold of optical flow is determined based on a comparison of movement in the n\u00d7m grid of pixels between a time t\u22121 and the time t.\n18. The system of claim 17, wherein the comparison of movement comprises comparing pixel intensity associated with each pixel of the n\u00d7m grid of pixels between the time t\u22121 and the time t and calculating a similarity measure based on compared pixel intensities.\n19. The system of claim 15, wherein the training of the convolutional network may further comprise evaluating a quality of the predicted one or more future frames of the video sequence by calculating a peak-signal-to-noise ratio between the one or more future frames of the video sequence and a ground truth frame of the video sequence.\n20. The system of claim 15, wherein the training of the convolutional network may further comprise evaluating a quality of the predicted one or more future frames of the video sequence by calculating a structural similarity index measure between the one or more future frames of the video sequence and a ground truth frame of the video sequence.",
    "status": "Active",
    "citations_own": [
        "US20110317888A1",
        "US20130114942A1",
        "US20140126769A1",
        "US20140372175A1",
        "US20150287203A1",
        "US20180231653A1",
        "US20180232571A1",
        "US20180286055A1",
        "US10268913B2"
    ],
    "citations_ftf": [
        "US7016539B1",
        "JP2010009177A",
        "US8930289B2",
        "US9390370B2",
        "US9672626B2",
        "US10891541B2",
        "US10452899B2",
        "US20180075347A1",
        "US10013773B1",
        "KR102563752B1"
    ],
    "citedby_own": [
        "CN112861758A"
    ],
    "citedby_ftf": [
        "US10354168B2",
        "US10929681B2",
        "US10839226B2",
        "US10332001B2",
        "US10482609B2",
        "WO2018199051A1",
        "WO2018203920A1",
        "US11019355B2",
        "US11017540B2",
        "US11481637B2",
        "US11347213B2",
        "CN109086668B",
        "GB2575628A",
        "CN110738540B",
        "US10430708B1",
        "US11017296B2",
        "US10846888B2",
        "CN109508660A",
        "KR102162451B1",
        "JP6569047B1",
        "EP3663965A1",
        "CN109756690B",
        "US10798386B2",
        "US10867375B2",
        "US11308598B2",
        "US11042803B2",
        "JP7162550B2",
        "US11176654B2",
        "US10929719B2",
        "US11055828B2",
        "US11417096B2",
        "CN110600085A",
        "GB201908530D0",
        "EP4003664A1",
        "EP3799416B1",
        "CN110769196A",
        "US11586912B2",
        "US11416774B2",
        "US11037531B2",
        "CN111177581A",
        "CN111179246B",
        "US11635802B2",
        "CN111582254A",
        "US11430085B2",
        "US11386532B2",
        "US20220179990A1",
        "CN113870314B",
        "CN116308978A"
    ]
}