{
    "patent_id": "US-11550614-B2",
    "title": "Packaging and deploying algorithms for flexible machine learning ",
    "assignee": "Amazon Technologies, Inc.",
    "publication_date": "2023-01-10",
    "patent_link": "https://patents.google.com/patent/US11550614B2/en",
    "inventors": [
        "Thomas Albert FAULHABER, JR.",
        "Gowda Dayananda ANJANEYAPURA RANGE",
        "Jeffrey John GEEVARGHESE",
        "Taylor GOODHART",
        "Charles Drummond Swan"
    ],
    "classifications": [
        "G06F9/45558",
        "G06N20/00",
        "G06F9/455",
        "G06F9/5055",
        "G06N5/04",
        "G06F2009/45575",
        "G06F2009/45579"
    ],
    "abstract": "Techniques for packaging and deploying algorithms utilizing containers for flexible machine learning are described. In some embodiments, users can create or utilize simple containers adhering to a specification of a machine learning service in a provider network, where the containers include code for how a machine learning model is to be trained and/or executed. The machine learning service can automatically train a model and/or host a model using the containers. The containers can use a wide variety of algorithms and use a variety of types of languages, libraries, data types, etc. Users can thus implement machine learning training and/or hosting with extremely minimal knowledge of how the overall training and/or hosting is actually performed.",
    "claims": "\n1. A computer-implemented method comprising:\nreceiving, at a service provider network, a user-created container image comprising user-provided machine learning (ML) training code;\nadding the container image to a container registry;\nreceiving a request to start a job to train an ML model within the service provider network on behalf of the user, the request identifying a location of the container image available in the container registry, a number of containers to be used for the job, a storage location where one or more model artifacts generated as a result of training the ML model are to be stored, and a stopping value amount of time indicating a maximum duration for the training;\nrunning the ML model training job based at least in part on use of the identified location of the container image, the identified storage location, the identified number of containers, and the identified stopping value, comprising:\nobtaining the container image from the container registry based on use of the identified location;\nexecuting one or more containers via use of the container image and based on use of the identified number of containers;\nproviding training data to the one or more containers, whereby the user-provided ML training code executes and utilizes the training data to train the ML model for up to the duration indicated by the stopping value; and\nstoring the one or more model artifacts generated as a result of training the ML model at the storage location.\n2. The computer-implemented method of claim 1, wherein:\nthe training data is provided to the one or more containers as one or more files in a first local directory in each container or as one or more input streams accessible within the container; and\nstoring the one or more model artifacts comprises obtaining the one or more model artifacts from a second local directory in the container and sending the one or more model artifacts or an archived version of the one or more model artifacts to the storage location.\n3. The computer-implemented method of claim 1, further comprising:\nreceiving a second request to create an instance of the ML model within the service provider network, the second request identifying the storage location of the one or more model artifacts;\nexecuting a second container, the second container including inference code based at least in part on use of the one or more model artifacts;\nreceiving a request to perform an inference;\ngenerating an inference value by executing the inference code; and\nsending a response to the request that comprises the inference value.\n4. The computer-implemented method of claim 1, wherein the ML training code is written in the Python language.\n5. The computer-implemented method of claim 4, wherein the ML training code utilizes a TensorFlow deep learning framework or an XGBoost algorithm.\n6. The computer-implemented method of claim 1, wherein the one or more containers are Docker containers.\n7. The computer-implemented method of claim 1, wherein each of the one or more containers is executed by a virtual machine instance.\n8. The computer-implemented method of claim 1, wherein the running of the ML model training job occurs based on use of a user-provided value indicating a role or set of permissions to be used for accessing resources within the service provider network.\n9. A system comprising:\na first one or more electronic devices to implement a storage service of a service provider network;\na second one or more electronic devices to implement a container registry in the service provider network, the container registry including instructions that upon execution cause the service provider network to:\nreceive a user-created container image comprising user-provided machine learning (ML) training code;\nadd the container image to the container registry; and\na third one or more electronic devices to implement a machine learning service in the provider network, the machine learning service including instructions that upon execution cause the machine learning service to:\nreceive a request to start a job to train an ML model on behalf of the user, the request at least identifying a location of the container image within the container registry, a number of containers to be used for the job, a storage location provided by the storage service where one or more model artifacts generated as a result of training the ML model are to be stored, and a stopping value amount of time indicating a maximum duration for the training; and\nrun the ML model training job based at least in part on use of the identified location of the container image, the identified storage location, the identified number of containers, and the identified stopping value, wherein the machine learning service is to:\nobtain the container image from the container registry based on use of the identified location;\nexecute one or more containers via use of the container image and based on use of the identified number of containers;\nprovide training data to the one or more containers, whereby the user-provided ML training code executes and utilizes the training data to train the ML model for up to the duration indicated by the stopping value; and\nstore the one or more model artifacts generated as a result of the training of the ML model at the storage location provided by the storage service.\n10. The system of claim 9, wherein the ML training code is written in the Python language.\n11. The system of claim 4, wherein the ML training code utilizes a TensorFlow deep learning framework or an XGBoost algorithm.\n12. The system of claim 9, wherein to store the one or more model artifacts generated as a result of training the ML model, the machine learning service is to:\naccess the one or more model artifacts from a first local directory in the container; and\nstore the one or more model artifacts or an archived version of the one or more model artifacts at the storage location.\n13. The system of claim 12, wherein to provide training data to the one or more containers, the machine learning service is to:\nprovide the training data to each container as one or more files in a second local directory in the corresponding container or as one or more input streams accessible within the corresponding container.\n14. The system of claim 9, wherein the running of the ML model training job occurs based on use of a user-provided value indicating a role or set of permissions to be used for accessing resources within the service provider network.\n15. One or more non-transitory computer-readable media storing instructions which, when executed by one or more processors of one or more computing devices operative as part of a service provider network, cause the one or more computing devices to perform operations comprising:\nreceiving a user-created container image comprising user-provided machine learning (ML) training code;\nadding the container image to a container registry;\nreceiving a request to start a job to train an ML model within the service provider network on behalf of the user, the request identifying a location of the container image available in the container registry, a number of containers to be used for the job, a storage location where one or more model artifacts generated as a result of training the ML model are to be stored, and a stopping value amount of time indicating a maximum duration for the training;\nrunning the ML model training job based at least in part on use of the identified location of the container image, the identified storage location, the identified number of containers, and the identified stopping value, comprising:\nobtaining the container image from the container registry based on use of the identified location;\nexecuting one or more containers via use of the container image and based on use of the identified number of containers;\nproviding training data to the one or more containers, whereby the user-provided ML training code executes and utilizes the training data to train the ML model for up to the duration indicated by the stopping value; and\nstoring the one or more model artifacts generated as a result of training the ML model at the storage location.\n16. The one or more non-transitory computer-readable media of claim 15, wherein:\nthe training data is provided to the one or more containers as one or more files in a first local directory in each container or as one or more input streams accessible within the container; and\nstoring the one or more model artifacts comprises obtaining the one or more model artifacts from a second local directory in the container and sending the one or more model artifacts or an archived version of the one or more model artifacts to the storage location.\n17. The one or more non-transitory computer-readable media of claim 15, wherein the operations further comprise:\nreceiving a second request to create an instance of the ML model within the service provider network, the second request identifying the storage location of the one or more model artifacts;\nexecuting a second container, the second container including inference code based at least in part on use of the one or more model artifacts;\nreceiving a request to perform an inference;\ngenerating an inference value by executing the inference code; and\nsending a response to the request that comprises the inference value.\n18. The one or more non-transitory computer-readable media of claim 15, wherein the ML training code is written in the Python language.\n19. The one or more non-transitory computer-readable media of claim 18, wherein the ML training code utilizes a TensorFlow deep learning framework or an XGBoost algorithm.\n20. The one or more non-transitory computer-readable media of claim 15, wherein the one or more containers are Docker containers."
}