{
    "patent_id": "US-11055847-B2",
    "title": "Adversarial and dual inverse deep learning networks for medical image analysis ",
    "assignee": "Siemens Healthcare Gmbh",
    "publication_date": "2021-07-06",
    "patent_link": "https://patents.google.com/patent/US11055847B2/en",
    "inventors": [
        "Shaohua Kevin Zhou",
        "Mingqing Chen",
        "Daguang Xu",
        "Zhoubing XU",
        "Dong Yang"
    ],
    "classifications": [
        "G06T7/0012",
        "G06F18/24",
        "G06K9/6267",
        "G06K9/66",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/084",
        "G06N7/005",
        "G06N7/01",
        "G06T7/11",
        "G06V10/82",
        "G06V30/19173",
        "G06K2209/05",
        "G06T2207/10072",
        "G06T2207/10081",
        "G06T2207/10088",
        "G06T2207/10104",
        "G06T2207/10116",
        "G06T2207/10132",
        "G06T2207/20081",
        "G06T2207/20084",
        "G06V2201/03"
    ],
    "abstract": "Methods and apparatus for automated medical image analysis using deep learning networks are disclosed. In a method of automatically performing a medical image analysis task on a medical image of a patient, a medical image of a patient is received. The medical image is input to a trained deep neural network. An output model that provides a result of a target medical image analysis task on the input medical image is automatically estimated using the trained deep neural network. The trained deep neural network is trained in one of a discriminative adversarial network or a deep image-to-image dual inverse network.",
    "claims": "\n1. A method for automatically performing a medical image analysis task on a medical image of a patient, comprising:\nreceiving a medical image of a patient;\ninputting the medical image to a trained deep neural network; and\nautomatically estimating an output model that provides a result of a target medical image analysis task on the input medical image using the trained deep neural network, wherein the trained deep neural network is trained in a deep image-to-image dual inverse network.\n2. The method of claim 1, wherein the trained deep neural network is a first deep image-to-image network trained in the deep image-to-image dual inverse network and automatically estimating an output model that provides a result of a target medical image analysis task on the input medical image using the trained deep neural network comprises:\nautomatically generating an output image that provides a result of the target medical image analysis task on the input medical image using the first deep image-to-image network.\n3. The method of claim 2, wherein the deep image-to-image dual inverse network includes the first deep image-to-image network trained to perform the target medical image analysis task and including an encoder that coverts an input medical image to a feature representation and a decoder that generates a predicted output image from the feature representation of the input medical image, and a second deep image-to-image network trained to perform an inverse task to the target medical image analysis task and including an encoder that coverts an output image for the target medical image analysis task to a feature representation and a decoder that generates a predicted input image from the feature representation of the output image.\n4. The method of claim 3, wherein the first deep image-to-image network and the second deep image-to-image network are trained together based on a set of training samples including ground truth input images and corresponding ground truth output images to minimize a cost function including a first loss function that calculates an error between the ground truth output images and the predicted output images generated by the first deep image-to-image network from the ground truth input images, a second loss function that calculates an error between the ground truth output images and the predicted input images generated by the second deep image-to-image network from the ground truth output images, and a third loss function that calculates and error between the feature representation of the ground truth input images generated by the encoder of the first deep image-to-image network and the feature representation of the ground truth output images generated by the encoder of the second deep image-to-image network.\n5. The method of claim 4, further comprising:\ntraining the first deep image-to-image network and the second deep image-to-image network together to minimize the cost function by repeating the following training operations for a plurality of iterations:\nwith parameters of the second deep image-to-image network fixed, learning parameters of the first deep image-to-image network to minimize the first loss function and the third loss function; and\nwith the parameters of the first deep image-to-image network fixed, learning the parameters of the second deep image-to-image network to minimize the second loss function and the third loss function.\n6. An apparatus for automatically performing a medical image analysis task on a medical image of a patient, comprising:\nmeans for receiving a medical image of a patient;\nmeans for inputting the medical image to a trained deep neural network; and\nmeans for automatically estimating an output model that provides a result of a target medical image analysis task on the input medical image using the trained deep neural network, wherein the trained deep neural network is trained in a deep image-to-image dual inverse network.\n7. The apparatus of claim 6, wherein the trained deep neural network is a first deep image-to-image network trained in the deep image-to-image dual inverse network and the means for automatically estimating an output model that provides a result of a target medical image analysis task on the input medical image using the trained deep neural network comprises:\nmeans for automatically generating an output image that provides a result of the target medical image analysis task on the input medical image using the first deep image-to-image network.\n8. The apparatus of claim 7, wherein the deep image-to-image dual inverse network includes the first deep image-to-image network trained to perform the target medical image analysis task and including an encoder that coverts an input medical image to a feature representation and a decoder that generates a predicted output image from the feature representation of the input medical image, and a second deep image-to-image network trained to perform an inverse task to the target medical image analysis task and including an encoder that coverts an output image for the target medical image analysis task to a feature representation and a decoder that generates a predicted input image from the feature representation of the output image.\n9. The apparatus of claim 8, wherein the first deep image-to-image network and the second deep image-to-image network are trained together based on a set of training samples including ground truth input images and corresponding ground truth output images to minimize a cost function including a first loss function that calculates an error between the ground truth output images and the predicted output images generated by the first deep image-to-image network from the ground truth input images, a second loss function that calculates an error between the ground truth output images and the predicted input images generated by the second deep image-to-image network from the ground truth output images, and a third loss function that calculates and error between the feature representation of the ground truth input images generated by the encoder of the first deep image-to-image network and the feature representation of the ground truth output images generated by the encoder of the second deep image-to-image network.\n10. The apparatus of claim 9, further comprising:\nmeans for training the first deep image-to-image network and the second deep image-to-image network together to minimize the cost function by repeating the following training operations for a plurality of iterations:\nwith parameters of the second deep image-to-image network fixed, learning parameters of the first deep image-to-image network to minimize the first loss function and the third loss function; and\nwith the parameters of the first deep image-to-image network fixed, learning the parameters of the second deep image-to-image network to minimize the second loss function and the third loss function.\n11. A non-transitory computer readable medium storing computer program instructions for automatically performing a medical image analysis task on a medical image of a patient, the computer program instructions when executed by a processor cause the processor to perform operations comprising:\nreceiving a medical image of a patient;\ninputting the medical image to a trained deep neural network; and\nautomatically estimating an output model that provides a result of a target medical image analysis task on the input medical image using the trained deep neural network, wherein the trained deep neural network is trained in a deep image-to-image dual inverse network.\n12. The non-transitory computer readable medium of claim 11, wherein the trained deep neural network is a first deep image-to-image network trained in the deep image-to-image dual inverse network and automatically estimating an output model that provides a result of a target medical image analysis task on the input medical image using the trained deep neural network comprises:\nautomatically generating an output image that provides a result of the target medical image analysis task on the input medical image using the first deep image-to-image network.\n13. The non-transitory computer readable medium of claim 12, wherein the deep image-to-image dual inverse network includes the first deep image-to-image network trained to perform the target medical image analysis task and including an encoder that coverts an input medical image to a feature representation and a decoder that generates a predicted output image from the feature representation of the input medical image, and a second deep image-to-image network trained to perform an inverse task to the target medical image analysis task and including an encoder that coverts an output image for the target medical image analysis task to a feature representation and a decoder that generates a predicted input image from the feature representation of the output image.\n14. The non-transitory computer readable medium of claim 13, wherein the first deep image-to-image network and the second deep image-to-image network are trained together based on a set of training samples including ground truth input images and corresponding ground truth output images to minimize a cost function including a first loss function that calculates an error between the ground truth output images and the predicted output images generated by the first deep image-to-image network from the ground truth input images, a second loss function that calculates an error between the ground truth output images and the predicted input images generated by the second deep image-to-image network from the ground truth output images, and a third loss function that calculates and error between the feature representation of the ground truth input images generated by the encoder of the first deep image-to-image network and the feature representation of the ground truth output images generated by the encoder of the second deep image-to-image network.\n15. The non-transitory computer readable medium of claim 14, wherein the operations further comprise:\ntraining the first deep image-to-image network and the second deep image-to-image network together to minimize the cost function by repeating the following training operations for a plurality of iterations:\nwith parameters of the second deep image-to-image network fixed, learning parameters of the first deep image-to-image network to minimize the first loss function and the third loss function; and\nwith the parameters of the first deep image-to-image network fixed, learning the parameters of the second deep image-to-image network to minimize the second loss function and the third loss function.",
    "status": "Active",
    "citations_own": [
        "US20150170002A1",
        "US20150238148A1",
        "US20160063359A1",
        "US20160093048A1",
        "US20160140424A1",
        "US9373059B1",
        "US20160180195A1",
        "US20160174902A1",
        "US20160210749A1",
        "US20160328643A1",
        "US20170200067A1",
        "US20170249548A1",
        "US20180075581A1",
        "US20180130203A1",
        "US20180174049A1",
        "US20180211164A1"
    ],
    "citations_ftf": [],
    "citedby_own": [],
    "citedby_ftf": [
        "US10115194B2",
        "US11273553B2",
        "JP2019079374A",
        "US11263525B2",
        "US11250329B2",
        "US10482600B2",
        "US10910099B2",
        "US10956785B2",
        "EP3598161A1",
        "CN109166126B",
        "CN109345575B",
        "CN110941188A",
        "CN109559358B",
        "CN109583474B",
        "US11244453B2",
        "EP3657433B1",
        "EP3660741B1",
        "US11158069B2",
        "US20220058798A1",
        "TWI705340B",
        "CN111325671B",
        "US20220092785A1",
        "CN109635774B",
        "CN111383741B",
        "CN109948658B",
        "CN110070935B",
        "WO2020198854A1",
        "CN110033033B",
        "US11748887B2",
        "CN110021052B",
        "KR102034248B1",
        "KR102062157B1",
        "CN110197716B",
        "CN110276728B",
        "CN110175655B",
        "CN110245598B",
        "CN110222502A",
        "CN110458770A",
        "CN110597628B",
        "CN110851835A",
        "US11538576B2",
        "CN110930318B",
        "US11423544B1",
        "US10762629B1",
        "CN110889836A",
        "US11790492B1",
        "US11651220B2",
        "US20220415019A1",
        "US11348243B2",
        "WO2021150889A1",
        "US11710042B2",
        "US11532086B2",
        "US11663481B2",
        "CN111462264B",
        "CN111626917A",
        "US11699514B2",
        "CN112102294A",
        "CN112150378B",
        "CN112381788B",
        "US11762951B2",
        "CN113012204A",
        "EP4092621A1",
        "CN113269812A",
        "CN113539408B",
        "CN114005168A"
    ]
}