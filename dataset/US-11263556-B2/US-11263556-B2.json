{
    "patent_id": "US-11263556-B2",
    "title": "Apparatus and method of implementing batch-mode active learning for technology-assisted review of documents ",
    "assignee": "Legility Data Solutions, Llc",
    "publication_date": "2022-03-01",
    "patent_link": "https://patents.google.com/patent/US11263556B2/en",
    "inventors": [
        "Jeffrey A. Johnson",
        "Md Ahsan Habib",
        "Chandler L. Burgess",
        "Tanay Kumar Saha",
        "Mohammad Al Hasan"
    ],
    "classifications": [
        "G06N20/00",
        "G06F16/35",
        "G06N20/10",
        "G06N3/08",
        "G06N7/005",
        "G06N7/01"
    ],
    "abstract": "The present disclosure relates to the electronic document review field and, more particularly, to various apparatuses and methods of implementing batch-mode active learning for technology-assisted review (TAR) of documents (e.g., legal documents).",
    "claims": "\n1. A method to implement a diversity sampler process to select new batches of unlabeled instances, comprising:\nby one or more computing devices:\nobtaining a current version of a classification model M, an unlabeled set of available documents D, and a cosine similarity threshold t;\nsorting the unlabeled set of available documents D based on each of the documents absolute distance from the current version of the classification model M to obtain sorted indices I for each document of the unlabeled set of available documents D;\ninserting the sorted document, of the sorted unlabeled set of available documents D, having a nearest sorted index I[1] from the current version of the classification model M into a new batch of unlabeled instances Bc;\nobtaining sorted indices I of the sorted unlabeled set of available documents D that have a cosine angle \u2265t with respect to the inserted document I[1];\nremoving the documents with the obtained sorted indices I from the sorted unlabeled set of available documents D; and\nrepeating the insert operation, the second obtain operation, and the remove operation until the new batch of unlabeled instances Bc are selected.\n2. The method of claim 1, wherein the classification model M is a hyperplane.\n3. The method of claim 1, wherein the unlabeled set of available documents D are sorted in increasing order.\n4. The method of claim 1, wherein, in the first obtaining step, the one or more computing devises further obtains a batch size k.\n5. The method of claim 4, wherein the one or more computing devises performs the steps of repeating the insert operation, the second obtain operation, and the remove operation until k documents are inserted into the new batch of unlabeled instances Bc.\n6. The method of claim 1, wherein the Diversity Sampler process is implemented using a support vector machine (SVM).\n7. The method of claim 1, wherein the Diversity Sampler process is implemented in a technology-assisted document review.\n8. The method of claim 1, wherein the current version of the classification model M is created by:\nobtaining an unlabeled set of documents D;\nobtaining a batch size k;\nconstructing a first batch of k documents D;\nobtaining labels for the first batch of k documents D, wherein the labeled first batch of k documents D are referred to as training data documents; and\nconstructing the current version of the classification model M using the training documents.\n9. The method of claim 8, further comprising:\nobtaining labels for the new batch of unlabeled instances Bc; and\nadding the labeled new batch of instances Bc to a current version of the training data documents referred to as extended training data documents Dc.\n10. The method of claim 9, further comprising constructing an updated classification model M using the extended training data documents Dc.\n11. A system configured to implement a diversity sampler process to select new batches of unlabeled instances, the apparatus comprising:\na processor; and\na memory that stores processor-executable instructions, wherein the processor interfaces with the memory to execute the processor-executable instructions, whereby the system is operable to:\nobtain a current version of a classification model M, an unlabeled set of available documents D, and a cosine similarity threshold t;\nsort the unlabeled set of available documents D based on each of the documents absolute distance from the current version of the classification model M to obtain sorted indices I for each document of the unlabeled set of available documents D;\ninsert the sorted document, of the sorted unlabeled set of available documents D, having a nearest sorted index I[1] from the current version of the classification model M into a new batch of unlabeled instances Bc;\nobtain sorted indices I of the sorted unlabeled set of available documents D that have a cosine angle \u2265t with respect to the inserted document I[1];\nremove the documents with the obtained sorted indices I from the sorted unlabeled set of available documents D; and\nrepeat the insert operation, the second obtain operation, and the remove operation until the new batch of unlabeled instances Bc are selected.\n12. The system of claim 11, wherein the diversity sampler process is implemented using a support vector machine (SVM).\n13. The system of claim 11, wherein the current version of the classification model M is created by:\nobtaining an unlabeled set of documents D;\nobtaining a batch size k;\nconstructing a first batch of k documents D;\nobtaining labels for the first batch of k documents D, wherein the labeled first batch of k documents D are referred to as training data documents; and\nconstructing the current version of the classification model M using the training documents.\n14. The system of claim 13, wherein the system is further operable to:\nobtain labels for the new batch of unlabeled instances Bc; and\nadd the labeled new batch of instances Bc to a current version of the training data documents referred to as extended training data documents Dc.\n15. The system of claim 14, wherein the system is further operable to construct an updated classification model M using the extended training data documents Dc.\n16. A method to implement a biased probabilistic sampler process to select new batches of unlabeled instances, comprising:\nby one or more computing devices:\nobtaining a current version of a classification model M, an unlabeled set of available documents D, and a cosine similarity threshold t;\nconstructing a weight vector w based on an inverse operation of a distance from the current version of the classification model M for each document from the unlabeled set of available documents D;\nnormalizing the weight vector w for each document to convert the weight vector w into a probability vector;\nchoosing a document I[1] from the unlabeled set of available documents D using the corresponding probability vector;\ninserting the chosen document I[1] into a new batch of unlabeled instances Bc;\nidentifying documents of the remaining unlabeled set of available documents D that have a cosine similarity angle \u2265t with respect to the chosen document I[1];\nremoving the identified documents from the unlabeled set of available documents D;\nre-normalizing the weight vector w for each remaining document in the set of documents D; and\nrepeating the choose operation, the insert operation, the identify operation, the remove operation, and the re-normalize operation until the new batch of unlabeled instances Bc are selected.\n17. The method of claim 16, wherein, in the obtaining step, the one or more computing devices further obtains a batch size k.\n18. The method of claim 17, wherein the one or more computing devises performs the steps of repeating the choose operation, the insert operation, the identify operation, the remove operation, and the re-normalize operation until k documents are inserted into the new batch of unlabeled instances Bc.\n19. The method of claim 16, wherein the current version of the classification model M is created by:\nobtaining an unlabeled set of documents D;\nobtaining a batch size k;\nconstructing a first batch of k documents D;\nobtaining labels for the first batch of k documents D, wherein the labeled first batch of k documents D are referred to as training data documents; and\nconstructing the current version of the classification model M using the training data documents.\n20. The method of claim 19, further comprising:\nobtaining labels for the new batch of unlabeled instances Bc; and\nadding the labeled new batch of instances Bc to a current version of the training data documents referred to as extended training data documents Dc."
}