{
    "patent_id": "US-11710303-B2",
    "title": "Systems and methods for prioritizing object prediction for autonomous vehicles ",
    "assignee": "Uatc, Llc",
    "publication_date": "2023-07-25",
    "patent_link": "https://patents.google.com/patent/US11710303B2/en",
    "inventors": [
        "Galen Clark Haynes"
    ],
    "classifications": [
        "G06V10/82",
        "B60W30/095",
        "B60W30/0956",
        "B60W40/00",
        "B60W50/06",
        "B60W60/00276",
        "G05D1/0088",
        "G05D1/0212",
        "G05D1/0221",
        "G06N20/00",
        "G06T7/70",
        "G06V10/764",
        "G06V20/58",
        "G08G1/0112",
        "B60W2050/065",
        "B60W2554/00",
        "B60W2554/402",
        "G05D1/0231",
        "G05D1/0257",
        "G05D2201/0213",
        "G08G1/0129",
        "G08G1/0133"
    ],
    "abstract": "Systems and methods for determining object prioritization and predicting future object locations for an autonomous vehicle are provided. A method can include obtaining, by a computing system comprising one or more processors, state data descriptive of at least a current or past state of a plurality of objects that are perceived by an autonomous vehicle. The method can further include determining, by the computing system, a priority classification for each object in the plurality of objects based at least in part on the respective state data for each object. The method can further include determining, by the computing system, an order at which the computing system determines a predicted future state for each object based at least in part on the priority classification for each object and determining, by the computing system, the predicted future state for each object based at least in part on the determined order.",
    "claims": "\n1. An autonomous vehicle control system comprising:\none or more processors; and\none or more computer-readable medium storing instructions that when executed by the one or more processors cause the control system to perform operations, the operations comprising:\nobtaining state data descriptive of at least a current state or a past state of a plurality of objects that are detected by one or more sensors of an autonomous vehicle;\ndetermining a priority classification for a particular object relative to the plurality of objects based, at least in part, on respective state data for the particular object;\ndetermining a predicted future state for the particular object of the plurality of objects based, at least in part, on the priority classification for the particular object; and\ninitiating a motion of the autonomous vehicle based, at least in part, on the predicted future state of the particular object.\n2. The control system of claim 1, wherein determining the predicted future state for the particular object of the plurality of objects based, at least in part, on the priority classification for the particular object comprises:\ndetermining an order for determining a respective predicted future state for each of the plurality of objects based, at least in part, on the priority classification for the particular object; and\ndetermining the predicted future state for the particular object of the plurality of objects based, at least in part, on the order for determining the respective predicted future state for each of the plurality of objects.\n3. The control system of claim 2, wherein determining the predicted future state for the particular object of the plurality of objects based, at least in part, on the order for determining the respective predicted future state for each of the plurality of objects, further comprises:\nidentifying one or more objects in a first priority category; and\nidentifying one or more objects in a second priority category, wherein the order for determining the respective predicted future state for each of the plurality of objects is determined such that the respective predicted future state for the one or more objects in the first priority category are determined before the respective predicted future state for the one or more objects in the second priority category.\n4. The control system of claim 3, wherein determining the predicted future state for the particular object of the plurality of objects based, at least in part, on the priority classification for the particular object, further comprises:\ndetermining the respective predicted future state for each of the one or more objects in the first priority category based, at least in part, on current state data obtained for a current time frame.\n5. The control system of claim 3, wherein determining the predicted future state for the particular object of the plurality of objects based, at least in part, on the priority classification for the particular object, comprises:\ndetermining the respective predicted future state for each of the one or more objects in the second priority category based, at least in part, on past state data obtained at a previous time frame.\n6. The control system of claim 3, wherein initiating the motion of the autonomous vehicle based, at least in part, on the predicted future state of the particular object, comprises:\ndetermining a motion plan for the autonomous vehicle based, at least in part, on the predicted future state of the particular object, wherein the motion of the autonomous vehicle is based, at least in part, on the motion plan.\n7. The control system of claim 3, wherein determining the predicted future state for the particular object of the plurality of objects based, at least in part, on the priority classification for the particular object, comprises:\nidentifying the respective predicted future state for each of the one or more objects in the first priority classification at a current time frame; and\nidentifying the respective predicted future state for each of the one or more objects in the second priority classification at a subsequent time frame subsequent to the current time frame.\n8. The control system of claim 3, wherein determining the predicted future state for the particular object of the plurality of objects based, at least in part, on the priority classification for the particular object, comprises:\nselecting at least one future location prediction system of a plurality of future location prediction systems based, at least in part, on the priority classification for the particular object; and\ndetermining the predicted future state for the particular object using the at least one future location prediction system.\n9. The control system of claim 8, wherein the plurality of future location prediction systems comprise a high-fidelity prediction system and a low-fidelity prediction system, wherein the high-fidelity prediction system can be associated with a higher computational intensity relative to the low-fidelity prediction system.\n10. The control system of claim 9, wherein determining the predicted future state for the particular object of the plurality of objects based, at least in part, on the priority classification for the particular object, comprises:\ndetermining the respective predicted future state for each of the one or more objects in the first priority category using the high-fidelity prediction system; and\ndetermining the respective predicted future state for each of the one or more objects in the second priority category using the low-fidelity prediction system.\n11. The control system of claim 9, wherein the high-fidelity prediction system leverages one or more machine-learned models that predict a future location for a respective object, the one or more machine-learned models comprising a machine-learned goal-scoring model that is trained to generate and score one or more predicted goals for the respective object and a machine-learned trajectory development model that is trained to determine an object trajectory for at least one of the one or more predicted goals for the respective object, wherein the future location for the respective object is based, at least in part, on the object trajectory.\n12. The control system of claim 9, wherein the low-fidelity prediction system leverages a state forward-integration model that predicts a future state for a respective object by forward integrating a respective current state for the respective object.\n13. An autonomous vehicle comprising:\none or more processors; and\none or more computer-readable medium storing instructions that when executed by the one or more processors cause the autonomous vehicle to perform operations, the operations comprising:\nobtaining state data descriptive of at least a current state or a past state of a plurality of objects that are detected by one or more sensors of the autonomous vehicle;\ndetermining a priority classification for particular object relative to the plurality of objects based, at least in part, on respective state data for the particular object;\ndetermining a predicted future state for the particular object of the plurality of objects based, at least in part, on the priority classification for the particular object; and\ninitiating a motion of the autonomous vehicle based, at least in part, on the predicted future state of the particular object.\n14. The autonomous vehicle of claim 13, wherein the current state or the past state of the plurality of objects is indicative of (i) a distance from the autonomous vehicle, (ii) a minimum path to an interaction with the autonomous vehicle, or (iii) a minimum time duration to the interaction with the autonomous vehicle.\n15. The autonomous vehicle of claim 14, wherein the priority classification is determined based, at least in part, on (i) the distance from the autonomous vehicle, (ii) the minimum path to the interaction with the autonomous vehicle, or (iii) the minimum time duration to the interaction with the autonomous vehicle.\n16. The autonomous vehicle of claim 13, wherein the priority classification is determined based, at least in part, on a threshold velocity or a velocity range of the autonomous vehicle.\n17. A computer-implemented method, comprising:\nobtaining state data descriptive of at least a current state or a past state of a plurality of objects that are detected by one or more sensor of an autonomous vehicle;\ndetermining a priority classification for a particular object relative to the plurality of objects based, at least in part, on respective state data for the particular object;\ndetermining a predicted future state for the particular object of the plurality of objects based, at least in part, on the priority classification for the particular object; and\n18. The computer-implemented method of claim 17, wherein determining the priority classification for the particular object relative to the plurality of objects based, at least in part, on the respective state data for the particular object comprises:\nproviding the state data as an input to a machine-learned object classifier model; and\nreceiving as an output of the machine-learned object classifier model, in response to receipt of the state data, a respective priority classification for each of the plurality of objects.\n19. The computer-implemented method of claim 18, wherein the machine-learned object classifier model is trained based, at least in part, on training data previously collected during one or more previous autonomous vehicle driving sessions.\n20. The computer-implemented method of claim 19, wherein the training data comprises previously recorded state data indicative of one or more previously perceived objects during the one or more previous autonomous vehicle driving sessions, wherein the training data comprises one or more object labels corresponding to the one or more previously perceived objects.",
    "status": "Active",
    "citations_own": [
        "US20090268946A1",
        "DE102009050503A1",
        "US20100328644A1",
        "US7899621B2",
        "US8126642B2",
        "JP2014157522A",
        "US20140330479A1",
        "US9164511B1",
        "US9302678B2",
        "CN105679021A",
        "US9373149B2",
        "US9440647B1",
        "US20170031361A1",
        "CN106504530A",
        "US20170120804A1",
        "US20170120902A1",
        "CN107024215A",
        "US20180032082A1"
    ],
    "citations_ftf": [],
    "citedby_own": [],
    "citedby_ftf": [
        "US10678244B2",
        "US10671349B2",
        "US11157441B2",
        "US11409692B2",
        "US10768626B2",
        "US11161464B2",
        "US11561791B2",
        "US10906536B2",
        "US10990096B2",
        "JP6988698B2",
        "US11215999B2",
        "US11035943B2",
        "US10909866B2",
        "US11636333B2",
        "US11562231B2",
        "US11196678B2",
        "US11403492B2",
        "US11256263B2",
        "US11537811B2",
        "US10814870B2",
        "KR102569134B1",
        "KR102505300B1",
        "US11610117B2",
        "US10997461B2",
        "US11567514B2",
        "US10956755B2",
        "US10741070B1",
        "US11335191B2",
        "US11335189B2",
        "US11403938B2",
        "US11341846B2",
        "US10699564B1",
        "RU2750152C1",
        "US20200363800A1",
        "US11772643B1",
        "US11526729B2",
        "US11643115B2",
        "US11634162B2",
        "US11340622B2",
        "US11577722B1",
        "DE102019216025A1",
        "US11636715B2",
        "KR20210114792A",
        "EP3913551A1",
        "CN111814970B",
        "CN114500736A",
        "US20220169282A1",
        "WO2022183329A1",
        "US20220379911A1",
        "WO2022251769A1",
        "US20230074873A1",
        "CN116588125B"
    ]
}