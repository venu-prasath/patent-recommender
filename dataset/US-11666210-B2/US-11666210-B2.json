{
    "patent_id": "US-11666210-B2",
    "title": "System for recognizing diabetic retinopathy ",
    "assignee": "Shenzhen Sibionics Technology Co., Ltd., Shenzhen Sibright Technology Co., Ltd.",
    "publication_date": "2023-06-06",
    "patent_link": "https://patents.google.com/patent/US11666210B2/en",
    "inventors": [
        "Juan Wang",
        "Bin Xia",
        "Yujing Bai",
        "Xiaoxin LI",
        "Zhigang Hu",
        "Yu Zhao"
    ],
    "classifications": [
        "A61B3/0025",
        "A61B3/12",
        "A61B3/14",
        "G06N20/10",
        "G06N20/20",
        "G06N3/045",
        "G06N3/08",
        "G06T7/0012",
        "G06T7/0014",
        "G06T2207/20081",
        "G06T2207/20084",
        "G06T2207/30041",
        "G06T2207/30096"
    ],
    "abstract": "Some embodiments of the disclosure provide an artificial neural network system for recognizing a lesion in a fundus image. The system includes a pre-processing module configured to pre-process a target fundus image and a reference fundus image taken from one person separately, a first neural network (12) configured to generate a first advanced feature set from the target fundus image, a second neural network (22) configured to generate a second advanced feature set from the reference fundus image, a feature combination module (13) configured to combine the first advanced feature set and the second advanced feature set to form a feature combination set, and a third neural network (14) configured to generate, according to the feature combination set, a judgmental result of lesions.",
    "claims": "\n1. A system for recognizing a diabetic retinopathy, comprising:\na first neural network configured to generate a first advanced feature set from a target retinal fundus image;\na second neural network configured to generate a second advanced feature set from a reference retinal fundus image;\na feature combination module configured to combine the first advanced feature set and the second advanced feature set to form a feature combination set; and\na third neural network configured to generate a diagnosis result according to the feature combination set;\nwherein:\nthe target retinal fundus image and the reference retinal fundus image are taken from one person; and\nfundus images of a same resolution are used during a training process of the system.\n2. The system according to claim 1, further comprising a pre-processing module configured to separately pre-process the target retinal fundus image and the reference retinal fundus image.\n3. The system according to claim 1, wherein:\nthe target retinal fundus image is an image of one eye of the one person; and\nthe reference retinal fundus image is an image of the other eye of the one person.\n4. The system according to claim 1, wherein the target retinal fundus images and the reference retinal fundus images are color images or grayscale images.\n5. The system according to claim 1, wherein the first neural network and the second neural network are the same.\n6. The system according to claim 1, wherein the first neural network and the second neural network are both convolutional neural network.\n7. The system according to claim 1, wherein:\nthe first neural network comprises a first convolutional layer, a second convolutional layer, a third convolutional layer, and a fourth convolutional layer;\nthe first convolution layer uses a 5\u00d75 convolutional kernel;\nthe second convolutional layer uses a 3 \u00d73 convolutional kernel;\nthe third convolutional layer uses a 3 \u00d73 convolutional kernel; and\nthe fourth convolutional layer uses a 3 \u00d73 convolutional kernel.\n8. The system according to claim 2, wherein the pre-processing module comprises:\nan area detection unit configured to detect designated fundus areas in the target retinal fundus image and the reference retinal fundus image;\nan adjustment unit configured to clip and resize the target retinal fundus image and the reference retinal fundus image; and\na normalization unit configured to normalize the target retinal fundus image and the reference retinal fundus image.\n9. The system according to claim 8, wherein the area detection unit detects a specific area in the retinal fundus image by at least one method selected from the group consisting of a sampling threshold method and a Hough transform method.\n10. The system according to claim 8, wherein the adjustment unit is configured to resize the target fundus image and the reference fundus image to at least one size selected from the group consisting of 128\u00d7128, 256\u00d7256, 512\u00d7512, 768\u00d7768, 1024\u00d71024, and 2048\u00d72048 pixels.\n11. The system according to claim 8, wherein the adjustment unit is configured to distinguish a retinal fundus area from a patient information area.\n12. The system according to claim 8, wherein the normalization unit is configured to normalize the target retinal fundus image and the reference retinal fundus image by at least one method selected from the group consisting of a zero mean method and a unit standard deviation method.\n13. The system according to claim 8, wherein the pre-processing module comprises an expansion unit configured to perform sample expansion through various transformations of the target retinal fundus image and reference fundus image.\n14. The system according to claim 8, wherein:\nthe pre-processing module comprises a first pre-processing module and a second pre-processing module;\nthe first pre-processing module is configured to perform retinal fundus area detection, image clipping, image resizing, and image normalizing for the target retinal fundus image; and\nthe second pre-processing module is configured to perform retinal fundus area detection, image clipping, image resizing, image normalizing for the reference retinal fundus image.\n15. The system according to claim 1, wherein the third neural network is configured to generate the diagnosis result according to the feature combination set and the patient information.\n16. The system according to claim 15, wherein the patient information comprises at least one of age, gender, eyesight, and medical history.\n17. The system according to claim 1, wherein the feature combination module is configured to perform a linear transformation or a nonlinear transformation on the first advanced feature set and the second advanced feature set to obtain the feature combination set.\n18. The system according to claim 1, wherein:\nthe first neural network is configured to generate the first advanced feature set from the target retinal fundus image using a first deep learning method; and\nthe second neural network is configured to generate the second advanced feature set from the reference retinal fundus image using a second deep learning method; and\nthe second deep learning method is the same as or different from the first deep learning method.\n19. The system according to claim 18, wherein:\nthe deep learning method utilizes four fundus images including two fundus images taken from a left eye of the one person and two fundus images taken from a right eye of the one person;\nthe four fundus images cover specific areas of the left eye and the right eye; and\nthe four fundus images are fundus images with a visual angle of 45 degrees.\n20. The system according to claim 1, wherein:\nthe third neural network module is configured to output probabilities of various lesion categories;\na probability sum of the third neural network module is 1; and\nthe third neural network module is configured to determine that a fundus has a lesion of a category when a probability of the category is the highest among various probabilities.",
    "status": "Active",
    "citations_own": [
        "US20150125052A1",
        "CN105701468A",
        "US20170112372A1",
        "CN107358606A",
        "CN107423571A",
        "US20180293737A1",
        "US10610098B1",
        "JP6745496B2",
        "US20200349710A1"
    ],
    "citations_ftf": [
        "CN202537482U",
        "CN103870838A",
        "CN103871061A",
        "CN104933715A",
        "US9949714B2",
        "CN106056595B",
        "CN105513077B",
        "CN106355575A",
        "CN106408562B",
        "CN106408564B",
        "CN106344005B",
        "CN106530295A"
    ],
    "citedby_own": [],
    "citedby_ftf": [
        "CN108172291B",
        "CN108771530B",
        "KR102421376B1",
        "US11200665B2",
        "CN109390053B",
        "JP7224757B2",
        "CN110399929B",
        "CN108399619B",
        "CN108154505A",
        "CN108021916B",
        "CN110037651B",
        "US11080850B2",
        "CN108230322B",
        "CN108388841B",
        "US11119915B2",
        "CN108470359A",
        "CN108542390A",
        "US11448632B2",
        "CN108614548B",
        "CN108231194A",
        "US11636340B2",
        "CN108596900B",
        "US11461599B2",
        "CN110335269A",
        "CN112236832A",
        "US11393082B2",
        "CN109448855A",
        "RU2709661C1",
        "CN109376767B",
        "CN109299697A",
        "JP7129870B2",
        "US11715059B2",
        "CN109363697B",
        "CN109363698B",
        "CN109376777A",
        "CN109671049A",
        "WO2020106332A1",
        "CN109658385A",
        "US10963757B2",
        "CN109686440A",
        "CN109411086A",
        "CN109886143A",
        "CN111507981B",
        "CN109998599A",
        "CN111724450A",
        "WO2020200087A1",
        "EP3924935A1",
        "CN109977905B",
        "CN110033422B",
        "CN110210286A",
        "CN110334575B",
        "CN110327013B",
        "CN110400288B",
        "CN110400289B",
        "CN110276411B",
        "WO2021020442A1",
        "EP4006915A4",
        "CN110598652B",
        "CN110728666B",
        "CN110728674B",
        "SG10201910949PA",
        "CN115511861A",
        "CN111096727B",
        "CN111260635B",
        "CN111460991A",
        "TWI734449B",
        "CN111583248B",
        "CN111640498A",
        "CN111754486B",
        "WO2022005336A1",
        "CN111803024B",
        "CA3194441A1",
        "TWI746287B",
        "CN112884729B",
        "CN112869706A",
        "CN113506284B",
        "TWI796784B",
        "CN114998353B"
    ]
}