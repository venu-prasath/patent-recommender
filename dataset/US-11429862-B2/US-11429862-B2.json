{
    "patent_id": "US-11429862-B2",
    "title": "Dynamic adaptation of deep neural networks ",
    "assignee": "Sri International",
    "publication_date": "2022-08-30",
    "patent_link": "https://patents.google.com/patent/US11429862B2/en",
    "inventors": [
        "Sek Meng Chai",
        "Aswin Nadamuni Raghavan",
        "Samyak Parajuli"
    ],
    "classifications": [
        "G06N3/084",
        "G06N3/006",
        "G06N3/04",
        "G06N3/044",
        "G06N3/045",
        "G06N3/065",
        "G06N3/088",
        "G06N3/10",
        "G06N3/048",
        "G06N3/082",
        "G06N5/045"
    ],
    "abstract": "Techniques are disclosed for training a deep neural network (DNN) for reduced computational resource requirements. A computing system includes a memory for storing a set of weights of the DNN. The DNN includes a plurality of layers. For each layer of the plurality of layers, the set of weights includes weights of the layer and a set of bit precision values includes a bit precision value of the layer. The weights of the layer are represented in the memory using values having bit precisions equal to the bit precision value of the layer. The weights of the layer are associated with inputs to neurons of the layer. Additionally, the computing system includes processing circuitry for executing a machine learning system configured to train the DNN. Training the DNN comprises optimizing the set of weights and the set of bit precision values.",
    "claims": "\n1. A computing system that trains a deep neural network (DNN) for reduced computing resource requirements, the computing system comprising:\na memory storing a set of weights of the DNN, the DNN including a plurality of layers, wherein for each layer of the plurality of layers,\nthe set of weights includes weights of the layer and a set of bit precision values includes a bit precision value of the layer,\nthe weights of the layer being represented in the memory using values having bit precisions equal to the bit precision value of the layer,\nthe weights of the layer being associated with inputs to neurons of the layer; and\nprocessing circuitry for executing a machine learning system configured to train the DNN, wherein training the DNN comprises optimizing the set of weights and the set of bit precision values.\n2. The computing system of claim 1, wherein the machine learning system is configured such that, as part of training the DNN, the machine learning system:\napplies a backpropagation algorithm over a plurality of iterations, wherein each iteration of the backpropagation algorithm updates the set of weights and optimizes the set of bit precision values.\n3. The computing system of claim 1, wherein two or more of the layers of the DNN have different bit precision values.\n4. The computing system of claim 1, wherein:\nthe set of weights is a first set of weights,\nthe memory stores a second set of weights that includes a fixed precision set of weights for each layer in the plurality of layers, each weight in the second set of weights having a bit precision equal to a predefined maximum bit precision value, and\nthe machine learning system is configured such that, as part of training the DNN, the machine learning system performs a plurality of iterations to train the DNN, wherein the machine learning system is configured such that, as part of performing the plurality of iterations, the machine learning system, for each iteration of the plurality of iterations:\nuses the second set of weights as weights of inputs of neurons in the DNN to calculate a first output data set based on a first input data set,\ndetermines a loss function;\nupdates the second set of weights based on the loss function;\nupdates the set of bit precision values based on the loss function; and\nafter updating the second set of weights and after updating the set of bit precision values, updates the first set of weights based on the updated second set of weights and the updated set of bit precision values, and\nthe machine learning system is further configured to use the first set of weights as the weights of the inputs of the neurons in the DNN to calculate a second output data set based on a second input data set.\n5. The computing system of claim 4, wherein the machine learning system is configured such that, as part of determining the loss function, the machine learning system:\ndetermines a first operand, the first operand being an intermediate loss function;\ndetermines a second operand such that the second operand is equal to a multiplication product of a value of a first hyperparameter and a sum of quantization errors for each of the layers in the plurality of layers;\ndetermines a third operand such that the third operand is equal to a multiplication product of the value of a second hyperparameter and \u03a3i=1 N2b i , where i is an index, N is a total number of layers in the plurality of layers, and bi is the bit precision value for the i\u2032th layer in the plurality of layers; and\ndetermines the loss function as the sum of the first operand, the second operand, and the third operand.\n6. The computing system of claim 5, wherein the machine learning system is further configured to:\nfor each layer of the plurality of layers, determine the quantization errors for the layer based on differences between weights of the layer in the first set of weights and weights of the layer in the second set of weights.\n7. The computing system of claim 5, wherein:\nthe first input data set comprises a batch of training data-label pairs, the machine learning system is configured such that, as part of determining the first operand, the machine learning system determines the first operand such that the first operand is equal to:\n8. The computing system of claim 4, wherein the machine learning system is configured such that, as part of updating the set of bit precision values, the machine learning system:\ndetermines the updated set of bit precision values such that the updated set of bit precision values is set equal to:\n9. The computing system of claim 4, wherein the machine learning system is configured such that, as part of updating the second set of weights, the machine learning system:\ndetermines the updated second set of weights such that the updated second set of weights is set equal to:\n10. The computing system of claim 4, wherein the machine learning system is configured such that, as part of updating the first set of weights, the machine learning system:\ndetermines an updated first set of weights such that, for each layer of the plurality of layers, updated precision-optimized weights for the layer are set equal to:\n11. The computing system of claim 10, wherein \u03b4 is equal\n12. The computing system of claim 4, wherein, for each layer of the plurality of layers, the machine learning system is configured to:\ndetermine an updated first parameter for the layer such that the updated first parameter for the layer is set equal to:\n13. The computing system of claim 4, wherein, for each weight of the first set of weights, the machine learning system is configured such that, as part of updating the first set of weights, the machine learning system:\ndetermines the weight of the first set of weights to be equal to a sign value multiplied by two to the power of an exponent value, the sign value indicating a sign of a corresponding weight in the second set of weights, and the exponent value being based on a log base 2 of the corresponding weight in the second set of weights.\n14. The computing system of claim 1, wherein each weight in the set of weights is equal to a power of 2.\n15. The computing system of claim 1, wherein each weight is represented in the memory using integer values having the bit precisions equal to the bit precision value of the layer, an offset value, and a quantization step size value.\n16. The computing system of claim 1, wherein the machine learning system is configured to train the DNN based on system architecture parameters that describe a system architecture.\n17. The computing system of claim 1, wherein:\nthe machine learning system is further configured to:\nobtain system architecture parameters that include data describing processors in a system architecture; and\ndetermine, based on the system architecture parameters, a neural network software architecture and a mapping of DNNs in the neural network software architecture to processors of the system architecture, the neural network software architecture including the DNN, and\nprocessors of the system architecture mapped to DNNs of the neural network software architecture are configured to execute the DNNs of the neural network software architecture.\n18. The computing system of claim 1, wherein:\nthe set of weights is a first set of weights,\nthe memory further stores a second set of weights of the DNN and a set of bit precision values,\nfor each layer of the plurality of layers:\nthe first set of weights includes first weights of the layer, wherein each weight in the first set of weights is equal to a power of 2,\nthe second set of weights includes second weights of the layer, wherein the second set of weights of the layer have a higher precision than the first set of weights of the layer,\nthe first weights of the layer are represented in the memory using values having bit precisions equal to a predefined maximum bit precision value\nthe first weights of the layer and the second weights of the layer are associated with inputs to neurons of the layer,\nthe second set of weights includes a fixed precision set of weights for each layer in the plurality of layers\nas part of training the DNN, the machine learning system performs a plurality of iterations to train the DNN, wherein the machine learning system is configured to, as part of performing the plurality of iterations, for each iteration of the plurality of iterations:\nuse the second set of weights as weights of inputs of neurons in the DNN to calculate a first output data set based on a first input data set,\ndetermine a loss function,\nupdate the second set of weights based on a first gradient of the loss function to determine the updated second set of weights,\nupdate the set of bit precision values based on a second gradient of the loss function to determine the updated bit precision values, and\nupdate the first set of weights based on the updated second set of weights and the updated bit precision values,\nwherein the loss function is defined as a sum of a first operand, a second operand, and a third operand,\nwherein the first operand being is an intermediate loss function, wherein the intermediate loss function captures a mean log-likelihood,\nwherein the second operand is equal to a multiplication product of a value of a first hyperparameter and a distillation loss that indicates a difference between using an output generated by the DNN when the machine learning system runs the DNN on the first input data set using the first set of weights and using the second set of weights, and\nwherein the third operand is equal to a multiplication product of the value of a second hyperparameter and a sum of terms that correspond to respective layers of the plurality of layers, wherein for each respective layer of the plurality of layers, the term corresponding to the respective layer is equal to 2 to the power of the bit precision value for the respective layer; and\nthe machine learning system is further configured to use the first set of weights as the weights of the inputs of the neurons in the DNN to calculate a second output data set based on a second input data set.\n19. A method of training a deep neural network (DNN) for reduced computing resource requirements, the method comprising:\nstoring a set of weights of the DNN and a set of bit precision values of the DNN, the DNN including a plurality of layers, wherein for each layer of the plurality of layers, the set of weights includes weights of the layer and the set of bit precision values includes a bit precision value of the layer, the weights of the layer being represented in memory using values having bit precisions equal to the bit precision value of the layer, the weights of the layer being associated with inputs to neurons of the layer; and\nexecuting a machine learning system configured to train the DNN, wherein training the DNN comprises optimizing the set of weights and the set of bit precision values.\n20. The method of claim 19, wherein training the DNN comprises:\napplying a backpropagation algorithm over a plurality of iterations, wherein each iteration of the backpropagation algorithm updates the set of weights and optimizes the set of bit precision values.\n21. The method of claim 19, wherein two or more of the layers of the DNN have different bit precisions.\n22. The method of claim 19, wherein:\nthe set of weights is a first set of weights,\nthe method further comprises storing a second set of weights that includes a fixed precision set of weights for each layer in the plurality of layers, each weight in the second set of weights having a bit precision equal to a predefined maximum bit precision value, and\ntraining the DNN comprises performing a plurality of iterations to train the DNN, wherein performing the plurality of iterations comprises, for each iteration of the plurality of iterations:\nusing the second set of weights as weights of inputs of neurons in the DNN to calculate a first output data set based on a first input data set,\ndetermining a loss function;\nupdating the second set of weights based on the loss function;\nupdating the set of bit precision values based on the loss function; and\nafter updating the second set of weights and after updating the set of bit precision values, updating the first set of weights based on the updated second set of weights and the updated set of bit precision values, and\nthe method further comprises using the first set of weights as the weights of the inputs of the neurons in the DNN to calculate a second output data set based on a second input data set.\n23. The method of claim 22, wherein determining the loss function comprises:\ndetermining a first operand, the first operand being an intermediate loss function;\ndetermining a second operand such that the second operand is equal to a multiplication product of a value of a first hyperparameter and a sum of quantization errors for each of the layers in the plurality of layers;\ndetermining a third operand such that the third operand is equal to a multiplication product of the value of a second hyperparameter and \u03a3i=1 N2b i , where i is an index, N is a total number of layers in the plurality of layers, and bi is the bit precision value for the i\u2032th layer in the plurality of layers; and\ndetermining the loss function as the sum of the first operand, the second operand, and the third operand.\n24. The method of claim 23, further comprising:\nfor each layer of the plurality of layers, determining the quantization errors for the layer based on differences between weights of the layer in the first set of weights and weights of the layer in the second set of weights.\n25. The method of claim 23, wherein:\nthe first input data set comprises a batch of training data-label pairs, determining the first operand comprises determining the first operand such that the first operand is equal to:\n26. The method of claim 22, wherein updating the set of bit precision values comprises:\ndetermining the updated set of bit precision values such that the updated set of bit precision values is set equal to:\n27. The method of claim 22, wherein updating the second set of weights comprises:\ndetermining the updated second set of weights such that the updated second set of weights is set equal to:\n28. The method of claim 22, wherein updating the first set of weights comprises:\ndetermining an updated first set of weights such that, for each layer of the plurality of layers, updated precision-optimized weights for the layer are set equal to:\n29. The method of claim 28, wherein \u03b4 is equal\n30. The method of claim 22, wherein updating the set of bit precision values comprises, for each layer of the plurality of layers:\ndetermining an updated first parameter for the layer such that the updated first parameter for the layer is set equal to:\n31. The method of claim 22, wherein updating the first set of weights comprises, for each weight of the first set of weights:\ndetermining the weight of the first set of weights to be equal to a sign value multiplied by two to the power of an exponent value, the sign value indicating a sign of a corresponding weight in the second set of weights, and the exponent value being based on a log base 2 of the corresponding weight in the second set of weights.\n32. The method of claim 19, wherein each weight in the set of weights is equal to a power of 2.\n33. The method of claim 19, wherein each weight is represented in the memory using integer values having the bit precisions equal to the bit precision value of the layer, an offset value, and a quantization step size value.\n34. The method of claim 19, wherein the machine learning system is configured to train the DNN based on system architecture parameters that describe a system architecture.\n35. The method of claim 19, further comprising:\nobtaining system architecture parameters that include data describing processors in a system architecture;\ndetermining, based on the system architecture parameters, a neural network software architecture and a mapping of DNNs in the neural network software architecture to processors of the system architecture, the neural network software architecture including the DNN; and\nexecuting, by the processors of the system architecture mapped to DNNs of the neural network software architecture, the DNNs of the neural network software architecture.\n36. A computer-readable data storage medium having instructions stored thereon that, when executed, cause one or more processors to:\nstore, in a memory, a set of weights of a deep neural network (DNN), the DNN including a plurality of layers, wherein for each layer of the plurality of layers, the set of weights includes weights of the layer and a set of bit precision values includes a bit precision value of the layer, the weights of the layer being represented in memory using values having bit precisions equal to the bit precision value of the layer, the weights of the layer being associated with inputs to neurons of the layer; and\nexecute a machine learning system configured to train the DNN, wherein training the DNN comprises optimizing the set of weights and the set of bit precision values."
}