{
    "patent_link": "https://patents.google.com/patent/US5901246A/en",
    "patent_id": "US5901246A",
    "title": "Ergonomic man-machine interface incorporating adaptive pattern recognition based control system",
    "abstract": "An adaptive interface for a programmable system, for predicting a desired user function, based on user history, as well as machine internal status and context. The apparatus receives an input from the user and other data. A predicted input is presented for confirmation by the user, and the predictive mechanism is updated based on this feedback. Also provided is a pattern recognition system for a multimedia device, wherein a user input is matched to a video stream on a conceptual basis, allowing inexact programming of a multimedia device. The system analyzes a data stream for correspondence with a data pattern for processing and storage. The data stream is subjected to adaptive pattern recognition to extract features of interest to provide a highly compressed representation which may be efficiently processed to determine correspondence. Applications of the interface and system include a VCR, medical device, vehicle control system, audio device, environmental control system, securities trading terminal, and smart house. The system optionally includes an actuator for effecting the environment of operation, allowing closed-loop feedback operation and automated learning.",
    "inventors": [
        "Steven M. Hoffberg",
        "Linda I. Hoffberg-Borghesani"
    ],
    "assignee": "Blanding Hovenweep LLC",
    "classifications": [
        "H04H60/59",
        "G06F9/453",
        "G06V40/103",
        "G11B27/107",
        "G11B27/11",
        "H04N5/782",
        "H04N5/913",
        "G11B27/328",
        "H04B1/205",
        "H04H60/45",
        "H04H60/46",
        "H04N2005/91328",
        "H04N2005/91364"
    ],
    "claims": "\n1. A method for classifying image data comprising the steps of:\nproviding a plurality of object-related models;\ncreating, from the image data, a plurality of accessible mapped ranges corresponding to different subsets of the image data;\nassigning at least one identifier to corresponding ones of the mapped ranges, each of the identifiers specifying for the corresponding mapped range a procedure and a corresponding subset of the image data;\nexecuting, for a plurality of the mapped ranges, a corresponding procedure upon a subset of the image data which corresponds to the mapped ranges;\nselecting at least one of the mapped ranges corresponding to a portion of the image data;\nrepresenting the image data as a set of the identifiers of the selected mapped ranges; and\ndetermining a class relations of the representation of the image data as a set of identfiers of the selected mapped ranges with at least one of said plurality of models based on an image-to-model correspondence.\n2. The method according to claim 1, further comprising the steps of:\ngenerating a plurality of addressable domains from the image data, each of the domains representing a portion of the image information;\nsubjecting a domain to one or more transforms selected from the group consisting of a null transformation, a predetemmed rotation, an inversion, a predetermined scaling, and a predetermined frequency domain preprocessing;\nsaid selecting step corlprising selecting, for each of the transformed domains, at least one of the mapped ranges which most closely corresponds according to predetermined criteria; and\nselecting, based on the determined class relationship a model which most closely corresponds to the set of identifiers representing the image information.\n3. The method according to claim 2 wherein the step of selecting the most closely corresponding one of the mapped ranges comprises the step of selecting, for each transformed domain, the mapped range which is the most similar, by a method selected from at least one of the group consisting of selecting a minimum Hausdorff distance from the domain, selecting the highest cross-correlation with the domain and selecting the lowest mean square error of the difference between the mapped range and the domain.\n4. The method according to claim 3 wherein the step of selecting the most closely corresponding one of mapped ranges includes the step of selecting, for each tranrformed domain, the mapped range wit the minimum modified Hausdorff distance calculated as D{db,mrb}+D{1-db,1-mrb}, where D is a distance calculated between a pair of sets of data each representative of an image, db is a domain, mrb is a mapped range, 1-db is the inverse of a domain, and 1-mrb is an inverse of a mapped range.\n5. The method according to claim 2, wherein the image data comprises a plurality of pixels each having one of a plurality of associated color map values, frter comprising the steps of:\noptionally transforming for each axis of the a the color map, values of the pixels of each doimain by a function including at least one scaling function each of which may be the same or different, and selected based, on a correspondeune between the domains and ranges to which they are to be matched; and\nselecting, for each of the domains, at least one of the mapped ranges having color map pixel values corresponding to the color map pixel values of the domain according to apredetermined criteria, wherein the step of representing the image color map information includes the substep of representing the image color map information as a set of values each including an identifier of the selected mapped range and the color map trasform.\n6. The method according to claim 2, wherein the image data includes a sequence of relatively delayed images representing at least one moving object, further comprising the steps of:\nstoring delayed image data;\ngeneratiug a plurality of addressable further domains from the stored delayed image data, each of the further domains representing a portion of the delayed image information, and corresponding to a domain;\ncreating, from the stored delayed image data, a plurality of addressable mapped ranges corresponding to different subsets of the stored delayed image data;\nmatching the further domain and the domain by subjecting a further domain to one or more corresponding transforms selected from the group consisting of a null transform, a predetermined rotation, an inversion, a predetermined scaling, and a predetermined frequency domain preprocessing, which corresponds to the transforms applied to a corresponding domain, and one or more noncorresponding transforms selected from the group consisting of a predetermined rotation, an inversion, a predetermined scaling, a translation and a predetermined frequency domain preprocessing, which does not correspond to the transforms applied to a corresponding domain;\ncomputing a motion vector between one of the domain and the further domain, or the set of identifiers representing the image data and the set of identifiers representing the delayed image data, and storing the motion vector;\ncompensating the further domain with the motion vector and computing a difference between the compensated Her domain and the domain;\nselecting, for each of the delayed domains, at least one of the mapped ranges corresponding to a portion of the delayed image data;\nrepresenting a difference between the compensated further domain and the domain as a set of difference identifiers of a set of selected mapping ranges and an associated motion vector, and representing the further domain as a set of identifiers of the selected mapping ranges;\ndetermining a complexity of the difference based on a density of representation; and\nwhen the difference has a complexity below a predetermined threshold, selecting, from the plurality of models, a model which most closely corresponds to the set of identifiers of the image data and the set of identifiers of the delayed image data.\n7. The method according to claim 1, wherein said representing step further comprises the steps of determining an object-related feature of interest of the image data, selecting at least one mapped range corresponding to the feature of interest, storing the identifiers of the selected mapped range, selecting a further mapped range corresponding to a portion of image data having a predetermined relationship to the object-related feature of interest and storing the identifiers of the further mapped range.\n8. The method according to claim 1, wherein said image data comprises data representing three associated physical dimensions obtained by a method selected from the group consisting of synthesizing a three dimensional representation based on a machine based prediction derived from two dimensional image data, synthesizing a three dimensional representation derived from a time series of pixel images, and synthesizing a three dimensional representation based on a image data representing a plurality of parallax views having at least two dimensions.\n9. An apparatus for automatically recognizing digital image data consisting of image information, comprising:\nmeans for storing object-related template data;\nmeans for storing the image data;\nmeans for generating a plurality of addressable domains from the stored image data, the domains representing different portions of the image information;\nmeans for creating, from the stored image data, a plurality of addressable mapped ranges corresponding to different subsets of the stored image data, the creating means including means for executing, for each of the mapped ranges, a procedure upon a subset of the stored image data which corresponds to the mapped range;\nmeans for assigning identifiers to corresponding ones of the mapped ranges, each of the identifiers specifying for the corresponding mapped range an address of the corresponding subset of stored image data;\nmeans for selecting, for each of the domnains, at least one of the mapped ranges which most closely corresponds according to predetermined criteria;\nmeans for representing at least a portion of the image information as a set of the identifiers of the selected mapped ranges; and\nmeans for selecting, from the stored templates, a template which most closely corresponds to the set of identifiers representing the portion of the image information.\n10. A method of selectively processing an image having a dimensionality, comprising te steps of:\nproviding information relating to a plurality of exemplars, said information relating to each exemplar having a dimensionality differing from the dimensionality of the image and including infornation representing at least one additional dimension;\ninputting an electronic representation of an image containing a representation of at least one physical object;\npreprocessing the electronic representation of the image to distinguish a representation of at least one object in the image;\nprocessing the distinguished object represented in the image to reduce a storage requirement of the distinguished representation of the object while retaining morphological information describing the distinguished representation of the object;\nfurther processing the processed distinguished representation of the object in conjunction with a plurality of exemplars to produce a comparison; and\nselectively producing an output signal based on said comparison.\n11. The method according to claim 10, wherein said processing step comprises modeling the distinguished representation of the object in at least three independent dimensions.\n12. The method according to claim 10, wherein said plurality of exemplars comprise models each having at least three dimensions, said comparing step comprising projecting said exemplar into two dimensions.\n13. The method according to claim 10, further comprising the steps of:\ninputting a search criteria relating to image morphology;\ncomparing the processed distinguished representation of the object to the search criteria; and\nselectively processing the image based on said output signal and said comparison of said search criteria.\n14. The method according to claim 13, further comprising the steps of:\nreceiving a user input relating to said search criteria;\npredicting a most probable intended search criteria based on said input;\npresenting to the user with a predicted search criteria based on said predicted intended search criteria; and\nreceiving feedback from the user to determine an agreement with said predicted search criteria.\n15. The method according to claim 14, wherein said predicting step is adaptive.\n16. The method according to claim 13, wherein said search criteria comprises an input image pattern.\n17. The method according to claim 13, wherein said search criteria comprises an identifier of an object or an image.\n18. An apparatus for selectively processing a received electronic representation of an image, having dimensionality and containing representation of at least one physical object, comprising:\nmeans for storing information relating to a plurality of exemplars, said information relating to each exemplar having a dimensionality differing from the dimensionality of the image and including information representing at least one additional dimension;\nmeans for distinguishing a representation of at least one object in the electronic representation of the image;\nmeans for reducing a storage requirement of the distinguished representation of the object in the image while retaining morphological information describing the distinguished object; and\nmeans for producing a comparison based on the processed distinguished representation of the object to information relating to a plurality of said exemplars, by processing said information relating to said exemplars in conjunction with said distinguished representation of the object, producing an output signal relating to said comparison.\n19. The apparatus according to claim 18, wherein said means for distinguishing comprises means for determining motion planes from a sequence of representations of the image.\n20. The apparatus according to claim 18, wherein said means for reducing generates a set of identifiers of an iterated function system describing the object morphology.\n21. The apparatus according to claim 18, wherein said received electronic representation of an image comprises a video signal, further comprising means for selectively recording the video signal based on said comparison.",
    "status": "Expired - Lifetime",
    "citations_own": [
        "US5046113A",
        "US5065447A",
        "US5067161A",
        "US5067166A",
        "US5128525A",
        "US5151789A",
        "US5214504A",
        "US5280530A",
        "US5303313A",
        "US5384867A",
        "US5495537A",
        "US5526479A",
        "US5546518A"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US5969753A",
        "US6021403A",
        "US6044464A",
        "US6053737A",
        "US6064976A",
        "US6125209A",
        "WO2000067197A1",
        "WO2000070546A1",
        "US6172674B1",
        "WO2001004790A1",
        "WO2001018739A1",
        "US6216133B1",
        "US6222465B1",
        "US6233667B1",
        "US6247006B1",
        "US6292714B1",
        "US6307971B1",
        "US20010033736A1",
        "US6314401B1",
        "US20020030578A1",
        "US6369838B1",
        "US6388443B1",
        "US20020071277A1",
        "US20020087593A1",
        "US20020103732A1",
        "US6438457B1",
        "US6453201B1",
        "US6456978B1",
        "US20020146178A1",
        "US20020147766A1",
        "US6470235B2",
        "EP1252911A2",
        "US20020158846A1",
        "US6478744B2",
        "US6497674B1",
        "US20030004888A1",
        "US20030013959A1",
        "US20030028486A1",
        "US20030038796A1",
        "US6532460B1",
        "US20030047643A1",
        "US20030063745A1",
        "US20030063798A1",
        "US20030073411A1",
        "US20030073412A1",
        "US20030071117A1",
        "US20030073432A1",
        "US20030072560A1",
        "US20030081937A1",
        "US20030088571A1",
        "US20030085867A1",
        "US20030093300A1",
        "US20030095686A1",
        "US20030095687A1",
        "US20030097532A1",
        "US20030095180A1",
        "WO2003047258A1",
        "US6577978B1",
        "US20030146928A1",
        "US20030152895A1",
        "US20030153846A1",
        "US20030161539A1",
        "US20030167471A1",
        "US20030169925A1",
        "US6640145B2",
        "EP1357373A2",
        "US20030206710A1",
        "US6661437B1",
        "US20030233660A1",
        "US20030233155A1",
        "US6678516B2",
        "US20040010705A1",
        "US6681031B2",
        "US20040015906A1",
        "US20040054572A1",
        "US6712702B2",
        "US6720984B1",
        "US20040073443A1",
        "US6725208B1",
        "US20040087876A1",
        "US6733455B2",
        "US20040095910A1",
        "US6756998B1",
        "US20040138569A1",
        "US6771268B1",
        "US20040153413A1",
        "US6775411B2",
        "US20040158504A1",
        "US20040158503A1",
        "US20040162783A1",
        "US20040161132A1",
        "US20040172342A1",
        "US20040172621A1",
        "US20040172274A1",
        "US20040172275A1",
        "US6792319B1",
        "US6795404B2",
        "US6816085B1",
        "US20040249700A1",
        "US20040249713A1",
        "US20040260600A1",
        "US20040261095A1",
        "US20040259537A1",
        "US20040260574A1",
        "US20040267604A1",
        "US20040267669A1",
        "US6853318B1",
        "US20050051620A1",
        "US20050060010A1",
        "US20050060009A1",
        "US20050060008A1",
        "US20050060007A1",
        "US20050062695A1",
        "US20050073439A1",
        "US6879967B1",
        "EP1522920A2",
        "US20050091685A1",
        "US6889207B2",
        "US20050105769A1",
        "US20050119965A1",
        "US6907289B2",
        "US6909921B1",
        "US6912429B1",
        "US20050143138A1",
        "US20050155053A1",
        "US20050240428A1",
        "US6963855B1",
        "US6970915B1",
        "US20050266907A1",
        "US6977646B1",
        "US20060013443A1",
        "US20060013442A1",
        "US6993131B1",
        "US6993479B1",
        "US7013394B1",
        "US7039698B2",
        "US7043700B1",
        "US20060098006A1",
        "US20060098880A1",
        "US20060107061A1",
        "US20060111816A1",
        "US7055168B1",
        "US20060136110A1",
        "US20060154726A1",
        "US20060158436A1",
        "US20060200258A1",
        "US20060212367A1",
        "US7127422B1",
        "US20060242269A1",
        "US7130824B1",
        "US20060259321A1",
        "US7143039B1",
        "US20060287030A1",
        "US20070061023A1",
        "US7216089B1",
        "US20070127887A1",
        "US7234115B1",
        "US20070225578A1",
        "US20070242883A1",
        "US20070255321A1",
        "US20070255346A1",
        "US20070258645A1",
        "US20070276675A1",
        "US7310589B2",
        "US7333989B1",
        "US7340438B2",
        "US20080071251A1",
        "US20080082426A1",
        "US20080154601A1",
        "US20080184154A1",
        "US7409366B1",
        "US7409356B1",
        "US20080195569A1",
        "US20080195282A1",
        "US20080195281A1",
        "US20080195293A1",
        "US20080201159A1",
        "US20080211782A1",
        "US20080270885A1",
        "US20080270401A1",
        "US7474698B2",
        "US20090028434A1",
        "US20090042695A1",
        "US20090060259A1",
        "US20090116692A1",
        "US20090140909A1",
        "US7571226B1",
        "US20090196510A1",
        "US20090196569A1",
        "US20090232388A1",
        "US20090231327A1",
        "US7594245B2",
        "US7624080B1",
        "US7647340B2",
        "US20100011020A1",
        "US20100013944A1",
        "US7652593B1",
        "US7657836B2",
        "US7657907B2",
        "US7676034B1",
        "US20100070909A1",
        "US20100070529A1",
        "US7685117B2",
        "US20100082175A1",
        "US7693697B2",
        "US20100086220A1",
        "US20100135597A1",
        "US20100135582A1",
        "US20100145703A1",
        "US7749089B1",
        "US20100207936A1",
        "US20100208981A1",
        "US20100209013A1",
        "US7788212B2",
        "US7793205B2",
        "US20100254577A1",
        "US20100298957A1",
        "US20100313141A1",
        "US20110026853A1",
        "US7894595B1",
        "US7904814B2",
        "US7916858B1",
        "US7941481B1",
        "US20110115812A1",
        "US20110194777A1",
        "US20110200249A1",
        "US8020183B2",
        "US8028314B1",
        "US8032477B1",
        "US8051446B1",
        "US8065702B2",
        "US8089458B2",
        "US20120114252A1",
        "US20120120245A1",
        "US8226561B2",
        "US8226493B2",
        "US20120219209A1",
        "USRE43753E1",
        "US8300798B1",
        "US8306624B2",
        "US8356317B2",
        "US8369967B2",
        "US8433622B2",
        "US20130156306A1",
        "US20130236098A1",
        "US8545435B2",
        "US8600830B2",
        "US8606020B2",
        "WO2014025344A2",
        "US8655617B1",
        "US8689253B2",
        "US8696827B2",
        "US8702515B2",
        "US20140114555A1",
        "US8708821B2",
        "US8732030B2",
        "US8753165B2",
        "US8788228B1",
        "US8825462B2",
        "US8892495B2",
        "US8904181B1",
        "US8900338B2",
        "US8949899B2",
        "US20150160003A1",
        "US9132352B1",
        "US9152877B2",
        "US9286441B2",
        "US9286643B2",
        "US9304593B2",
        "US9446319B2",
        "US9530150B2",
        "US20170123586A1",
        "US9690979B2",
        "US9794797B2",
        "US9847038B2",
        "JP2018018141A",
        "US20180039882A1",
        "RU2654126C2",
        "US10003934B1",
        "US10015478B1",
        "US10164776B1",
        "US20180374253A1",
        "US10361802B1",
        "CN110175551A",
        "USRE47908E1",
        "US10614366B1",
        "USRE48056E1",
        "US10718031B1",
        "US10830545B2",
        "US10839302B2",
        "US10869108B1",
        "WO2021163496A1",
        "US11322171B1",
        "US11341962B2",
        "US11598593B2",
        "US11712637B1"
    ],
    "citedby_ftf": []
}