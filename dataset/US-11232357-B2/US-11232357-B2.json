{
    "patent_id": "US-11232357-B2",
    "title": "Method for injecting human knowledge into AI models ",
    "assignee": "UMNAI Limited",
    "publication_date": "2022-01-25",
    "patent_link": "https://patents.google.com/patent/US11232357B2/en",
    "inventors": [
        "Angelo Dalli",
        "Mauro PIRRONE"
    ],
    "classifications": [
        "G06N3/08",
        "G06N5/045",
        "G06N20/00",
        "G06N3/084",
        "G06N10/00",
        "G06N20/20",
        "G06N3/045",
        "G06N3/0454",
        "G06N5/025",
        "G06N7/01"
    ],
    "abstract": "Human knowledge may be injected in an explainable AI system in order to improve the model's generalization error, model accuracy, interpretability of the model, avoid or eliminate bias, while providing a path towards the integration of connectionist systems with symbolic logic in a combined AI system. Human knowledge injection may be implemented by harnessing the white-box nature of explainable/interpretable models. In one exemplary embodiment, a user applies intuition to model-specific cases or exceptions. In another embodiment, an explainable model may be embedded in workflow systems which enable users to apply pre-hoc and post-hoc operations. A third exemplary embodiment implements human-assisted focusing. An exemplary embodiment also presents a method to train and refine explainable or interpretable models without losing the injected knowledge defined by humans when applying gradient descent techniques. The white-box nature of explainable models allows for precise source attribution and traceability of knowledge incorporated into the model.",
    "claims": "\n1. A method for injecting knowledge into an explainable model, comprising:\nidentifying a plurality of partitions, wherein each partition comprises a local model, and wherein the plurality of partitions form a global model, wherein the global model is linear or non-linear;\nforming a set of rules based on one or more local models;\nrepresenting the set of rules in a symbolic logic format;\nforming an explainable model with the set of rules and the one or more local models;\nreceiving, from a user, a set of data corresponding to human knowledge;\nrepresenting the set of data in the symbolic logic format, said representation comprising a localization trigger and an action;\ncreating a condition based on the localization trigger, wherein the explainable model is configured to execute the action upon detection of the condition.\n2. The method of claim 1, wherein a step of representing the set of data in the symbolic logic format is performed without incorporating training data into the set of data and without performing re-training of the explainable model.\n3. The method of claim 1, wherein the explainable model is one or more of an explainable neural network (XNN), an interpretable neural network (INN), an explainable artificial intelligence (XAI), an explainable convolutional neural network (XNN-CNN), an explainable generative adversarial network (XGAN), an explainable transducer transformer (XTT), an explainable spiking net (XSN), an explainable memory net (XMN), an explainable reinforcement learning (XRL) system, an explainable autoencoder (XAE), a dense neural network, a sparse neural network, a micro neural network, an expert system, and an explainable predictive network (PR-XNN).\n4. The method of claim 1, further comprising converting the set of rules into a workflow comprising one or more computational graphs.\n5. The method of claim 1, wherein the human knowledge comprises a privacy law.\n6. The method of claim 1, wherein the set of data corresponding to human knowledge is applied to a specific partition.\n7. The method of claim 1, further comprising identifying a weak partition from the plurality of partitions, and creating at least one further rule based on the weak partition.\n8. The method of claim 1, wherein a further rule is based on a safety feature, and wherein the further rule is unchangeable by the model.\n9. The method of claim 1, wherein the explainable model is embedded in a user-defined workflow.\n10. The method of claim 9, further comprising:\nreceiving an input related to the human knowledge;\ninputting the input to the explainable model and receiving an output from the explainable model;\nreceiving a localized explanation from the explainable model;\nsummarizing the localized explanation and identifying relevant feature attributions;\nperforming a control and quality check;\nvalidating, by a control node, the result of the control and quality check and determining if an exception should be triggered;\ngenerating an analysis of the partitions;\ngenerating a visualization of the summarized explanations and relevant feature attributions;\noutputting the analysis of the partitions, visualization of the summarized explanations, and relevant feature attributions.\n11. The method of claim 10, further comprising:\nverifying, via a user input, the localized explanation and feature attributions.\n12. The method of claim 10, further comprising:\nperforming pre-hoc processing on the input related to the human knowledge prior to inputting the input into the explainable model, said pre-hoc processing step further comprising preventing further processing until input is provided from a human user; and\nperforming post-hoc processing at a step after a step of receiving the localized explanation from the explainable model, said post-hoc processing step further comprising preventing further processing until input is provided from the human user.\n13. The method of claim 1, further comprising defining a further partition based on the condition and fitting a further local model to the further partition.\n14. The method of claim 13, wherein a step of fitting the further local model to the new partition is performed by the explainable model.\n15. The method of claim 1, further comprising\ntraining the explainable model using a gradient descent method, wherein the at least one partition is unchangeable by the gradient descent method.\n16. The method of claim 1, wherein the condition is selected based on input from a plurality of users.\n17. The method of claim 16, wherein the condition is selected based on a vote from the plurality of users, and further comprising adjusting, based on the vote, at least one of: at least one feature attribution, at least one model weight, and at least one model coefficient.\n18. The method of claim 16, further comprising identifying one or more relevant partitions based on the vote, and inserting the condition based on the relevant partition or partitions.\n19. The method of claim 1, further comprising organizing the set of rules into a plurality of groups of rules, wherein rules in a group of rules are organized hierarchically.\n20. The method of claim 19, further comprising aggregating the rules in one of the plurality of groups of rules.\n21. The method of claim 19, further comprising converting one of the plurality of groups of rules into one of an explainable neural network, an explainable artificial intelligence, an expert system, and a workflow.\n22. The method of claim 1, further comprising assigning a priority to each rule in the set of rules.\n23. The method of claim 1, further comprising performing a combined operation on one or more rules, said combined operation comprising performing at least one of: selecting, merging, splitting or aggregating the one or more rules.\n24. The method of claim 1, further comprising converting the explainable model into one of an explainable neural network, an explainable artificial intelligence, an expert system, a workflow, and a system of rules.\n25. The method of claim 24, wherein the explainable model is converted into a workflow, and further comprising presenting the workflow to a human user, receiving user input relating to the rules in the workflow, and updating the workflow based on the user input.\n26. The method of claim 25, wherein the workflow is updated to provide an explanation based on one or more of partition information, weights, boundary conditions, rules, and feature attributions.\n27. The method of claim 25, further comprising receiving at least one of a user goal and a user context and personalizing an explanation and the action based on the user goal and/or user context.\n28. The method of claim 25, further comprising converting the workflow to an explainable neural network, wherein the converted explainable neural network forms an end-to-end computational graph, and wherein the end-to-end computational graph is trainable via gradient descent techniques.\n29. The method of claim 1, wherein the explainable model is further configured to interface with or be integrated with a further system, said further system comprising a Robotic Process Automation system; and\nfurther comprising outputting the action to the Robotic Process Automation system.",
    "status": "Active",
    "citations_own": [
        "US20210133610A1"
    ],
    "citations_ftf": [
        "US11461643B2"
    ],
    "citedby_own": [],
    "citedby_ftf": [
        "US11587161B2",
        "WO2021194516A1",
        "US11270214B1"
    ]
}