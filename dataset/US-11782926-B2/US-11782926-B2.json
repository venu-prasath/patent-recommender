{
    "patent_id": "US-11782926-B2",
    "title": "Automated provisioning for database performance ",
    "assignee": "Oracle International Corporation",
    "publication_date": "2023-10-10",
    "patent_link": "https://patents.google.com/patent/US11782926B2/en",
    "inventors": [
        "Sam Idicula",
        "Tomas Karnagel",
        "Jian Wen",
        "Seema Sundara",
        "Nipun Agarwal",
        "Mayur BENCY"
    ],
    "classifications": [
        "G06F16/24545",
        "G06F16/217",
        "G06N20/00",
        "G06N20/20",
        "G06N3/044",
        "G06N3/045",
        "G06N3/048",
        "G06N3/084",
        "G06N5/01"
    ],
    "abstract": "Embodiments utilize trained query performance machine learning (QP-ML) models to predict an optimal compute node cluster size for a given in-memory workload. The QP-ML models include models that predict query task runtimes at various compute node cardinalities, and models that predict network communication time between nodes of the cluster. Embodiments also utilize an analytical model to predict overlap between predicted task runtimes and predicted network communication times. Based on this data, an optimal cluster size is selected for the workload. Embodiments further utilize trained data capacity machine learning (DC-ML) models to predict a minimum number of compute nodes needed to run a workload. The DC-ML models include models that predict the size of the workload dataset in a target data encoding, models that predict the amount of memory needed to run the queries in the workload, and models that predict the memory needed to accommodate changes to the dataset.",
    "claims": "\n1. A computer-executed method comprising:\naccessing workload information for a particular database workload, wherein the workload information includes at least (a) a portion of a dataset for the particular database workload and (b) a plurality of queries being run in the particular database workload;\npredicting, based on the workload information and using one or more trained data capacity machine learning models, a predicted volatile memory capacity required by the particular database workload;\ndetermining a minimum number of compute nodes, for the particular database workload, having the predicted volatile memory capacity; and\ngenerating, within a memory, output that specifies the minimum number of compute nodes for the particular database workload;\nwherein the method is performed by one or more computing devices.\n2. The method of claim 1, wherein:\nthe portion of the dataset is encoded according to an original data encoding;\nthe one or more trained data capacity machine learning models comprises one or more trained data encoding machine learning models;\nsaid predicting, based on the workload information and using the one or more trained data capacity machine learning models, the predicted volatile memory capacity required by the particular database workload comprises:\npredicting, using the one or more trained data encoding machine learning models, a predicted data size of the dataset of the particular database workload according to a target data encoding,\nwherein the target data encoding is different from the original data encoding, and\ndetermining the predicted volatile memory capacity required by the particular database workload based at least in part on the predicted data size of the dataset of the particular database workload.\n3. The method of claim 1, wherein:\nthe one or more trained data capacity machine learning models comprises one or more trained workload space machine learning models;\nsaid predicting, based on the workload information and using the one or more trained data capacity machine learning models, the predicted volatile memory capacity required by the particular database workload comprises:\npredicting, using the one or more trained workload space machine learning models, a predicted amount of space needed to run the plurality of queries of the particular database workload, and\nwherein said determining the predicted volatile memory capacity required by the particular database workload is based, at least in part, on the predicted amount of space needed to run the one or more queries of the particular database workload.\n4. The method of claim 3, wherein the predicted amount of space needed to run the plurality of queries of the particular database workload accommodates, for each query of the plurality of queries, intermediate and/or final results produced by running said each query.\n5. The method of claim 3, wherein:\nthe predicted amount of space needed to run the plurality of queries of the particular database workload comprises a predicted number of rows;\nthe method further comprises determining a target-encoding-based predicted amount of space needed to run the plurality of queries, comprising:\npredicting, using one or more trained data encoding machine learning models, one or more predicted sizes of one or more types of data of the particular database workload according to a target data encoding,\nwherein the target data encoding is different from an original data encoding, and\ndetermining the target-encoding-based predicted amount of space based at least in part on the one or more predicted sizes of the one or more types of data and the predicted number of rows;\nsaid determining the predicted volatile memory capacity required by the particular database workload based, at least in part, on the target-encoding-based predicted amount of space comprises determining the predicted volatile memory capacity based, at least in part, on the target-encoding-based predicted amount of space.\n6. The method of claim 3, wherein predicting, using the one or more trained workload space machine learning models, a predicted amount of space needed to run the plurality of queries of the particular database workload comprises identifying the predicted amount of space based on a largest predicted amount of space predicted for a query of the plurality of queries.\n7. The method of claim 1, wherein:\nthe one or more trained data capacity machine learning models comprises one or more trained data change rate machine learning models;\npredicting, based on the workload information and using the one or more trained data capacity machine learning models, the predicted volatile memory capacity required by the particular database workload comprises:\npredicting, using the one or more trained data change rate machine learning models, a predicted data change rate for the particular database workload, and\ndetermining the predicted volatile memory capacity required by the particular database workload based at least in part on the predicted data change rate for the particular database workload.\n8. The method of claim 7, wherein said determining the predicted volatile memory capacity required by the particular database workload based at least in part on the predicted data change rate for the particular database workload is performed based, at least in part, on the particular database workload operating in change propagation mode.\n9. The method of claim 7, wherein said predicting, using the one or more trained data change rate machine learning models, the predicted data change rate for the particular database workload is based, at least in part, on one or more features of the particular database workload comprising one or more of:\na rate of changes from historical information maintained in a database log;\na configurable parameter indicating frequency of change offload;\npredicted dataset size in a target data encoding;\nhow many old versions are stored for the particular database workload at a target database management system;\na frequency of garbage collection on a server side; or\ngranularity of changes.\n10. The method of claim 1, further comprising:\nusing a plurality of compute node cardinalities, running the plurality of queries on a sample of the dataset for the particular database workload; and\ngathering a set of run-based workload features by, for each compute node cardinality of the plurality of compute node cardinalities, gathering one or more run-based workload features based on a result of running the plurality of queries over the sample of the dataset using said each compute node cardinality;\nwherein said predicting is based, at least in part, on the one or more run-based workload features.\n11. The method of claim 10, wherein the sample of the dataset is a first sample of the dataset and the set of run-based workload features is a first set of run-based workload features, the method further comprising:\nusing the plurality of compute node cardinalities, running the plurality of queries on a second sample of the dataset for the particular database workload that has a different size than the first sample of the dataset; and\ngathering a second set of run-based workload features by, for each compute node cardinality of the plurality of compute node cardinalities, gathering second one or more run-based workload features based on a result of running the plurality of queries over the second sample of the dataset using said each compute node cardinality;\nwherein a plurality of run-based workload features comprises the first set of run-based workload features and the second set of run-based workload features;\nwherein said predicting is based, at least in part, on one or more gathered run-based workload features of the plurality of run-based workload features.\n12. The method of claim 11, further comprising:\npredicting a plurality of predicted volatile memory capacities for the particular database workload by performing inference, using the one or more trained data capacity machine learning models, based on each set of run-based workload features of the plurality of run-based workload features;\ndetermining whether the plurality of predicted volatile memory capacities converge;\nresponsive to determining that the plurality of predicted volatile memory capacities converge, using at least one of the plurality of predicted volatile memory capacities as the predicted volatile memory capacity required by the particular database workload.\n13. The method of claim 11, further comprising:\npredicting a plurality of predicted volatile memory capacities for the particular database workload by performing inference, using the one or more trained data capacity machine learning models, based on each set of run-based workload features of the plurality of run-based workload features;\ndetermining whether the plurality of predicted volatile memory capacities converge;\nwherein the second sample of the dataset is larger than the first sample of the dataset;\nresponsive to determining that the plurality of predicted volatile memory capacities do not converge:\nusing the plurality of compute node cardinalities, running the plurality of queries on a third sample of the dataset for the particular database workload that has a larger size than the second sample of the dataset; and\ngathering a third set of run-based workload features by, for each compute node cardinality of the plurality of compute node cardinalities, gathering third one or more run-based workload features based on a result of running the plurality of queries over the third sample of the dataset using said each compute node cardinality;\nwherein a second plurality of run-based workload features comprises the third set of run-based workload features and the second set of run-based workload features;\nwherein said predicting is based, at least in part, on one or more gathered run-based workload features of the second plurality of run-based workload features.\n14. The method of claim 1, further comprising:\ngenerating a set of workload training data, comprising, for each database workload of one or more database workloads:\nperforming a set of experiments using each of a plurality of compute node cardinalities,\nwherein each experiment of the set of experiments comprising running a set of queries of said each workload over a particular portion of a dataset of said each workload,\nwherein each experiment, of the set of experiments, comprises a different size portion of the dataset of said each workload,\nwherein at least one experiment, of the set of experiments, comprises the entire dataset of said each workload, and\nrecording, in the set of workload training data, results of the set of experiments; and\ntraining one or more data capacity machine learning models, using the set of workload training data, to produce the one or more trained data capacity machine learning models.\n15. One or more non-transitory computer readable media storing one or more sequences of instructions that, when executed by one or more processors, cause:\naccessing workload information for a particular database workload, wherein the workload information includes at least (a) a portion of a dataset for the particular database workload and (b) plurality of queries being run in the particular database workload;\npredicting, based on the workload information and using one or more trained data capacity machine learning models, a predicted volatile memory capacity required by the particular database workload;\ndetermining a minimum number of compute nodes, for the particular database workload, having the predicted volatile memory capacity; and\ngenerating, within a memory, output that specifies the minimum number of compute nodes for the particular database workload.\n16. The one or more non-transitory computer readable media of claim 15, wherein:\nthe portion of the dataset is encoded according to an original data encoding;\nthe one or more trained data capacity machine learning models comprises one or more trained data encoding machine learning models;\nsaid predicting, based on the workload information and using the one or more trained data capacity machine learning models, the predicted volatile memory capacity required by the particular database workload comprises:\npredicting, using the one or more trained data encoding machine learning models, a predicted data size of the dataset of the particular database workload according to a target data encoding,\nwherein the target data encoding is different from the original data encoding, and\ndetermining the predicted volatile memory capacity required by the particular database workload based at least in part on the predicted data size of the dataset of the particular database workload.\n17. The one or more non-transitory computer readable media of claim 15, wherein:\nthe one or more trained data capacity machine learning models comprises one or more trained workload space machine learning models;\nsaid predicting, based on the workload information and using the one or more trained data capacity machine learning models, the predicted volatile memory capacity required by the particular database workload comprises:\npredicting, using the one or more trained workload space machine learning models, a predicted amount of space needed to run the plurality of queries of the particular database workload, and\nwherein said determining the predicted volatile memory capacity required by the particular database workload is based, at least in part, on the predicted amount of space needed to run the plurality of queries of the particular database workload.\n18. The one or more non-transitory computer readable media of claim 17, wherein the predicted amount of space needed to run the plurality of queries of the particular database workload accommodates, for each query of the plurality of queries, intermediate and/or final results produced by running said each query.\n19. The one or more non-transitory computer readable media of claim 17, wherein:\nthe predicted amount of space needed to run the plurality of queries of the particular database workload comprises a predicted number of rows; and\nthe one or more sequences of instructions further comprise instructions that, when executed by one or more processors, cause determining a target-encoding-based predicted amount of space needed to run the plurality of queries, comprising:\npredicting, using one or more trained data encoding machine learning models, one or more predicted sizes of one or more types of data of the particular database workload according to a target data encoding,\nwherein the target data encoding is different from an original data encoding, and\ndetermining the target-encoding-based predicted amount of space based at least in part on the one or more predicted sizes of the one or more types of data and the predicted number of rows;\nsaid determining the predicted volatile memory capacity required by the particular database workload based, at least in part, on the target-encoding-based predicted amount of space comprises determining the predicted volatile memory capacity based, at least in part, on the target-encoding-based predicted amount of space.\n20. The one or more non-transitory computer readable media of claim 17, wherein predicting, using the one or more trained workload space machine learning models, a predicted amount of space needed to run the one or more queries of the particular database workload comprises identifying the predicted amount of space based on a largest predicted amount of space predicted for a query of the plurality of queries.\n21. The one or more non-transitory computer readable media of claim 15, wherein:\nthe one or more trained data capacity machine learning models comprises one or more trained data change rate machine learning models;\npredicting, based on the workload information and using the one or more trained data capacity machine learning models, the predicted volatile memory capacity required by the particular database workload comprises:\npredicting, using the one or more trained data change rate machine learning models, a predicted data change rate for the particular database workload, and\ndetermining the predicted volatile memory capacity required by the particular database workload based at least in part on the predicted data change rate for the particular database workload.\n22. The one or more non-transitory computer readable media of claim 21, wherein said determining the predicted volatile memory capacity required by the particular database workload based at least in part on the predicted data change rate for the particular database workload is performed based, at least in part, on the particular database workload operating in change propagation mode.\n23. The one or more non-transitory computer readable media of claim 21, wherein said predicting, using the one or more trained data change rate machine learning models, the predicted data change rate for the particular database workload is based, at least in part, on one or more features of the particular database workload comprising one or more of:\na rate of changes from historical information maintained in a database log;\na configurable parameter indicating frequency of change offload;\npredicted dataset size in a target data encoding;\nhow many old versions are stored for the particular database workload at a target database management system;\na frequency of garbage collection on a server side; or\ngranularity of changes.\n24. The one or more non-transitory computer readable media of claim 15, wherein the one or more sequences of instructions further comprise instructions that, when executed by one or more processors, cause:\nusing a plurality of compute node cardinalities, running the plurality of queries on a sample of the dataset for the particular database workload; and\ngathering a set of run-based workload features by, for each compute node cardinality of the plurality of compute node cardinalities, gathering one or more run-based workload features based on a result of running the plurality of queries over the sample of the dataset using said each compute node cardinality;\nwherein said predicting is based, at least in part, on the one or more run-based workload features.\n25. The one or more non-transitory computer readable media of claim 24, wherein:\nthe sample of the dataset is a first sample of the dataset;\nthe set of run-based workload features is a first set of run-based workload features; and\nthe one or more sequences of instructions further comprise instructions that, when executed by one or more processors, cause:\nusing the plurality of compute node cardinalities, running the plurality of queries on a second sample of the dataset for the particular database workload that has a different size than the first sample of the dataset; and\ngathering a second set of run-based workload features by, for each compute node cardinality of the plurality of compute node cardinalities, gathering second one or more run-based workload features based on a result of running the plurality of queries over the second sample of the dataset using said each compute node cardinality;\nwherein a plurality of run-based workload features comprises the first set of run-based workload features and the second set of run-based workload features;\nwherein said predicting is based, at least in part, on one or more gathered run-based workload features of the plurality of run-based workload features.\n26. The one or more non-transitory computer readable media of claim 25, wherein the one or more sequences of instructions further comprise instructions that, when executed by one or more processors, cause:\npredicting a plurality of predicted volatile memory capacities for the particular database workload by performing inference, using the one or more trained data capacity machine learning models, based on each set of run-based workload features of the plurality of run-based workload features;\ndetermining whether the plurality of predicted volatile memory capacities converge;\nresponsive to determining that the plurality of predicted volatile memory capacities converge, using at least one of the plurality of predicted volatile memory capacities as the predicted volatile memory capacity required by the particular database workload.\n27. The one or more non-transitory computer readable media of claim 25, wherein the one or more sequences of instructions further comprise instructions that, when executed by one or more processors, cause:\npredicting a plurality of predicted volatile memory capacities for the particular database workload by performing inference, using the one or more trained data capacity machine learning models, based on each set of run-based workload features of the plurality of run-based workload features;\ndetermining whether the plurality of predicted volatile memory capacities converge;\nwherein the second sample of the dataset is larger than the first sample of the dataset;\nresponsive to determining that the plurality of predicted volatile memory capacities do not converge:\nusing the plurality of compute node cardinalities, running the plurality of queries on a third sample of the dataset for the particular database workload that has a larger size than the second sample of the dataset; and\ngathering a third set of run-based workload features by, for each compute node cardinality of the plurality of compute node cardinalities, gathering third one or more run-based workload features based on a result of running the plurality of queries over the third sample of the dataset using said each compute node cardinality;\nwherein a second plurality of run-based workload features comprises the third set of run-based workload features and the second set of run-based workload features;\nwherein said predicting is based, at least in part, on one or more gathered run-based workload features of the second plurality of run-based workload features.\n28. The one or more non-transitory computer readable media of claim 15, wherein the one or more sequences of instructions further comprise instructions that, when executed by one or more processors, cause:\ngenerating a set of workload training data, comprising, for each database workload of one or more database workloads:\nperforming a set of experiments using each of a plurality of compute node cardinalities,\nwherein each experiment of the set of experiments comprising running a set of queries of said each workload over a particular portion of a dataset of said each workload,\nwherein each experiment, of the set of experiments, comprises a different size portion of the dataset of said each workload,\nwherein at least one experiment, of the set of experiments, comprises the entire dataset of said each workload, and\nrecording, in the set of workload training data, results of the set of experiments; and\ntraining one or more data capacity machine learning models, using the set of workload training data, to produce the one or more trained data capacity machine learning models.",
    "status": "Active",
    "citations_own": [
        "US6789069B1",
        "US6842751B1",
        "US20080097938A1",
        "WO2008133509A1",
        "US20100153956A1",
        "US20130231552A1",
        "US20140114952A1",
        "US20140188768A1",
        "US20140344193A1",
        "US20160004621A1",
        "US20160321561A1",
        "US20160328273A1",
        "EP3101599A2",
        "US20170017686A1",
        "US20170068675A1",
        "US20170286252A1",
        "US20170344394A1",
        "US20180060330A1",
        "US20180060738A1",
        "US20180107711A1",
        "US10079507B2",
        "US20180314735A1",
        "US10140161B1",
        "US20180349986A1",
        "US20180357541A1",
        "US20180357511A1",
        "US20180360390A1",
        "US20180365065A1",
        "US20190042867A1",
        "US20190095785A1",
        "US20190095818A1",
        "US20190095756A1",
        "US20190095764A1",
        "US20190303475A1",
        "US20190340095A1",
        "US20190347511A1",
        "US10489215B1",
        "US20200034467A1",
        "US20200034197A1",
        "US10554738B1",
        "US20200074306A1",
        "US20200073986A1",
        "US10606649B2",
        "US20200104397A1",
        "US20200118036A1",
        "US20200125568A1",
        "US10936589B1",
        "US20220043681A1"
    ],
    "citations_ftf": [
        "IL191744A0",
        "US11615265B2"
    ],
    "citedby_own": [],
    "citedby_ftf": [
        "US9471631B2",
        "US11797641B2",
        "CA2881033C",
        "US11176487B2",
        "US11544494B2",
        "US11074256B2",
        "US11392621B1",
        "US11436527B2",
        "WO2020033446A1",
        "US11544630B2",
        "US11061902B2",
        "US20200193332A1",
        "US10824625B1",
        "US11157471B2",
        "US11366697B2",
        "JP7279507B2",
        "US20210004675A1",
        "US11003501B2",
        "US11635995B2",
        "US11531671B2",
        "US11514044B2",
        "US11748206B2",
        "US11182724B2",
        "US11762751B2",
        "US11481627B2",
        "US11727284B2",
        "US20210216351A1",
        "US11748350B2",
        "US11762670B2",
        "CN111581454B",
        "US11210275B2",
        "US11797340B2",
        "US11620547B2",
        "US11392304B2",
        "US20210406717A1",
        "CN113872788A",
        "US20220043690A1",
        "US20220100763A1",
        "CA3188066A1",
        "JP2022077165A",
        "US11775264B2",
        "CN112650770B",
        "US11537594B2",
        "US20220254505A1",
        "KR20230011857A",
        "KR102384821B1",
        "US20230021502A1",
        "US11520834B1",
        "CN113673866A",
        "US11748352B2",
        "US20230132501A1",
        "CN114124554B",
        "US20230168950A1",
        "US20230244643A1",
        "US11775516B2",
        "US20230297573A1",
        "CN116073836B"
    ]
}