{
    "patent_id": "US-11614610-B2",
    "title": "Systems, devices, and methods for image processing to generate an image having predictive tagging ",
    "assignee": "Allen Institute",
    "publication_date": "2023-03-28",
    "patent_link": "https://patents.google.com/patent/US11614610B2/en",
    "inventors": [
        "Gregory Johnson",
        "Chawin OUNKOMOL",
        "Forrest COLLMAN",
        "Sharmishtaa SESHAMANI"
    ],
    "classifications": [
        "G06T7/11",
        "A61B5/0275",
        "G01N15/1429",
        "G01N15/1475",
        "G02B21/008",
        "G06F18/2413",
        "G06N20/20",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/08",
        "G06T7/174",
        "G06T7/187",
        "G06V10/25",
        "G06V10/50",
        "G06V10/764",
        "G06V20/695",
        "G06V20/698",
        "G01N2015/1006",
        "G01N2015/1488",
        "G06T2207/10016",
        "G06T2207/10056",
        "G06T2207/10061",
        "G06T2207/10064",
        "G06T2207/20081",
        "G06T2207/20084",
        "G06T2207/30024"
    ],
    "abstract": "A computing device, method, system, and instructions in a non-transitory computer-readable medium for performing image analysis on 3D microscopy images to predict localization and/or labeling of various structures or objects of interest, by predicting the location in such images at which a dye or other marker associated with such structures would appear. The computing device, method, and system receives sets of 3D images that include unlabeled images, such as transmitted light images or electron microscope images, and labeled images, such as images captured with fluorescence tagging. The computing device trains a statistical model to associate structures in the labeled images with the same structures in the unlabeled light images. The processor further applies the statistical model to a new unlabeled image to generate a predictive labeled image that predicts the location of a structure of interest in the new image.",
    "claims": "\n1. A computing device, comprising:\na communication interface configured to receive microscopy images;\na processor; and\na non-transitory computer-readable medium communicatively coupled to the communication interface and to the processor, and storing computer-executable instructions that, when executed by the processor, causes the processor to:\nreceive, via the communication interface, a first set of microscopy images and a second set of microscopy images, wherein the first set of microscopy images are fluorescence images of a plurality of tissue samples each having one or more sub-cellular structures or one or more cells, and wherein the second set of microscopy images are electron micrograph (EM) images of the one or more sub-cellular structures or one or more cells of the plurality of tissue samples, wherein no fluorescence labeling is included in the second set of microscopy images;\ndetermine that each of the first set of microscopy images is aligned with one of the second set of microscopy images;\ngenerate, after determining that each of the first set of microscopy images is aligned with one of the second set of microscopy images, a neural network configured to convert a first type of image that is an EM image of any sub-cellular structure or cell to a second type of image that is a predicted fluorescence image of the sub-cellular structure or cell, wherein no fluorescence labeling is included in the first type of image, and wherein the instructions cause the processor to generate the neural network by training the neural network based on the first set of 3D microscopy images and the second set of 3D microscopy images;\nreceive, after the neural network is generated, a pair of microscopy images that include a third microscopy image and a fourth microscopy image, wherein the third microscopy image is a fluorescence image of one or more cellular structures or one or more cells of an additional tissue sample, and the fourth microscopy image is an EM image of the one or more sub-cellular structures or one or more cells of the additional tissue sample, wherein the third microscopy image and the fourth microscopy image are not aligned with each other;\ngenerate, with the neural network and the EM image of the fourth microscopy image, a predicted fluorescence image that includes predicted fluorescence labeling for the additional tissue sample;\ndetermine registration information that indicates how the predicted fluorescence image can be aligned with the fluorescence image of the third microscopy image; and\nperform registration of the third microscopy image and the fourth microscopy image using the determined registration information.\n2. The computing device of claim 1, wherein the instructions cause the processor to perform the registration by performing at least one of shifting, rotating, or scaling of the third microscopy image relative to the fourth microscopy image based on the registration information.\n3. The computing device of claim 2, wherein the instructions cause the processor to overlay the third microscopy image on the fourth microscopy image after the third microscopy image has been shifted, rotated, or scaled based on the registration information.\n4. The computing device of claim 1, wherein the instructions cause the processor to determine the registration information by using an intensity-based registration process that performs intensity matching between the predicted fluorescence image and the third microscopy image.\n5. The computing device of claim 1, wherein each of the third microscopy image and the fourth microscopy image includes a plurality of pixels, and wherein, before registration is performed, each pixel of the third microscopy image represents a bigger region of the additional tissue sample than does each pixel of the fourth microscopy image, such that the fluorescence image of the third microscopy image is at a lower level of magnification relative to the EM image of the fourth microscopy image.\n6. The computing device of claim 5, wherein, before registration is performed, each pixel of the third microscopy image represents a region of the additional tissue sample that is at least 100 times larger than a region of the additional tissue sample represented by each pixel of the fourth microscopy image.\n7. The computing device of claim 5, wherein the EM image of the fourth microscopy image was captured by an electron microscope at a first level of magnification of a first region of the additional tissue sample, wherein the instructions further cause the processor to control the electron microscope to acquire a fifth microscopy image of a second region that is a portion of the first region, wherein a location of the second region within the first region is indicated by the registration information, and wherein the fifth microscopy image is an EM image that is at a second level of magnification higher than the first level.\n8. The computing device of claim 7, wherein the registration information is a first set of registration information, and wherein performing registration of the third microscopy image with the fourth microscopy image results in a first amount of alignment error between the third microscopy image and the fourth microscopy image, wherein the instructions further cause the processor to:\ngenerate, with the neural network and the fifth microscopy image, an additional predicted fluorescence image;\ndetermine a second set of registration information that indicates how the additional predicted fluorescence image can be aligned with the fluorescence image of the third microscopy image; and\nperform registration of the third microscopy image and the fifth microscopy image using the second set of registration information, wherein performing the registration of the third microscopy image with the fifth microscopy image results in a smaller amount of alignment error, relative to the first amount of alignment error, between the third microscopy image and the fifth microscopy image.\n9. The computing device of claim 8, wherein the second level of magnification is at least ten times the first level of magnification.\n10. A non-transitory computer-readable medium having computer-executable instructions that, when executed by a processor, causes the processor to:\nreceive, via the a communication interface, a first set of microscopy images and a second set of microscopy images, wherein the first set of microscopy images are fluorescence images of a plurality of tissue samples each having one or more sub-cellular structures or one or more cells, and wherein the second set of microscopy images are electron micrograph (EM) images of the one or more sub-cellular structures or one or more cells of the plurality of tissue samples, wherein no fluorescence labeling is included in the second set of microscopy images;\ndetermine that each of the first set of microscopy images is aligned with one of the second set of microscopy images;\ngenerate, after determining that each of the first set of microscopy images is aligned with one of the second set of microscopy images, a neural network configured to convert a first type of image that is an EM image of any sub-cellular structure or cell to a second type of image that is a predicted fluorescence image of the sub-cellular structure or cell, wherein no fluorescence labeling is included in the first type of image, and wherein the instructions cause the processor to generate the neural network by training the neural network based on the first set of 3D microscopy images and the second set of 3D microscopy images;\nreceive, after the neural network is generated, a pair of microscopy images that include a third microscopy image and a fourth microscopy image, wherein the third microscopy image is a fluorescence image of one or more cellular structures or one or more cells of an additional tissue sample, and the fourth microscopy image is an EM image of the one or more sub-cellular structures or one or more cells of the additional tissue sample, wherein the third microscopy image and the fourth microscopy image are not aligned with each other;\ngenerate, with the neural network and the EM image of the fourth microscopy image, a predicted fluorescence image that includes predicted fluorescence labeling for the additional tissue sample;\ndetermine registration information that indicates how the predicted fluorescence image can be aligned with the fluorescence image of the third microscopy image; and\nperform registration of the third microscopy image and the fourth microscopy image using the determined registration information.\n11. The non-transitory computer-readable medium of claim 10, wherein the instructions, when executed by the processor, cause the processor to perform the registration by performing at least one of shifting, rotating, or scaling of the third microscopy image relative to the fourth microscopy image based on the registration information.\n12. The non-transitory computer-readable medium of claim 11, wherein the instructions, when executed by the processor, cause the processor to overlay the third microscopy image on the fourth microscopy image after the third microscopy image has been shifted, rotated, or scaled based on the registration information.\n13. The non-transitory computer-readable medium of claim 10, wherein the instructions, when executed by the processor, cause the processor to determine the registration information by using an intensity-based registration process that performs intensity matching between the predicted fluorescence image and the third microscopy image.\n14. A computing device, comprising:\na communication interface configured to receive microscopy images;\na processor; and\na non-transitory computer-readable medium communicatively coupled to the processor and storing computer-executable instructions that, when executed by the processor, causes the processor to:\nreceive, via the communication interface, a first set of three-dimensional (3D) microscopy images and a second set of 3D microscopy images, wherein the first set of 3D microscopy images are 3D confocal laser scanning microscopy (CLSM) fluorescence images of a plurality of tissue samples each having a plurality of cells, and wherein the second set of 3D microscopy images are 3D transmitted light images of the same plurality of tissue samples, wherein fluorescence labeling is applied to the plurality of cells in the first set of 3D microscopy images, and wherein no fluorescence labeling is included in the second set of 3D microscopy images;\ngenerate a neural network configured to convert a first type of image that is a 3D transmitted light image of cells to a second type of image that is a predicted 3D CLSM fluorescence image of the cells, wherein no fluorescence labeling is included in the first type of image, and wherein the instructions cause the processor to generate the neural network by training the neural network based on the first set of 3D microscopy images and the second set of 3D microscopy images;\nreceive, after the neural network is generated and trained, an additional 3D microscopy image that is a transmitted light image of an additional tissue sample having a plurality of cells, wherein no fluorescence labeling is included in the additional 3D microscopy image; and\ngenerate, with the neural network and the additional 3D microscopy image, a predicted 3D CLSM fluorescence image that includes predicted fluorescence labeling of the plurality of cells for the additional tissue sample.\n15. The computing device of claim 14, wherein the instructions further cause the processor to determine, using the predicted 3D CLSM fluorescence image, a cell characteristic of the plurality of cells of the additional tissue sample, wherein the cell characteristic is at least one of an average or median cell size, a cell count, cell morphology of at least one of the plurality of cells, a cell cycle phase of at least one of the plurality of cells, or the presence or absence a protein biomarker on a surface of at least one of the plurality of cells.\n16. The computing device of claim 15, wherein the neural network is a first neural network, wherein the instructions further cause the processor to:\nreceive an indication of which cell in the plurality of tissue samples have a classification of being a diseased cell,\ngenerate a second neural network configured to convert the second type of image that is the predicted 3D CLSM fluorescence image to a predicted classification of whether the predicted fluorescence 3D CLSM image includes a diseased cell, wherein the instructions cause the processor to generate the second neural network by training the second neural network with predicted 3D CLSM fluorescence images generated by the first neural network and with the received indication of which cell in the plurality of tissue samples is a diseased cell; and\ngenerate, with the second neural network and the predicted 3D CLSM fluorescence image of the additional tissue sample, a predicted classification of whether the additional tissue samples include a diseased cell."
}