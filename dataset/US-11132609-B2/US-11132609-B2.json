{
    "patent_id": "US-11132609-B2",
    "title": "Multi-task neural network systems with task-specific policies and a shared policy ",
    "assignee": "Deepmind Technologies Limited",
    "publication_date": "2021-09-28",
    "patent_link": "https://patents.google.com/patent/US11132609B2/en",
    "inventors": [
        "Razvan Pascanu",
        "Raia Thais Hadsell",
        "Victor Constant Bapst",
        "Wojciech Czarnecki",
        "James Kirkpatrick",
        "Yee Whye Teh",
        "Nicolas Manfred Otto Heess"
    ],
    "classifications": [
        "G06N3/006",
        "G06N3/045",
        "G06N3/084",
        "G06N3/10",
        "G06N5/043"
    ],
    "abstract": "A method is proposed for training a multitask computer system, such as a multitask neural network system. The system comprises a set of trainable workers and a shared module. The trainable workers and shared module are trained on a plurality of different tasks, such that each worker learns to perform a corresponding one of the tasks according to a respective task policy, and said shared policy network learns a multitask policy which represents common behavior for the tasks. The coordinated training is performed by optimizing an objective function comprising, for each task: a reward term indicative of an expected reward earned by a worker in performing the corresponding task according to the task policy; and at least one entropy term which regularizes the distribution of the task policy towards the distribution of the multitask policy.",
    "claims": "\n1. A method of training a multitask neural network system to perform a plurality of tasks, the neural network system comprising (i) a plurality of task policies each corresponding to a different one of the plurality of tasks and (ii) a shared policy that is shared between the plurality of tasks,\nwherein each task policy has a respective plurality of task-specific parameters and maps an input observation to a task-specific probability distribution over actions in a set of actions,\nwherein the shared policy has a plurality of shared policy parameters and maps the input observation to a shared probability distribution over the actions in the set of actions, and\nwherein the method comprises:\ntraining each of the plurality of task policies on data for the task corresponding to the task policy to update the respective plurality of task-specific parameters for each of the task policies, wherein, for each task, the data for the task includes a sequence of actions each performed by a worker while attempting to perform the task, wherein the sequence of actions is generated by selecting actions to be performed by the worker using a set of input observations; and\nduring the training of each of the plurality of task policies on data for the task corresponding to the task policy:\ntraining the shared policy using the plurality of task polices to update the plurality of shared policy parameters to learn a multitask policy representing common behavior for the tasks; and\ntraining each of the task policies (i) to generate a set of task-specific probability distributions that maximize expected rewards when performing the corresponding task using the set of task-specific probability distributions and (ii) using the shared policy to regularize the training of the task policies by encouraging each of the task policies to generate task-specific probability distributions that are similar to the shared probability distribution generated by processing the same input observation.\n2. A method as claimed in claim 1 wherein the training is performed by optimizing an objective function, the objective function comprising, for each task:\na reward term indicative of an expected reward earned by a worker in performing the task according to the task policy corresponding to the task; and\nat least a first entropy term measuring a difference between a distribution of the task policy corresponding to the task and a distribution of the shared policy to regularize the distribution of the task policy towards the distribution of the shared policy.\n3. A method as claimed in claim 2 in which the objective function further comprises, for each task, a second, exploration entropy term dependent upon the distribution of the task policy corresponding to the task to encourage exploration.\n4. A method as claimed in claim 2, wherein the tasks each comprise performing sequence of actions each to be performed in response to a corresponding observation.\n5. A method as claimed in claim 4 wherein said reward term is a discounted reward term dependent upon an expected reward from taking an action in response to an observation.\n6. A method as claimed in claim 1 wherein the training comprises alternating between training one or more of the task policies and training the shared policy.\n7. A method as claimed in claim 1 wherein the training comprises jointly optimizing the shared policy and one or more of the task policies.\n8. A method as claimed in claim 1 wherein each of the task policies is parameterized using the multitask policy.\n9. A method as claimed in claim 1 wherein the task policies and the shared policy comprise one or more neural networks and the training comprises training the one or more neural networks jointly by back propagation.\n10. A method as claimed in claim 9 wherein said back propagation uses an objective function which includes a term for matching probabilities from the task policies to probabilities from the multitask policy to distil the multitask policy from the task policies.\n11. A system comprising one or more computers and one or more storage devices storing instructions that when executed by one or more computers cause the one or more computers to perform operations for training a multitask neural network system to perform a plurality of tasks, the neural network system comprising (i) a plurality of task policies each corresponding to a different one of the plurality of tasks and (ii) a shared policy that is shared between the plurality of tasks,\nwherein each task policy has a respective plurality of task-specific parameters and maps an input observation to a task-specific probability distribution over actions in a set of actions,\nwherein the shared policy has a plurality of shared policy parameters and maps the input observation to a shared probability distribution over the actions in the set of actions, and wherein the operations comprise:\ntraining each of the plurality of task policies on data for the task corresponding to the task policy to update the respective plurality of task-specific parameters for each of the task policies, wherein, for each task, the data for the task includes a sequence of actions each performed by a worker while attempting to perform the task, wherein the sequence of actions is generated by selecting actions to be performed by the worker using a set of input observations; and\nduring the training of each of the plurality of task policies on data for the task corresponding to the task policy:\ntraining the shared policy using the plurality of task polices to update the plurality of shared policy parameters to learn a multitask policy representing common behavior for the tasks; and\ntraining each of the task policies (i) to generate a set of task-specific probability distributions that maximize expected rewards when performing the corresponding task using the set of task-specific probability distributions and (ii) using the shared policy to regularize the training of the task policies by encouraging each of the task policies to generate task-specific probability distributions that are similar to the shared probability distribution generated by processing the same input observation.\n12. A system as claimed in claim 11 wherein the training is performed by optimizing an objective function, the objective function comprising, for each task:\na reward term indicative of an expected reward earned by a worker in performing the task according to the task policy corresponding to the task; and\nat least a first entropy term measuring a difference between a distribution of the task policy corresponding to the task and a distribution of the shared policy to regularize the distribution of the task policy towards the distribution of the shared policy.\n13. A system as claimed in claim 12 in which the objective function further comprises, for each task, a second, exploration entropy term dependent upon the distribution of the task policy corresponding to the task to encourage exploration.\n14. A system as claimed in claim 12, wherein the tasks each comprise performing sequence of actions each to be performed in response to a corresponding observation.\n15. A system as claimed in claim 14 wherein said reward term is a discounted reward term dependent upon an expected reward from taking an action in a response to an observation.\n16. A system as claimed in claim 11 wherein the training comprises alternating between training one or more of the task policies and training the shared policy.\n17. A system as claimed in claim 11 wherein the training comprises jointly optimizing the shared policy and one or more of the task policies.\n18. A system as claimed in claim 11 wherein each of the task policies is parameterized using the multitask policy.\n19. A system as claimed in claim 11 wherein the task policies and the shared policy comprise one or more neural networks and the training comprises training the one or more neural networks jointly by back propagation.\n20. A system as claimed in claim 19 wherein said back propagation uses an objective function which includes a term for matching probabilities from the task policies to probabilities from the multitask policy to distil the multitask policy from the task policies.\n21. One or more non-transitory computer-readable storage media storing instructions that when executed by one or more computers cause the one or more computers to perform operations for training a multitask neural network system to perform a plurality of tasks, the neural network system comprising (i) a plurality of task policies each corresponding to a different one of the plurality of tasks and (ii) a shared policy that is shared between the plurality of tasks,\nwherein each task policy has a respective plurality of task-specific parameters and maps an input observation to a task-specific probability distribution over actions in a set of actions,\nwherein the shared policy has a plurality of shared policy parameters and maps the input observation to a shared probability distribution over the actions in the set of actions, and wherein the operations comprise:\ntraining each of the plurality of task policies on data for the task corresponding to the task policy to update the respective plurality of task-specific parameters for each of the task policies, wherein, for each task, the data for the task includes a sequence of actions each performed by a worker while attempting to perform the task, wherein the sequence of actions is generated by selecting actions to be performed by the worker using a set of input observations; and\nduring the training of each of the plurality of task policies on data for the task corresponding to the task policy:\ntraining the shared policy using the plurality of task polices to update the plurality of shared policy parameters to learn a multitask policy representing common behavior for the tasks; and\ntraining each of the task policies (i) to generate a set of task-specific probability distributions that maximize expected rewards when performing the corresponding task using the set of task-specific probability distributions and (ii) using the shared policy to regularize the training of the task policies by encouraging each of the task policies to generate task-specific probability distributions that are similar to the shared probability distribution generated by processing the same input observation.\n22. One or more non-transitory computer-readable storage media as claimed in claim 21 wherein the training is performed by optimizing an objective function, the objective function comprising, for each task:\na reward term indicative of an expected reward earned by a worker in performing the task according to the task policy corresponding to the task; and\nat least a first entropy term measuring a difference between a distribution of the task policy corresponding to the task and a distribution of the shared policy to regularize the distribution of the task policy towards the distribution of the shared policy.\n23. One or more non-transitory computer-readable storage media as claimed in claim 22 in which the objective function further comprises, for each task, a second, exploration entropy term dependent upon the distribution of the task policy corresponding to the task to encourage exploration.\n24. One or more non-transitory computer-readable storage media as claimed in claim 22, wherein the tasks each comprise performing sequence of actions each to be performed in response to a corresponding observation.\n25. One or more non-transitory computer-readable storage media as claimed in claim 24 wherein said reward term is a discounted reward term dependent upon an expected reward from taking an action in response to an observation.\n26. One or more non-transitory computer-readable storage media as claimed in claim 21 wherein the task policies and the shared policy comprise one or more neural networks and the training comprises training the one or more neural networks jointly by back propagation.\n27. One or more non-transitory computer-readable storage media as claimed in claim 26 wherein said back propagation uses an objective function which includes a term for matching probabilities from the task policies to probabilities from the multitask policy to distil the multitask policy from the task policies.",
    "status": "Active",
    "citations_own": [
        "US20180165602A1"
    ],
    "citations_ftf": [
        "JP4803212B2",
        "JP5733166B2",
        "EP3602412A1"
    ],
    "citedby_own": [
        "US20220083869A1"
    ],
    "citedby_ftf": [
        "US11250327B2",
        "US11507844B2",
        "US10573295B2",
        "US11562287B2",
        "US11003994B2",
        "WO2019118299A1",
        "US11527308B2",
        "US11735028B2",
        "US11481639B2",
        "CA3129731A1",
        "WO2020198520A1",
        "US20200327379A1",
        "US11699062B2",
        "US11403668B2",
        "CN111309893A",
        "US11775841B2",
        "CN111708355B",
        "CN111680934B",
        "CN112001585B",
        "WO2022025568A1",
        "CN114528469A",
        "CN112527383A",
        "CN112784958B",
        "CN112685318A",
        "CN112766493B",
        "CN113556287B",
        "CN113485203B",
        "CN114936783B",
        "CN115496208B"
    ]
}