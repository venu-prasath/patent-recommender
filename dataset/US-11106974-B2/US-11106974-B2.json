{
    "patent_id": "US-11106974-B2",
    "title": "Pre-training of neural network by parameter decomposition ",
    "assignee": "International Business Machines Corporation",
    "publication_date": "2021-08-31",
    "patent_link": "https://patents.google.com/patent/US11106974B2/en",
    "inventors": [
        "Takashi Fukuda",
        "Osamu Ichikawa"
    ],
    "classifications": [
        "G06N3/08",
        "G06N3/084",
        "G06N3/04",
        "G06N3/045",
        "G06N3/0454",
        "G06N7/005",
        "G06N7/01",
        "G10L15/144",
        "G10L15/16",
        "G10L15/26",
        "G10L25/30",
        "G10L15/063"
    ],
    "abstract": "A technique for training a neural network including an input layer, one or more hidden layers and an output layer, in which the trained neural network can be used to perform a task such as speech recognition. In the technique, a base of the neural network having at least a pre-trained hidden layer is prepared. A parameter set associated with one pre-trained hidden layer in the neural network is decomposed into a plurality of new parameter sets. The number of hidden layers in the neural network is increased by using the plurality of the new parameter sets. Pre-training for the neural network is performed.",
    "claims": "\n1. A computer-implemented method for training a neural network including an input layer, one or more hidden layers, and an output layer, the method comprising:\npreparing a base of the neural network, the base having at least a pre-trained hidden layer;\ndecomposing a parameter set, associated with one pre-trained hidden layer in the neural network, into a plurality of new parameter sets;\nincreasing the number of the hidden layers in the neural network based at least in part on the plurality of new parameter sets, wherein increasing the number of hidden layers in the neural network includes:\nreplacing the one pre-trained hidden layer with multiple new hidden layers;\nsetting a first parameter set of the plurality of new parameter sets to an upper layer of the multiple new hidden layers; and\nsetting a second parameter set of the plurality of new parameter sets to a lower of the multiple new hidden layers; and\npre-training the neural network.\n2. The method of claim 1, wherein the decomposing, the increasing and the pre-training are repeatedly performed in a layer-by-layer manner until the number of the hidden layers in the neural network reaches a predetermined number while the decomposing is applied to a same position in the neural network.\n3. The method of claim 1, wherein the parameter set is represented by a matrix at least partially and the decomposing comprises:\napplying singular value decomposition to the matrix to generate a first orthogonal matrix, a singular value matrix and a second orthogonal matrix, the plurality of the new parameter sets being based on the first orthogonal matrix, the singular value matrix and the second orthogonal matrix.\n4. The method of claim 3, wherein:\nthe matrix has a weight matrix connected to the one pre-trained hidden layer;\nthe first new parameter set includes a first weight matrix obtained from the first orthogonal matrix; and\nthe second new parameter set includes a second weight matrix obtained from the second orthogonal matrix.\n5. The method of claim 3, wherein the parameter set further includes:\na bias vector of the one pre-trained hidden layer;\nthe first new parameter set includes a first bias vector; and\nthe second new parameter set includes a second bias vector, the first bias vector and the second bias vector being calculated by using the bias vector of the one pre-trained hidden layer.\n6. The method of claim 5, wherein the first bias vector is set to be identical to the bias vector of the one pre-trained hidden layer and the second bias vector is set to be zero.\n7. The method of claim 4, wherein:\nthe matrix has a block of an extended bias vector arranged next to a block of the weight matrix in the matrix, the extended bias vector having a bias vector of the one pre-trained hidden layer and a constant of 1 attached thereto;\nthe first new parameter set includes a first bias vector; and\nthe second new parameter set includes a second bias vector, the second bias vector being obtained by the singular value decomposition for the matrix, the first vector being set to be a value calculated by using the bias vector of the one pre-trained hidden layer and the second bias vector.\n8. The method of claim 2, wherein the pre-training is discriminative pre-training and the method further comprises:\nfine-tuning the neural network after the number of the hidden layers in the neural network reaches the predetermined number.\n9. The method of claim 1, wherein the one or more hidden layers of the neural network includes one or more fully-connected layers under the output layer and one or more convolutional layers between the input layer and the one or more fully-connected layers, and one layer of the one or more fully-connected layers after previous pre-training is targeted as the one pre-trained hidden layer for the decomposing.\n10. The method of claim 1, wherein the neural network is a neural network based acoustic model.\n11. A computer system training a neural network including an input layer, one or more hidden layers and an output layer, by executing program instructions, the computer system comprising:\na memory storing the program instructions;\na processing circuitry in communications with the memory for executing the program instructions, wherein the processing circuitry is configured to:\nprepare a base of the neural network, the base having at least a pre-trained hidden layer;\ndecompose a parameter set associated with one pre-trained hidden layer in the neural network into a plurality of new parameter sets;\nincrease the number of hidden layers in the neural network based at least in part on the plurality of new parameter sets, wherein increasing the number of hidden layers in the neural network includes:\nreplacing the one pre-trained hidden layer with multiple new hidden layers;\nsetting a first parameter set of the plurality of new parameter sets to an upper layer of the multiple new hidden layers; and\nsetting a second parameter set of the plurality of new parameter sets to a lower of the multiple new hidden layers; and\npre-train the neural network.\n12. The computer system of claim 11, wherein the decomposition, the increase, and the pre-training are repeatedly performed in a layer-by-layer manner until the number of the hidden layers in the neural network reaches a predetermined number while the decomposition of the parameter set is applied to same position in the neural network.\n13. The computer system of claim 11, wherein the parameter set is represented by a matrix at least partially and the parameter set is decomposed by applying singular value decomposition to the matrix to generate a first orthogonal matrix, a singular value matrix, and a second orthogonal matrix, the plurality of the new parameter sets being based on the first orthogonal matrix, the singular value matrix and the second orthogonal matrix.\n14. The computer system of claim 12, wherein the pre-training is discriminative pre-training and the processing circuitry is further configured to:\nfine-tune the neural network after the number of the hidden layers in the neural network reaches the predetermined number.\n15. A computer program product for training a neural network including an input layer, one or more hidden layers and an output layer, the computer program product comprising a computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer to cause the computer to perform a method comprising:\npreparing a base of the neural network, the base having at least a pre-trained hidden layer;\ndecomposing a parameter set, associated with one pre-trained hidden layer in the neural network, into a plurality of new parameter sets;\nincreasing the number of the hidden layers in the neural network based at least in part on the plurality of new parameter sets, wherein increasing the number of hidden layers in the neural network includes:\nreplacing the one pre-trained hidden layer with multiple new hidden layers;\nsetting a first parameter set of the plurality of new parameter sets to an upper layer of the multiple new hidden layers; and\nsetting a second parameter set of the plurality of new parameter sets to a lower of the multiple new hidden layers; and\npre-training the neural network.",
    "status": "Active",
    "citations_own": [
        "US5596681A",
        "WO2002061679A2",
        "US20130138436A1",
        "US20130304395A1",
        "US20140372112A1",
        "WO2015011688A2",
        "US20150170020A1",
        "US20150269933A1",
        "US9418334B2",
        "US20170024642A1"
    ],
    "citations_ftf": [],
    "citedby_own": [],
    "citedby_ftf": [
        "US10706327B2",
        "WO2019067542A1",
        "US11250038B2",
        "US10832137B2",
        "US20190258928A1",
        "US11593633B2",
        "US10992619B2",
        "CN113795850A",
        "SG10201904549QA",
        "CN112215329B",
        "US11227579B2",
        "US11521063B1",
        "CN111461314B",
        "US11710476B2",
        "CN111931698B",
        "CN112434804A",
        "CN115345367B"
    ]
}