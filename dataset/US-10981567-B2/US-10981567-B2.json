{
    "patent_id": "US-10981567-B2",
    "title": "Feature-based prediction ",
    "assignee": "Zoox, Inc.",
    "publication_date": "2021-04-20",
    "patent_link": "https://patents.google.com/patent/US10981567B2/en",
    "inventors": [
        "Benjamin John Sapp",
        "Daylen Guang Yu Yang"
    ],
    "classifications": [
        "B60W30/0956",
        "G08G1/167",
        "G01S13/00",
        "G01S13/86",
        "G01S13/931",
        "G01S15/931",
        "G01S17/931",
        "G01S7/412",
        "G01S7/417",
        "G01S7/539",
        "G05D1/0221",
        "G06K9/00791",
        "G06K9/00798",
        "G06K9/00825",
        "G06N20/00",
        "G06V20/56",
        "G06V20/584",
        "G06V20/588",
        "G08G1/00",
        "G08G1/0112",
        "G08G1/0129",
        "B60W2554/00",
        "G01S13/862",
        "G01S13/865",
        "G01S2013/9318",
        "G01S2013/93185",
        "G01S2013/9319",
        "G05D2201/0213",
        "G06K2009/00738",
        "G06N20/20",
        "G06N5/003",
        "G06N5/01",
        "G06V20/44"
    ],
    "abstract": "Feature-based prediction is described. In an example, a vehicle can capture sensor data while traversing an environment and can provide the sensor data to computing system(s). The sensor data can indicate event(s), such as a lane change, associated with agent(s) in the environment. The computing system(s) can determine, based on the sensor data, a time associated with the event and can determine features associated with a period of time relative to the time of the event. In an example, the computing system(s) can aggregate the features with additional features associated with other similar events to generate training data and can train, based at least in part on the training data, a machine learned model for predicting new events. In an example, the machine learned model can be transmitted to vehicle(s), which can be configured to alter drive operation(s) based, at least partly, on output(s) of the machine learned model.",
    "claims": "\n1. A first vehicle comprising:\none or more processors; and\nnon-transitory computer-readable media storing instructions executable by the one or more processors, wherein the instructions, when executed by the one or more processors, cause the first vehicle to perform actions comprising:\nreceiving sensor data from a sensor of the first vehicle traveling in a lane;\nreceiving map data associated with an environment associated with the first vehicle;\ninputting a representation of the sensor data and the map data into a machine learned model trained to predict events proximate the first vehicle, the machine learned model trained based at least in part on a set of representations of previously recorded sensor data and map data collected at different time periods from one or more past cut-in events;\nreceiving, from the machine learned model, an indication of a new event proximate the first vehicle driving in the lane, the new event comprising a second vehicle being predicted to enter the lane from a second lane; and\ndetermining a trajectory to navigate the first vehicle based at least in part on the indication of the new event proximate the vehicle driving in the lane.\n2. The vehicle of claim 1, wherein:\nthe lane is a first lane;\neach representation of the sensor data comprises a set of features aggregated over a period of time, the set of features indicative of one or more of:\na set of speeds of the second vehicle;\na set of positions of the second vehicle;\na set of orientations of the second vehicle;\na set of accelerations of the second vehicle;\na set of distance between the second vehicle and the first lane;\na width of the second lane; and\na semantic feature associated with the second vehicle or the environment.\n3. The vehicle of claim 2, wherein:\nthe instructions cause the vehicle to perform further actions comprising:\ndetermining the trajectory to navigate the first vehicle that causes the first vehicle to slow down to increase a follow distance between the first vehicle and another vehicle that the first vehicle is following to allow the second vehicle to enter the first lane.\n4. The vehicle of claim 2, wherein:\nthe instructions cause the vehicle to perform further actions comprising:\ndetermining the trajectory to navigate the first vehicle that causes the first vehicle to change to another lane to allow the second vehicle to enter the first lane.\n5. The vehicle of claim 1, wherein the machine learned model comprises at least a neural network model.\n6. The vehicle of claim 1, wherein the sensor comprises a light detection and ranging (LIDAR) sensor, a radio detection and ranging (RADAR) sensor, a sound navigation and ranging (SONAR) sensor, a location sensor, an inertial sensor, a camera, a microphone, or an environment sensor.\n7. A method comprising:\nreceiving sensor data from a sensor of a first vehicle traveling in a first lane of an environment, the sensor data associated with a second vehicle traveling in a second lane in the environment;\nreceiving map data associated with the environment;\nanalyzing the sensor data and the map data using a machine learned model trained to predict a first event proximate the first vehicle in the environment, the machine learned model trained based at least in part on a set of representations of previously recorded sensor data and map data collected at different time periods from one or more past cut-in events;\nreceiving, from the machine learned model, an indication of the first event proximate the first vehicle in the environment, the first event comprising a cut-in event of the second vehicle being predicted to enter the first lane from the second lane; and\ndetermining a trajectory to navigate the first vehicle in the environment in response to the indication of first event comprising the cut-in event of the second vehicle being predicted to enter the first lane from the second lane.\n8. The method of claim 7, wherein:\nanalyzing the sensor data comprises:\ndetermining, based at least in part on the sensor data, a set of features comprising one or more of:\na pose of the second vehicle,\na velocity of the second vehicle,\nan acceleration of the second vehicle,\na direction of travel of the second vehicle,\na distance between the second vehicle and a proximate lane,\na width of a lane within which the second vehicle is positioned, or\na semantic feature associated with the second vehicle or the environment;\naggregating one or more sets of features over a period of time; and\ninputting the one or more sets of features into the machine learned model.\n9. The method of claim 8, wherein the period of time comprises at least one second.\n10. The method of claim 8, further comprising:\ndetermining, based at least in part on the sensor data, a second set of features associated with the cut-in event in real-time; and\nadding the second set of features to the training data.\n11. The method of claim 7, wherein the machine learned model comprises a gradient boosted decision tree.\n12. The method of claim 7, wherein the sensor comprises a light detection and ranging (LIDAR) sensor, a radio detection and ranging (RADAR) sensor, a sound navigation and ranging (SONAR) sensor, a location sensor, an inertial sensor, a camera, a microphone, or an environment sensor.\n13. A non-transitory computer-readable medium having a set of instructions that, when executed, cause one or more processors to perform operations comprising:\nreceiving sensor data from a sensor of a first vehicle traveling in a first lane of an environment, the sensor data representing a second vehicle traveling in a second lane in the environment;\nanalyzing the sensor data and map data using a machine learned model trained to predict a first event proximate the first vehicle in the environment, the machine learned model trained based at least in part on a set of representations of previously recorded sensor data and map data collected at different time periods from one or more past cut-in events;\nreceiving, from the machine learned model, an indication of the first event proximate the first vehicle in the environment, the first event comprising a cut-in event of the second vehicle being predicted to enter the first lane from the second lane; and\ndetermining a trajectory to navigate the first vehicle in the environment in response to the indication of the first event comprising the cut-in event of the second vehicle being predicted to enter the first lane from the second lane.\n14. The non-transitory computer-readable medium of claim 13, wherein:\nanalyzing the sensor data comprises:\ndetermining, based at least in part on the sensor data, a set of features comprising one or more of:\na pose of the second vehicle,\na velocity of the second vehicle,\nan acceleration of the second vehicle,\na direction of travel of the second vehicle,\na distance between the second vehicle and a proximate lane,\na width of a lane within which the second vehicle is positioned, or\na semantic feature associated with the second vehicle or the environment;\naggregating one or more sets of features over a period of time; and\ninputting the one or more sets of features into the machine learned model.\n15. The non-transitory computer-readable medium of claim 13, wherein the machine learned model comprises a gradient boosted decision tree.\n16. The non-transitory computer-readable medium of claim 15, wherein the training data comprises data indicative of a change of a lane identification associated with a sample vehicle.\n17. The non-transitory computer-readable medium of claim 15, wherein instructions, when executed, further cause the one or more processors to perform operations comprising:\ndetermining, based at least in part on the sensor data, a second set of features associated with the cut-in event in real-time; and\nadding the second set of features to the training data.\n18. The vehicle of claim 1, wherein the instructions, when executed by the one or more processors, cause the first vehicle to further perform actions comprising:\ndetermining a second trajectory to navigate the first vehicle that causes the vehicle to slow down to stop to allow a pedestrian to cross the lane in which the first vehicle is traveling.\n19. The method of claim 7, further comprising receiving additional sensor data from the sensor of the first vehicle traveling in the first lane of the environment, the sensor data associated with a pedestrian crossing the first lane.\n20. The non-transitory computer-readable medium of claim 13, further comprising instructions that, when executed, further cause the one or more processors to perform operations comprising:\ndetermining a second trajectory to navigate the first vehicle in response to an indication of a pedestrian being predicted to cross the first lane.",
    "status": "Active",
    "citations_own": [
        "US5177685A",
        "US20030187578A1",
        "US6707421B1",
        "US20050049785A1",
        "US20110125344A1",
        "US20110205042A1",
        "US8112225B2",
        "US8121749B1",
        "US8473447B2",
        "US8645310B2",
        "US20140324268A1",
        "US8880272B1",
        "US9381916B1",
        "WO2016130719A2",
        "US9495874B1",
        "US9507347B1",
        "US9558659B1",
        "US20170031361A1",
        "US9568915B1",
        "US9612123B1",
        "US20170131719A1",
        "WO2017091690A1",
        "US20170193338A1",
        "US20170192426A1",
        "US20170192437A1",
        "GB2547082A",
        "EP3217332A1",
        "US20170277195A1",
        "US20170364758A1",
        "US20180068191A1",
        "US20180095465A1",
        "US20180137380A1",
        "US20180144202A1",
        "US20180148051A1",
        "US20180164816A1",
        "US20180224860A1",
        "US10061322B1",
        "US20180251126A1",
        "US20190025841A1",
        "US20190051069A1",
        "US20190061765A1",
        "US20190092318A1",
        "US20190122059A1",
        "US20190250626A1",
        "US10387736B2",
        "US20190272750A1",
        "US20190308620A1",
        "US20190329769A1",
        "US20190344804A1",
        "US20190354786A1",
        "US20200086855A1",
        "US20200110416A1"
    ],
    "citations_ftf": [],
    "citedby_own": [],
    "citedby_ftf": [
        "US10353390B2",
        "US10671076B1",
        "US10678244B2",
        "US10671349B2",
        "US11409692B2",
        "US11157441B2",
        "US10860019B2",
        "US11561791B2",
        "US10955851B2",
        "JP7152165B2",
        "US10414395B1",
        "US11126873B2",
        "US10901400B2",
        "US11215999B2",
        "US11636333B2",
        "US11562231B2",
        "US11126186B2",
        "JP7052660B2",
        "US11196678B2",
        "JP7225695B2",
        "US11537811B2",
        "US11610117B2",
        "US10997461B2",
        "US11567514B2",
        "US10956755B2",
        "US10891318B2",
        "KR102206512B1",
        "US11242054B2",
        "US11788846B2",
        "CN110796856B",
        "US11104269B2",
        "US20210124355A1",
        "US10816993B1",
        "CN110825093B",
        "US11590969B1",
        "CN111002980B",
        "US20210200229A1",
        "CN113460042B",
        "KR102317842B1",
        "DE102020118629B4",
        "US11524627B2",
        "US20220035376A1",
        "CN114056347A",
        "CN112269385B",
        "CN112330967B",
        "CN112785845B",
        "CN112967498B",
        "US20220388534A1",
        "US20230059808A1",
        "US20230202530A1"
    ]
}