{
    "patent_id": "US-9104961-B2",
    "title": "Modeling a data generating process using dyadic Bayesian models ",
    "assignee": "Microsoft Technology Licensing, Llc",
    "publication_date": "2015-08-11",
    "patent_link": "https://patents.google.com/patent/US9104961B2/en",
    "inventors": [
        "Andrew D. Gordon",
        "Thore Graepel",
        "Aditya Nori",
        "Sriram Rajamani",
        "Johannes Borgstroem"
    ],
    "classifications": [
        "G06N7/005",
        "G06N20/00",
        "G06N5/048",
        "G06N7/01",
        "G06N99/005"
    ],
    "abstract": "There is provided a method and system for modeling a data generating process. The method includes generating a dyadic Bayesian model including a pair of probabilistic functions representing a prior distribution and a sampling distribution, and modeling a data generating process based on the dyadic Bayesian model using observed data. The method includes generating a learner object for the dyadic Bayesian model. The method further includes training the dyadic Bayesian model with the learner object based on the observed data to produce a trained dyadic Bayesian model. The method also includes generating a posterior distribution over parameters based on the trained dyadic Bayesian model. The method also further includes generating a posterior predictive distribution based on the posterior distribution. The method also includes predicting an outcome of observable variables based on the posterior predictive distribution.",
    "claims": "\n1. A method for modeling a data generating process, comprising:\ngenerating a dyadic Bayesian model comprising a pair of probabilistic functions representing a prior distribution and a sampling distribution; and\nmodeling a data generating process based on the dyadic Bayesian model using observed data, wherein modeling the data generating process further comprises:\ngenerating a learner object for the dyadic Bayesian model;\ntraining the dyadic Bayesian model with the learner object based on the observed data to produce a trained dyadic Bayesian model;\ngenerating a posterior distribution over parameters based on the trained dyadic Bayesian model;\ngenerating a posterior predictive distribution based on the posterior distribution; and\npredicting an outcome of observable variables based on the posterior predictive distribution.\n2. The method of claim 1, comprising:\ngenerating a sampler object for the dyadic Bayesian model; and\nusing the sampler object to test the learner object by computing synthetic data for the dyadic Bayesian model and using the synthetic data to judge an accuracy of the posterior distribution or the posterior predictive distribution.\n3. The method of claim 1, comprising generating a new dyadic Bayesian model based on one or more previously-generated dyadic Bayesian models using a model combinator.\n4. The method of claim 1, comprising generating the prior distribution by computing a probability density function of a prior function corresponding to the dyadic Bayesian model.\n5. The method of claim 1, comprising generating the sampling distribution by computing a probability density function of a generation function corresponding to the dyadic Bayesian model.\n6. The method of claim 1, comprising generating the prior distribution by computing a probability mass function of a prior function corresponding to the dyadic Bayesian model.\n7. The method of claim 1, comprising generating the sampling distribution by computing a probability mass function of a generation function corresponding to the dyadic Bayesian model.\n8. The method of claim 1, comprising using a programming language to generate the dyadic Bayesian model.\n9. A system for modeling a data generating process, comprising:\na processor that is adapted to execute stored instructions; and\na system memory, wherein the system memory comprises code configured to:\ngenerate a dyadic Bayesian model comprising a pair of probabilistic functions representing a prior distribution and a sampling distribution, a sampler object, and a learner object;\ntrain the dyadic Bayesian model based on observed data with the learner object to produce a trained dyadic Bayesian model;\ngenerate a posterior distribution over parameters based on the trained dyadic Bayesian model;\ngenerate a posterior predictive distribution based on the posterior distribution; and\npredict an outcome of observable variables using the posterior predictive distribution.\n10. The system of claim 9, wherein the system memory comprises code configured to test the learner object with the sampler object by computing synthetic data for the dyadic Bayesian model and using the synthetic data to judge an accuracy of the posterior distribution or the posterior predictive distribution.\n11. The system of claim 9, wherein the system memory comprises code configured to generate a new dyadic Bayesian model based on one or more previously-generated dyadic Bayesian models using a model combinator.\n12. The system of claim 9, wherein the parameters comprise general parameters, hyperparameters, input parameters, or output parameters, or any combinations thereof.\n13. The system of claim 9, comprising code configured to provide an inference engine for executing the sampler object and the learner object.\n14. The system of claim 9, wherein the posterior predictive distribution of the data set comprises a distribution of output data within the data set based on input data within the data set.\n15. The system of claim 9, wherein the observable variables comprise new input data, and wherein the posterior predictive distribution is used to predict new output data based on the new input data.\n16. One or more computer-readable storage media comprising a plurality of instructions that when executed by a processor, cause the processor to:\ngenerate a dyadic Bayesian model comprising a pair of probabilistic functions representing a prior distribution and a sampling distribution from one or more previously-generated dyadic Bayesian models using a model combinator;\nmodel a data generating process by transforming data according to the dyadic Bayesian model;\ngenerate a learner object for the dyadic Bayesian model;\ntrain the dyadic Bayesian model with the learner object based on the observed data to produce a trained dyadic Bayesian model;\ngenerate a posterior distribution over parameters based on the trained dyadic Bayesian model;\ngenerate a posterior predictive distribution based on the posterior distribution; and\npredict an outcome of observable variables based on the posterior predictive distribution.\n17. The one or more computer-readable storage media of claim 16, wherein the model combinator comprises a mixture model combinator.\n18. The one or more computer-readable storage media of claim 16, wherein the model combinator comprises an identically and independently distributed array (IIDArray) model combinator.\n19. The one or more computer-readable storage media of claim 16, wherein the plurality of instructions cause the processor to generate a sampler object and a learner object for modeling the data generating process.\n20. The one or more computer-readable storage media of claim 16, comprising using a programming language to generate the dyadic Bayesian model.",
    "status": "Active",
    "citations_own": [
        "US20070239431A1",
        "WO2009010293A2",
        "US7650272B2",
        "US7984002B2",
        "US20120185728A1"
    ],
    "citations_ftf": [
        "CN100589122C",
        "CN102214246B"
    ],
    "citedby_own": [
        "US20170090881A1",
        "US10375095B1",
        "WO2020232460A1",
        "US20210056456A1",
        "US11630881B2"
    ],
    "citedby_ftf": [
        "US10496929B2",
        "CN109690576A",
        "JP6686151B2",
        "US10977574B2",
        "US11568179B2",
        "US11573962B2",
        "US20200175415A1",
        "US11580425B2"
    ]
}