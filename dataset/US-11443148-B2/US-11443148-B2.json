{
    "patent_id": "US-11443148-B2",
    "title": "Multiple stage image based object detection and recognition ",
    "assignee": "Uatc, Llc",
    "publication_date": "2022-09-13",
    "patent_link": "https://patents.google.com/patent/US11443148B2/en",
    "inventors": [
        "Carlos Vallespi-Gonzalez",
        "Joseph Lawrence Amato",
        "George Totolos, Jr."
    ],
    "classifications": [
        "G06K9/6268",
        "G06V20/64",
        "G06F18/214",
        "G06F18/241",
        "G06F18/24323",
        "G06K9/6256",
        "G06K9/6282",
        "G06N20/00",
        "G06N7/005",
        "G06N7/01",
        "G06T15/08",
        "G06T7/521",
        "G06V10/28",
        "G06V10/50",
        "G06V10/56",
        "G06V10/764",
        "G06V20/58",
        "G06V20/584",
        "G05D1/0238",
        "G05D2201/0213",
        "G06T2207/20081",
        "G06T2207/30261",
        "G06T2210/12"
    ],
    "abstract": "Systems, methods, tangible non-transitory computer-readable media, and devices for autonomous vehicle operation are provided. For example, a computing system can receive object data that includes portions of sensor data. The computing system can determine, in a first stage of a multiple stage classification using hardware components, one or more first stage characteristics of the portions of sensor data based on a first machine-learned model. In a second stage of the multiple stage classification, the computing system can determine second stage characteristics of the portions of sensor data based on a second machine-learned model. The computing system can generate an object output based on the first stage characteristics and the second stage characteristics. The object output can include indications associated with detection of objects in the portions of sensor data.",
    "claims": "\n1. A computer-implemented method of autonomous vehicle operation, the computer-implemented method comprising:\nreceiving sensor data comprising image;\ndetermining in a first stage of a multiple stage classification using one or more hardware components, one or more first stage characteristics of the sensor data based in part on a first machine-learned model, wherein the first stage characteristics are determined by the first machine-learned model with a first level of confidence;\nexcluding a portion of the sensor data corresponding to one or more areas within the image from an input to a second stage of the multiple stage classification based on the one or more first stage characteristics of the sensor data;\ndetermining in the second stage of the multiple stage classification and based on the input to the second stage, one or more second stage characteristics of the sensor data based in part on a second machine-learned model, wherein the second stage characteristics are determined by the second machine-learned model with a second level of confidence that is higher than the first level of confidence; and\ngenerating an object output based on the second stage characteristics, wherein the object output describes detection of one or more objects in the sensor data.\n2. The computer-implemented method of claim 1, wherein the one or more first stage characteristics of the sensor data determined in the first stage of the multiple stage classification describes a likelihood that the portion of the sensor data that is excluded from the input to the second stage of the multiple stage classification contains objects.\n3. The computer-implemented method of claim 1, further comprising generating in the first stage, a heat map associated with the sensor data, the heat map describing a probability of an object being contained within a respective area of a plurality of areas of the sensor data.\n4. The computer-implemented method of claim 3, wherein excluding the portion of the sensor data from the input to the second stage of the multiple stage classification based on the one or more first stage characteristics of the sensor data comprises excluding the portion of the sensor data based on the heat map.\n5. The computer-implemented method of claim 3, wherein excluding the portion of the sensor data from the input to the second stage of the multiple stage classification based on the one or more first stage characteristics of the sensor data comprises excluding the portion of the sensor data based on the visual descriptor output associated with the sensor data.\n6. The computer-implemented method of claim 1, wherein the portion of the sensor data that is excluded from the input to the second stage of the multiple stage classification is associated with one or more background portions of the sensor data.\n7. The computer-implemented method of claim 1, wherein the input to the second stage of the multiple stage classification is associated with one or more foreground portions of the sensor data.\n8. The computer-implemented method of claim 1, further comprising:\ngenerating in the first stage and based in part on the sensor data, visual descriptor output associated with the sensor data, the visual descriptor output comprising color hue information, color saturation information, brightness information, or histogram of oriented gradients information, wherein the one or more first stage characteristics are determined based in part on the visual descriptor output.\n9. A computing system comprising:\none or more processors; and\na memory comprising one or more computer-readable media, the memory storing computer-readable instructions that when executed by the one or more processors cause the one or more processors to perform operations comprising:\nreceiving sensor data comprising an image;\ndetermining, in a first stage of a multiple stage classification using one or more hardware components, one or more first stage characteristics of the sensor data based in part on a first machine-learned model, wherein the first stage characteristics are determined by the first machine-learned model with a first level of confidence;\nexcluding a portion of the sensor data corresponding to one or more areas within the image from an input to a second stage of the multiple stage classification based on the one or more first stage characteristics of the sensor data,\ndetermining in the second stage of the multiple stage classification and based on the input to the second stage, one or more second stage characteristics of the sensor data based in part on a second machine-learned model, wherein the second stage characteristics are determined by the second machine-learned model with a second level of confidence that is higher than the first level of confidence; and\ngenerating an object output based on the second stage characteristics, wherein the object output describes detection of one or more objects in the sensor data.\n10. The computing system of claim 9, wherein the one or more first stage characteristics of the sensor data determined in the first stage of the multiple stage classification describes a likelihood that the portion of the sensor data that is excluded from the input to the second stage of the multiple stage classification contains objects.\n11. The computing system of claim 9, wherein the operations further comprise generating, in the first stage, a heat map associated with the sensor data, the heat map describing a probability of an object being contained within a respective area of a plurality of areas of the sensor data.\n12. The computing system of claim 9, wherein the portion of the sensor data that is excluded from the input to the second stage of the multiple stage classification is associated with one or more background portions of the sensor data.\n13. The computing system of claim 9, wherein the input to the second stage of the multiple stage classification is associated with one or more foreground portions of the sensor data.\n14. The computing system of claim 9, wherein the operations further comprise generating, in the first stage and based in part on the sensor data, visual descriptor output associated with the sensor data, the visual descriptor output comprising color hue information, color saturation information, brightness information, or histogram of oriented gradients information, wherein the one or more first stage characteristics are determined based in part on the visual descriptor output.\n15. The computing system of claim 14, wherein excluding the portion of the sensor data from the input to the second stage of the multiple stage classification based on the one or more first stage characteristics of the sensor data comprises excluding the portion of the sensor data based on the visual descriptor output associated with the sensor data.\n16. One or more tangible, non-transitory computer-readable media storing computer-readable instructions that when executed by one or more processors cause the one or more processors to perform operations, the operations comprising:\nreceiving, by a computing system comprising one or more computing devices, sensor data comprising image;\ndetermining, by the computing system, in a first stage of a multiple stage classification using one or more hardware components, one or more first stage characteristics of the sensor data based in part on a first machine-learned model, wherein the first stage characteristics are determined by the first machine-learned model with a first level of confidence;\nexcluding, by the computing system, a portion of the sensor data corresponding to one or more areas within the image from an input to a second stage of the multiple stage classification based on the one or more first stage characteristics of the sensor data;\ndetermining, by the computing system, in the second stage of the multiple stage classification and based on the input to the second stage, one or more second stage characteristics of the sensor data based in part on a second machine-learned model, wherein the second stage characteristics are determined by the second machine-learned model with a second level of confidence that is higher than the first level of confidence; and\ngenerating, by the computing system, an object output based on the second stage characteristics, wherein the object output describes detection of one or more objects in the sensor data.\n17. One or more tangible, non-transitory computer-readable media of claim 16, wherein the one or more first stage characteristics of the sensor data determined in the first stage of the multiple stage classifications describes a likelihood that the portion of the sensor data that is excluded from the input to the second stage of the multiple stage classification contains objects.\n18. The one or more tangible, non-transitory computer-readable media of claim 16, wherein the operations further comprise generating, by the computing system, in the first stage, a heat map associated with the sensor data, the heat map describing a probability of an object being contained within a respective area of a plurality of areas of the sensor data.\n19. The one or more tangible, non-transitory computer-readable media of claim 18, wherein excluding the portion of the sensor data from the input to the second stage of the multiple stage classification based on the one or more first stage characteristics of the sensor data comprises excluding the portion of the sensor data based on the heat map.",
    "status": "Active",
    "citations_own": [
        "US20150363660A1",
        "US20160379094A1",
        "JP2017130155A",
        "US20180018524A1",
        "US20180211403A1",
        "US20190164290A1"
    ],
    "citations_ftf": [
        "WO2016100814A1"
    ],
    "citedby_own": [],
    "citedby_ftf": [
        "US8326775B2",
        "US9984314B2",
        "US10678244B2",
        "WO2019008581A1",
        "US10671349B2",
        "US11409692B2",
        "US11157441B2",
        "US20190079526A1",
        "US11403816B2",
        "US10984257B2",
        "WO2019136375A1",
        "US11561791B2",
        "WO2019152888A1",
        "CN111133447A",
        "US10997433B2",
        "US11537139B2",
        "US11080590B2",
        "CN108537151A",
        "US11436484B2",
        "US11334960B2",
        "US11215999B2",
        "US11636333B2",
        "US11562231B2",
        "CN109376594A",
        "US20200133308A1",
        "US11181911B2",
        "US10839694B2",
        "US11126870B2",
        "US11196678B2",
        "US11244176B2",
        "CN113039563A",
        "US10789535B2",
        "US11537811B2",
        "US11610117B2",
        "WO2020140049A1",
        "US11170299B2",
        "CN113454636A",
        "US10997461B2",
        "WO2020163390A1",
        "US11567514B2",
        "US10956755B2",
        "US11643005B2",
        "US11285963B2",
        "CN113811886A",
        "US11694088B2",
        "US11132548B2",
        "US10789527B1",
        "US10796444B1",
        "US11488290B2",
        "US11222069B2",
        "US10776669B1",
        "EP3734391A1",
        "US10984543B1",
        "US11087494B1",
        "US10937178B1",
        "US11373318B1",
        "CN110502979B",
        "US20210012230A1",
        "US11713978B2",
        "US11599799B1",
        "JP2021047705A",
        "CN111008561B",
        "US11593662B2",
        "US10748022B1",
        "CN111242178A",
        "CN111310670A",
        "US11590988B2",
        "JP7115502B2",
        "JP7359735B2",
        "JP2021165080A",
        "US11203361B2",
        "US11756424B2",
        "WO2022109000A1",
        "US11443147B2",
        "JP2022098117A",
        "US11760376B2",
        "US20230311930A1"
    ]
}