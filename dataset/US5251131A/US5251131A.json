{
    "patent_link": "https://patents.google.com/patent/US5251131A/en",
    "patent_id": "US5251131A",
    "title": "Classification of data records by comparison of records to a training database using probability weights",
    "abstract": "Classification of natural language data wherein the natural language data has an open-ended range of possible values or the data values do not have a relative order. A training database stores training records, wherein each training record includes predictor data fields. Each predictor data field containes a feature, wherein each feature is a natural language term, and a target data field containing a target value representing a classification of the record. Features may also include conjunctions of natural language terms and each feature may also be a member of a category subset of features. The training database stores, for each feature, a probability weight value representing the probability that a record will have the target value contained in the target data field if a feature contained in a corresponding predictor data field occurs in the record. Features are extracted from a new record and each feature from the new record is used to query the training records to determine the probability weights from the training records having matching features. The probability weights are accumulated for each training record to determine a comparison score representing the probability that the training record matches the new record and provide an output indicating the training records most probability matching the new record.",
    "inventors": [
        "Brij M. Masand",
        "Stephen J. Smith"
    ],
    "assignee": "Oracle International Corp",
    "classifications": [
        "G10L15/18",
        "G06F18/24147",
        "G06F40/30"
    ],
    "claims": "\n1. A system for classifying natural language data, comprising:\nmeans for storing a new record including a plurality of predictor data fields containing the natural language data expressed in natural language values,\nmeans for storing a plurality of training records,\neach training record including\na plurality of predictor data fields, each predictor data field containing a feature, wherein each feature is a natural language term, and\na target data field containing a target value representing a classification of a training record, and\nprobability weight means for storing, for each feature, a probability weight value representing a probability that a new record will have the target value contained in the target data field if a feature contained in a corresponding predictor data field occurs in the new record,\nquery means for extracting features from the new record and querying the training records with each feature extracted from the new record,\nthe query means being responsive to a match between a feature extracted from the new record and a feature stored in said training record for providing the probability weight corresponding to the feature, and\nmetric means for receiving the probability weights from the query means and accumulating for each training record a comparison score representing the probability that said training record matches the new record, and\nproviding an output indicating said target field value of said training record as said target value of the new record.\n2. The system of claim 1 for classifying records containing natural language data, wherein each probability weight value is a per-target value weight representing, for a corresponding single combination of a feature and a target field value, a conditional probability that the target field will have the target field value when the feature occurs among the features of the record containing the target field value.\n3. The system of claim 2 for classifying natural language data, wherein the probability weight means includes:\nfor each feature in each training record, a corresponding probability weight field associated with the predictor data field containing the feature, each probability weight field containing a per-target probability weight value for the feature contained in the associated predictor data field.\n4. The system of claim 1 for classifying natural language data, wherein each probability weight value is a cross-target value weight representing, for a given corresponding feature, a relative conditional probability that the corresponding feature is of significance in determining target field values across all target field values of the training records.\n5. The system of claim 4 for classifying natural language data, wherein the probability weight means further includes:\nfor each feature, a probability weight field for containing the cross-target probability weight value for the feature.\n6. The system of claim 4 for classifying natural language data, wherein a per-target value of a feature is a conditional probability that a target field will have a target field value given that the feature occurs among the features of the record containing the target field value and wherein each cross-target value weight is the sum of the squares of the per-target value weights for the corresponding feature.\n7. The system of claim 1 for classifying natural language data, wherein natural language data is characterized by data values having an open-ended range of possible values.\n8. The system of claim 1 for classifying natural language data, wherein natural language data is characterized in that the natural language data values do not have a relative order or ranking.\n9. The system of claim 1 for classifying natural language data, wherein features further comprise conjunctions of natural language terms.\n10. The system of claim 1 for classifying natural language data, wherein features comprise words and conjunctions of pairs of words.\n11. The system of claim 1 for classifying natural language data, wherein:\neach feature is a member of one of a plurality of category subsets of features, and\na feature appearing in identical form a multiplicity of the category subsets comprises a corresponding multiplicity of separate and distinct features.\n12. The system of claim 1 for classifying natural language data, wherein the accumulated comparison score for each training record is the sum of the probability weights of the features in the new record which match features in the training record.\n13. The system of claim 1 for classifying natural language data, wherein the accumulated comparison score for each training record is the highest probability weight of all the probability weights of all features in the new record that match features in the training record.\n14. The system of claim 1 for classifying natural language data, wherein the accumulated comparison score for each training record is a cumulative probability of error in predicting the target value of the new record target field over all features of the new record which matched features of the training record.\n15. The system of claim 1 for classifying natural language data, wherein the metric means further comprises means for selecting from the training records a subset of the training records having the highest comparison scores, without regard to the target field values of the selected training records, aggregating the selected training records by their target field values, and selecting the target field value of the training records having the highest aggregate match score as the target field value for the new record.\n16. The system of claim 15 for classifying natural language data, wherein the metric means further comprises means for determining a confidence score for the selected target field value as the ratio of the aggregate match score of the training records having the highest aggregate match score to the sum of the aggregate match scores of the training records having the highest aggregate match score and the second highest aggregate match score.\n17. In a data parallel system, means for classifying natural language data, comprising:\na plurality of processing elements for storing a corresponding plurality of training records, each training record residing in a process in a memory associated with the corresponding processor element and including\na plurality of predictor data fields, each predictor data field containing a feature, wherein each feature is a natural language term, and\na target data field containing a target value representing a classification of a training record,\na probability weight means for storing, for each feature, a probability weight value representing a probability that a new record will have said target value contained in the target data field if a feature contained in a corresponding predictor data field occurs in the new record,\na query means for extracting features from the new record and querying the training records with each feature extracted from the new record, including\na control means for storing the new record, extracting the features from the new record and transmitting the features to the processing elements, and\nthe processing elements for reading the features from each associated training record and responsive to a match between a feature extracted from the new record and a feature stored in said training record for reading the probability weight corresponding to the feature, and\na metric means for receiving the probability weights from the query means and accumulating for each training record a comparison score representing the probability that the training record matches the new record and providing an output indicating said training record most probably matching the new record, including\nthe processing elements for receiving the probability weights, each processor being responsive to instructions from the control means for accumulating for each corresponding training record a comparison score representing the probability that the training record matches the new record, and\na global combining means for providing an output indicating a training record most probability matching the new record.\n18. A system for generating training records for use in classifying natural language data, comprising:\nmeans for storing a plurality of historic records,\neach historic record including\na target data field containing a target value representing a classification of a historic record, and\na plurality of predictor data fields, each predictor data field containing a natural language term, and\nmeans for storing a plurality of training records,\neach training record including\na plurality of predictor data fields, each predictor data field containing a feature, wherein each feature is a natural language term, and\nsaid target data field containing said target value representing a classification of a training record, and\na probability weight memory for storing, for each feature,\na probability weight value representing a probability that a new record will have the target value contained in the target data field if said feature contained in a corresponding predictor data field occurs in the new record,\nmeans for reading the natural language terms from each of the historic records and identifying each feature appearing in the historic records,\nprobability weight generating means for\nselecting in turn each feature identified from the historic records, and\nfor each feature, selecting in turn each historic record target value for which the feature appears,\ndetermining, for each historic record target value for each feature a probability weight, and\ngenerating, for each historic record, a corresponding training record and\nstoring in the predictor data fields of the training record the features identified from the corresponding historic record, and\nin the target data field of the training record the target value from the historic record, and\nstoring the probability weight generated for the feature and target in the probability weight memory.\n19. In a data parallel system, means for generating training records for use in classifying natural language data, comprising:\na first plurality of processing elements for storing a corresponding plurality of historic records,\neach historic record residing in a process in a memory of a corresponding processor element and including\na target data field containing a target value representing a classification of a historic record, and\na plurality of predictor data fields, each predictor data field containing a natural language term, and\na second plurality of processing elements for storing a corresponding plurality of training records,\neach training record residing in said process in said memory of a corresponding processor element and including\na plurality of predictor data fields, each predictor data field containing a feature, wherein each feature is a natural language term, and\na target data field containing a target value representing a classification of a training record,\na probability weight memory for storing, for each feature,\na probability weight value representing a probability that a new record will have the target value contained in the target data field if said feature contained in a corresponding predictor data field occurs in the new record,\na query means including a control means and the processing elements for reading the natural language terms from each of the historic records and identifying each feature appearing in the historic records,\na probability weight generating means including the control means and the processing elements for\nselecting in turn each feature identified from the historic records, and\nfor each feature, selecting in turn each historic record target value for which the feature appears, and,\na global combining means for determining, for each historic record target value and for each feature a probability weight value representing the probability that a record will have the target value contained in the target data field if said feature contained in a corresponding predictor data field occurs in the record, and\nstoring in the predictor data fields of each training record the features identified from the corresponding historic record, and\nin the target data fields of the corresponding training record the target value from the historic record, and\nstoring the probability weight generated for the feature and target value in a probability weight memory.\n20. In a data parallel system, means for classifying natural language data, comprising:\na plurality of processing elements for storing a corresponding plurality of training records,\neach training record including\na plurality of predictor data fields, each predictor data field containing a feature, wherein each feature is a natural language term, and\na target data field containing a target value representing a classification of a training record,\ncontrol means for storing a new record containing features comprised of the natural language data, and for broadcasting the features of the new record to the processing elements storing the training records,\neach processing element being responsive to the broadcast features of the new record to construct, in a process associated with each training data record, a boolean comparison table for storing indications of matches between the broadcast new record features and the features of the corresponding training record,\nscanning means for reading the indications of matches in each boolean comparison table and determining, for each broadcast feature of the new record and each target value of the training records,\nnumber of indications of matches between the broadcast feature and the features of the training record for each target value, and\nnumber of indications of matches between the broadcast feature and the features of the training record over all target values, and\nfor determining, for each broadcast feature and each target value,\na probability weight representing a probability that the new sample will have the target value of the record if the new sample feature appears in the record.\n21. The means for classifying natural language data of claim 20, further comprising:\na metric means responsive to each probability weight corresponding to a feature of the new sample for determining the training record most probably matching the new record.\n22. The means for classifying natural language data of claim 20, wherein the scanning means further comprises:\nmeans for performing logical AND operations on each possible combination of the match indications in each boolean comparison table, each AND operation representing a conjunctive feature, and determining, for each conjunctive feature and each target value of the training records,\nthe number of matches between each conjunctive feature and the features of the training record for each target value, and\nthe number of matches between the conjunctive feature and the features of the training record over all target values, and\nfor determining, for each broadcast feature and each target value,\na probability weight representing the probability that the new sample will have the target value of the record if the conjunctive feature appears in the record.\n23. The means for classifying natural language data of claim 20, wherein the probability weights determined over each target value are per target probability weights and the scanning means further comprises:\nmeans for determining, for each feature, the square of each per target probability weight of the feature for all target values, and\ndetermining the cross target probability weight for each feature by summing the squares of the per target probability weights of the feature over all target values.\n24. In a data parallel system, means for constructing probability weight tables for storing probability weights of a plurality of features of each of a plurality of training records for use in classifying natural language data, wherein each training record includes a plurality of predictor data fields, each predictor data field containing a feature wherein each feature is a natural language term, and a target data field containing a target value representing a classification of a record, wherein a probability weight is a probability that a new sample of a record containing natural language data will have the target value of a training record if a feature of the training record appears in the new sample of the new sample, comprising:\na plurality of processors for storing a corresponding plurality of record feature structures,\neach record feature structure corresponding to said training record and including\na plurality of record feature structure entries,\neach record feature structure entry containing a feature or a possible conjunction of features of the corresponding training record, and wherein\nthe record feature structure entries of each record feature structure are sorted according to the values of the keys formed by a concatenation of the values of the features of each entry and the target value of the corresponding training record,\na plurality of processors for storing a corresponding plurality of probability weight tables,\neach probability weight table corresponding to said training record and containing an entry for each possible conjunction of features of the training record,\na single feature of said training record being represented in the corresponding probability weight table as a conjunction of the single feature with itself,\nmeans for selecting sets of record feature structures,\nwherein each set of record feature structures have a common value for a feature portion of their keys, and\nselecting, within each set of register feature structures, a plurality of subsets of record feature structures, wherein the record feature structures of each subset of record feature structures have a common target field value, and\nwherein each subset of record feature structures corresponds to a feature of the record feature structure,\nmeans for determining, for each set of record feature structures,\nthe number of record feature structures in the set of record feature structures, and\nfor each subset of record feature structures in the set of register feature structures, the number of record feature structures in the subset, and\nfor each subset in the set of record feature structures,\ndividing the number of record feature structures in the subset by the number of record feature structures in the set of record feature structures to determine a conjunctive probability weight of the corresponding feature of the record feature structure, and\nwriting the conjunctive probability weight for each feature of each record feature structure into a corresponding entry of the probability weight table of the corresponding training record.\n25. In a data parallel system including a means for storing a plurality of weight tables for storing probability weights of features of each of a plurality of training records, wherein a probability weight is a probability that a new sample of a record containing natural language data will have the target value of a training record if a feature of the training record appears in the new sample of the new sample, means for classifying new records containing natural language data, comprising:\na plurality of processors for storing a corresponding plurality of training records,\neach training record including\na plurality of predictor data fields, each predictor data field containing a feature, wherein each feature is a natural language term, and\na target data field containing a target value representing a classification of said training record,\na plurality of processors for storing a corresponding plurality of probability weight tables,\neach probability weight table corresponding to said training record and containing an entry for each possible conjunction of features of the training record,\na single feature of said training record being represented in the corresponding probability weight table as a conjunction of the single feature with itself,\na control means for storing a new record containing features comprised of natural language data, and for broadcasting the features of the new sample to the processors storing the training records,\neach processor being responsive to the broadcast features of the new sample to construct, in a process associated with each training data record, a boolean comparison table for storing indications of matches between the broadcast new sample features and the features of the corresponding training record,\nscanning means for performing logical AND operations on each combination of the match indications in each boolean comparison table to find conjunctive feature matches, wherein each AND operation represents a conjunctive feature, and,\nfor each conjunctive feature match found in a boolean comparison table, using the values of the conjunctive features resulting in the match as indices into the probability weight table of the corresponding training record and reading from the probability weight table a conjunctive probability weight of the conjunctive feature.\n26. The means for classifying natural language data of claim 25, further comprising:\na metric means responsive to each probability weight corresponding to a feature of the new sample for determining the training record most probably matching the new record.\n27. In a data parallel system including a plurality of processing elements, each processing element including a memory for storing data and an associated processor to operate on the memory for performing operations on the data residing in the memory, and control means for issuing instructions for directing operations of the system, each processor being responsive to the instructions for performing the operations in parallel on the data stored in the associated memory, a method for classifying natural language data, comprising steps of:\nstoring a plurality of training records in a corresponding plurality of processor elements,\neach training record residing in a process in the memory of the processor element and including\na plurality of predictor data fields, each predictor data field containing a feature, wherein each feature is a natural language term, and\na target data field containing a target value representing a classification of training record, and\nstoring in a probability weight memory, and for each feature, a probability weight value representing a probability that a new record will have the target value contained in the target data field if a feature contained in a corresponding predictor data field occurs in the new record,\nquerying the training records with each feature extracted from a new record, by\nstoring the new record in the control means and, by operation of the control means, extracting the features from a new record and transmitting the features to the processors of the processing elements, and\nin the processors of the processing elements,\nreading the features from the training records stored in each associated training record, and\nresponsive to each match between a feature extracted from the new record and a feature stored in a training record, reading the probability weight corresponding to the feature, and\naccumulating, in the processing element for each corresponding training record and according to a selected metric, a comparison score representing the probability that said training record matches the new record and providing an output indicating said training record most probably matching the new record, and\nselecting the target field value of a training record as a target value of the new record.\n28. A method for implementing in a data parallel system which includes a plurality of processing elements, each processing element including a memory for storing data and an associated processor to operate on the memory for performing operations on the data residing in the memory, a global combining means for performing operations on outputs of the processing elements, and control means for issuing instructions for directing operations of the system, each processor being responsive to the instructions for performing the operations in parallel on the data stored in the associated memory, said method for generating training records for use in classifying natural language data, comprising steps of:\nstoring in a first plurality of processing elements a corresponding plurality of historic records,\neach historic record residing in a process in the memory of a corresponding processor element and including\na target data field containing a target value representing a classification of a historic record, and\na plurality of predictor data fields, each predictor data field containing a natural language term, and\nstoring in a second plurality of processing elements a corresponding plurality of training records,\neach training record residing in said process in the memory of a corresponding processor element and including\na plurality of predictor data fields, each predictor data field containing said feature, wherein said each feature is a natural language term, and\na target data field containing a target value representing a classification of a training record, and\nby operation of the control means and the processing elements, reading the natural language terms from each of the historic records and identifying each feature appearing in the historic records,\ngenerating a probability weight for said each feature, including\nby operation of the control means and the processing elements,\nselecting in turn said each feature identified from the historic records, and\nfor said each feature, selecting in turn each historic record target value for which the feature appears, and,\nby operation of the global combining means,\ndetermining, for each historic record target value and for said each feature a probability weight value representing a probability that a record will have the target value contained in the target data field if said feature contained in a corresponding predictor data field occurs in the record, and\nstoring in the predictor data fields of each training record the features identified from the corresponding historic record, and\nin the target data fields of the corresponding training record the target value from the historic record, and\nstoring in a probability weight memory, for said each feature,\na probability weight value generated for each feature and target value.\n29. A method for implementing in a data parallel system which includes a plurality of processing elements, each processing element including a memory for storing data and an associated processor to operate on the memory for performing operations on the data residing in the memory, a global combining means for performing operations on outputs of the processing elements, and control means for issuing instructions for directing operations of the data parallel system, each processor of each processing element being responsive to the instructions for performing the operations in parallel on the data stored in the memory of the processing element, said method for classifying natural language data, comprising the steps of:\nstoring in the processing elements a plurality of training records,\neach training record including\na plurality of predictor data fields, each predictor data field containing a feature, wherein each feature is a natural language term, and\na target data field containing a target value representing a classification of a training record,\nstoring a new record containing features comprised of the natural language data in the control means and, by operation of the control means, broadcasting the features of the new record to the processing elements storing the training records,\nin each processing element, and responsive to the broadcast features of the new record, constructing in a process associated with each training data record a boolean comparison table for storing indications of matches between the broadcast new record features and the features of the corresponding training record,\nin the processing elements, scanning each of the boolean comparison tables for indications of matches and determining, for each broadcast feature and each target value of the training records,\na number of indications of matches between the broadcast feature and the features of the training record for each target value, and\na number of indications of matches between the broadcast feature and the features of the training record over all target values, and\ndetermining, for each broadcast feature and each target value,\na probability weight representing a probability that the new record will have the target value of a training record if a new sample feature appears in the training record.\n30. The method for classifying natural language data of claim 29, further comprising the steps of:\naccumulating according to a selected metric each probability weight corresponding to a feature of the new sample, and\nselecting a training record most probably matching the new record.\n31. The method for classifying natural language data of claim 29, wherein the scanning steps further comprise the steps of:\nscanning the boolean comparison tables with a logical AND operations on each possible combination of the match indications in each boolean comparison table, each AND operation representing a conjunctive feature, and determining, for each conjunctive feature and each target value of the training records,\na number of indications of matches between each conjunctive feature and the features of the training record for each target value, and\na number of indications of matches between the conjunctive feature and the features of the training record over all target values, and\ndetermining, for each broadcast feature and each target value,\na probability weight representing a probability that the new sample will have the target value of a training record if the conjunctive feature appears in the training record.\n32. The method for classifying natural language data of claim 29, wherein the probability weights determined over each target value are per target probability weights and the steps for determining probability weights further comprise the steps of:\ndetermining, for said feature, a square of each per target probability weight of the feature for all target values, and\ndetermining the cross target probability weight for said feature by summing the squares of the per target probability weights of the feature over all target values.\n33. In a data parallel system which includes a plurality of processing elements, each processing element including a memory for storing data and an associated processor to operate on the memory for performing operations on the data residing in the memory, a global combining means for performing operations on outputs of the processing elements, and control means for issuing instructions for directing operations of the data parallel system, each processor being responsive to the instructions for performing the operations in parallel on the data, stored in the associated memory, said method for constructing probability weight tables for storing probability weights of a plurality of features of each of a plurality of training records for use in classifying natural language data, wherein each training record includes a plurality of predictor data fields, each predictor data field containing a feature wherein each feature is a natural language term, and a target data field containing a target value representing a classification of a record, wherein a probability weight is a probability that a new sample of a record containing natural language data will have the target value of a training record if a feature of the training record appears in the new sample of the new sample, comprising the steps of:\nstoring in a first plurality of processors a corresponding plurality of record feature structures,\neach record feature structure corresponding to said training record and including\na plurality of record feature structure entries,\neach record feature structure entry containing a feature or a possible conjunction of features of the corresponding training record, and wherein\nthe record feature structure entries of each record feature structure are sorted according to the values of the keys formed by the concatenation of the values of the features of each entry and the target value of the corresponding training record,\nstoring in a second plurality of processors a corresponding plurality of probability weight tables,\neach probability weight table corresponding to said training record and containing an entry for each possible conjunction of features of the training record,\na single feature of said training record being represented in the corresponding probability weight table as a conjunction of the single feature,\nin the processors, selecting sets of record feature structures,\nwherein each set of record feature structures have a common value for a feature portion of their keys, and\nwithin each set of register feature structures, a plurality of subsets of record feature structures, wherein a record feature structure of each subset of record feature structures have a common target field value, and\nwherein each subset of record feature structures corresponds to a feature of the record feature structure,\nin the processors, determining, for each set of record feature structures,\na number of record feature structures in the set of record feature structures, and\nfor each subset of record feature structures in the set of register feature structures, the number of record feature structures in the subset, and\nfor each subset in the set of record feature structures,\ndividing the number of record feature structures in the subset by the number of record feature structures in the set of record feature structures to determine a conjunctive probability weight of the corresponding feature of the record feature structure, and\nwriting the conjunctive probability weight for each feature of each record feature structure into a corresponding entry of the probability weight table of the corresponding training record.\n34. A method for implementing in a data parallel system which includes a plurality of processing elements, each processing element including a memory for storing data and an associated processor to operate on the memory for performing operations on the data residing in the memory, a global combining means for performing operations on outputs of the processing elements, and control means for issuing instructions for directing operations of the system, each processor of each processing element being responsive to the instructions for performing the operations in parallel on the data stored in the memory of the processing element, said method for classifying new records containing natural language data, comprising the steps of:\nstoring in a plurality of processors a corresponding plurality of training records,\neach training record including\na plurality of predictor data fields, each predictor data field containing a feature, wherein said feature is a natural language term, and\na target data field containing a target value representing a classification of a training record,\nstoring in a plurality of processors a corresponding plurality of probability weight tables,\neach probability weight table corresponding to said training record and containing a probability weight entry for each possible conjunction of features of the training record, wherein\na single feature of said training record is represented in the corresponding probability weight table as a conjunction of a single feature, and wherein\na probability weight is a probability that a new sample record containing natural language data will have the target value of said training record if said feature of the training record appears in the new record,\nstoring a new record containing features comprised of natural language data and broadcasting the features of the new sample to the processors storing the training records,\nin each processor and responsive to the broadcast features of the new sample, constructing in a process associated with each training data record a boolean comparison table for storing indications of matches between the broadcast new sample features and the features of the corresponding training record,\nin the processors, performing logical AND scanning operation on each combination of the indications of matches in each boolean comparison table to find conjunctive feature matches, wherein each AND operation represents a conjunctive feature, and,\nfor each indication of said conjunctive feature match found in a boolean comparison table, using the values of the conjunctive features resulting in the indication of said conjunctive feature match as indices into the probability weight table of the corresponding training record and reading from the probability weight table the conjunctive probability weight of the conjunctive feature.\n35. The method for classifying natural language data of claim 34, further comprising the steps of:\naccumulating the probability weights corresponding to a feature of the new sample according to a selected metric, and\ndetermining the training record most probably matching the new record.\n36. A system for classifying natural language data, comprising:\nmeans for storing a new record including a plurality of predictor data fields containing the natural language data expressed in natural language values,\nmeans for storing a plurality of training records,\neach training record including\na plurality of predictor data fields, each predictor data field containing a feature, wherein\nsaid feature is a natural language term,\nsaid feature is a member of one of a plurality of category subsets of features, and\nsaid feature appearing in identical form in a multiplicity of the category subsets comprises a corresponding multiplicity of separate and distinct features, and\na target data field containing a target value representing a classification of said training record, and\nprobability weight means for storing, for said feature, a probability weight value representing a probability that a new record will have the target value contained in the target data field if said feature contained in a corresponding predictor data field occurs in the new record,\nquery means for extracting features from the new record and querying the training records with said feature extracted from the new record,\nthe query means being responsive to a match between said feature extracted from the new record and said feature stored in a training record for providing the probability weight corresponding to the feature, and\nmetric means for receiving the probability weights from the query means and accumulating for said training record a comparison score representing the probability that said training record matches the new record, and\nproviding an output indicating a target field value of said training record as a target value of the new record.\n37. A system for comparing a new data record to training data records, comprising:\nmeans for storing a new record including a plurality of data fields containing new record data values,\nmeans for storing a plurality of training records,\neach training record including a plurality of data fields containing training record data values,\nprobability weight means for storing a probability weight for each training record data value,\neach probability weight value representing a probability that a new record will have a match with said training record if a data value in a training record data field occurs in a new record data field, and\ncomparison means for\nquerying the training records with each data value of the new record,\naccumulating for said training record a comparison score of the probability weights for each match between a new record data value and a training record data value, and\nproviding an output indicating a comparison score.",
    "status": "Expired - Lifetime",
    "citations_own": [
        "US4814973A",
        "US4823306A",
        "US4942526A"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US5388259A",
        "US5418948A",
        "US5418951A",
        "US5424945A",
        "US5454063A",
        "US5513313A",
        "US5559925A",
        "US5634134A",
        "US5642502A",
        "US5659731A",
        "US5671333A",
        "US5689696A",
        "US5696981A",
        "US5704011A",
        "US5717914A",
        "WO1998019253A1",
        "AU691447B1",
        "US5875445A",
        "WO1999013408A2",
        "US5911132A",
        "US5913185A",
        "US5918240A",
        "US5930803A",
        "US5950185A",
        "US5960435A",
        "US5963742A",
        "US5978785A",
        "US5999918A",
        "US6023670A",
        "US6026399A",
        "US6032678A",
        "US6064952A",
        "US6112172A",
        "US6111578A",
        "US6115709A",
        "US6144934A",
        "US6154783A",
        "US6167369A",
        "US6182058B1",
        "US6219707B1",
        "US6223175B1",
        "US6260007B1",
        "US6272467B1",
        "US6278464B1",
        "US6301579B1",
        "US6317752B1",
        "US6332195B1",
        "US6343656B1",
        "US6366910B1",
        "US6373483B1",
        "US6377949B1",
        "US6405197B2",
        "US20020073006A1",
        "US6408277B1",
        "US20020082778A1",
        "US6415295B1",
        "US20020129017A1",
        "US20020138332A1",
        "US6460049B1",
        "US6480194B1",
        "US6493697B1",
        "US20030004716A1",
        "US6553365B1",
        "US20030097312A1",
        "US6640229B1",
        "US6640228B1",
        "US6668251B1",
        "US6704698B1",
        "US6741981B2",
        "US20040111410A1",
        "US20040111386A1",
        "US6757692B1",
        "US6772332B1",
        "US6823333B2",
        "US20040261016A1",
        "US20050053057A1",
        "US20050080613A1",
        "US20050240424A1",
        "US20060136259A1",
        "US20060136466A1",
        "US20060136467A1",
        "US20060136417A1",
        "US20060136143A1",
        "US20060184489A1",
        "US7099855B1",
        "US7107266B1",
        "US20070078873A1",
        "US7203725B1",
        "US20070156749A1",
        "US20070192061A1",
        "US20070282827A1",
        "US20070299855A1",
        "US20080010240A1",
        "US20080077402A1",
        "US20080097982A1",
        "US20080097992A1",
        "US7376641B2",
        "US7389230B1",
        "US20080154875A1",
        "US20080320411A1",
        "US20090076739A1",
        "US20090150391A1",
        "US20090182741A1",
        "US20090271363A1",
        "US20090271700A1",
        "US7644057B2",
        "US20100005056A1",
        "US20100010806A1",
        "WO2010019209A1",
        "US20100116780A1",
        "US7756810B2",
        "US20100185447A1",
        "US20100228733A1",
        "US20100228629A1",
        "US20100281043A1",
        "US20100325082A1",
        "US20100332443A1",
        "US20110029393A1",
        "WO2011150097A2",
        "US20120130960A1",
        "US20120259618A1",
        "US8290768B1",
        "US8335767B2",
        "US8341178B2",
        "US8380875B1",
        "US8386378B2",
        "US20130138643A1",
        "US8478732B1",
        "US8577094B2",
        "US8782087B2",
        "US20140278547A1",
        "US8903801B2",
        "US9015171B2",
        "US9189505B2",
        "US20160005421A1",
        "US9268780B2",
        "US20160171376A1",
        "US9411859B2",
        "US20160292584A1",
        "US9699129B1",
        "US9710431B2",
        "US9805373B1",
        "US9846739B2",
        "US10055501B2",
        "US20180246876A1",
        "US10127130B2",
        "US10275424B2",
        "US10621064B2",
        "US10796232B2",
        "US10802687B2",
        "US11087747B2",
        "US11100374B2",
        "US11138477B2",
        "US11327932B2",
        "US20220191219A1",
        "US11386058B2",
        "WO2022212618A1",
        "US11468322B2"
    ],
    "citedby_ftf": []
}