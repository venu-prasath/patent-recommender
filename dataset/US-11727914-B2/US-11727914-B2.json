{
    "patent_id": "US-11727914-B2",
    "title": "Intent recognition and emotional text-to-speech learning ",
    "assignee": "Microsoft Technology Licensing, Llc",
    "publication_date": "2023-08-15",
    "patent_link": "https://patents.google.com/patent/US11727914B2/en",
    "inventors": [
        "Pei Zhao",
        "Kaisheng Yao",
        "Max Leung",
        "Bo Yan",
        "Jian Luan",
        "Yu Shi",
        "Malone MA",
        "Mei-Yuh Hwang"
    ],
    "classifications": [
        "G10L13/027",
        "G06F3/167",
        "G06F40/30",
        "G06N3/044",
        "G06N7/01",
        "G10L13/08",
        "G10L15/063",
        "G10L15/1807",
        "G10L15/22",
        "G10L15/26",
        "G10L25/03",
        "G10L25/63",
        "G10L2015/223",
        "H04M2250/74"
    ],
    "abstract": "An example intent-recognition system comprises a processor and memory storing instructions. The instructions cause the processor to receive speech input comprising spoken words. The instructions cause the processor to generate text results based on the speech input and generate acoustic feature annotations based on the speech input. The instructions also cause the processor to apply an intent model to the text result and the acoustic feature annotations to recognize an intent based on the speech input. An example system for adapting an emotional text-to-speech model comprises a processor and memory. The memory stores instructions that cause the processor to receive training examples comprising speech input and receive labelling data comprising emotion information associated with the speech input. The instructions also cause the processor to extract audio signal vectors from the training examples and generate an emotion-adapted voice font model based on the audio signal vectors and the labelling data.",
    "claims": "\n1. A system for adapting an emotional text-to-speech model, the system comprising:\nat least one processor; and\nmemory, operatively connected to the at least one processor and storing instructions that, when executed by the at least processor, cause the at least one processor to:\nreceive training examples comprising speech input;\nreceive labelling data comprising emotion information associated with the speech input;\nextract audio signal vectors from the training examples;\nadapt a voice font model based on the audio signal vectors and the labelling data to generate an emotion-adapted voice font model;\ngenerate first prosody annotations from the speech input;\ngenerate second prosody annotations from the labelling data;\ndetermine differences between the first prosody annotations and the second prosody annotations;\ngenerate a prosody model based on the determined differences between the first prosody annotations and the second prosody annotations;\ngenerate a prosody-adjusted pronunciation sequence for text input using the prosody model; and\nrender the text input to synthesized speech using the emotion-adapted voice font model and the prosody-adjusted pronunciation sequence.\n2. The system of claim 1, wherein the memory further stores instructions that, when executed by the at least one processor, cause the at least one processor to apply an audio signal vector conversion model to the extracted audio signal vectors to generate converted audio signal vectors, and wherein the voice font model is adapted based on the converted audio signal vectors.\n3. The system of claim 1, wherein the labelling data is generated by an intent recognition system processing the speech input of the training examples.\n4. The system of claim 3, wherein the memory further stores instructions that, when executed by the at least one processor, cause the at least one processor to generate the labelling data for the speech input.\n5. The system of claim 1, wherein the labelling data further comprises audio feature annotations for the speech input.\n6. The system of claim 1, wherein the emotion-adapted voice font model is usable by an emotional text-to-speech system to generate synthesized speech that conveys a particular emotion.\n7. A method for generating an emotional text-to-speech model, the method comprising:\nreceiving training examples comprising speech input;\nreceiving labelling data comprising emotion information associated with the speech input;\nextracting audio signal vectors from the training examples;\napplying an audio signal vector conversion model to the extracted audio signal vectors to generate converted audio signal vectors;\nadapting a voice font model based on the converted audio signal vectors and the labelling data to generate an emotion-adapted voice font model;\ngenerating first prosody annotations from the speech input;\ngenerating second prosody annotations from the labelling data;\ndetermining differences between the first prosody annotations and the second prosody annotations;\ngenerating a prosody model based on the determined differences between the first prosody annotations and the second prosody annotations;\ngenerating a prosody-adjusted pronunciation sequence for text input using the prosody model; and\nrendering the text input to synthesized speech using the emotion-adapted voice font model and the prosody-adjusted pronunciation sequence.\n8. The method of claim 7, wherein the labelling data is generated by an intent recognition system processing the speech input of the training examples.\n9. The method of claim 7, wherein the labelling data further comprises audio feature annotations for the speech input.\n10. The method of claim 7, wherein the emotion-adapted voice font model is usable by an emotional text-to-speech system to generate synthesized speech that conveys a particular emotion.\n11. Computer storage media storing instructions that, when executed by at least one processor, cause the at least one processor to:\nreceive training examples comprising speech input;\nreceive labelling data comprising emotion information associated with the speech input;\nextract audio signal vectors from the training examples;\nadapt a voice font model based on the audio signal vectors and the labelling data to generate an emotion-adapted voice font model;\ngenerate first prosody annotations from the speech input;\ngenerate second prosody annotations from the labelling data;\ndetermine differences between the first prosody annotations and the second prosody annotations;\ngenerate a prosody model based on the determined differences between the first prosody annotations and the second prosody annotations;\ngenerate a prosody-adjusted pronunciation sequence for text input using the prosody model; and\nrender the text input to synthesized speech using the emotion-adapted voice font model and the prosody-adjusted pronunciation sequence.\n12. The computer storage media of claim 11, further comprising instructions that, when executed by the at least one processor, cause the at least one processor to apply an audio signal vector conversion model to the extracted audio signal vectors to generate converted audio signal vectors, and wherein the voice font model is adapted based on the converted audio signal vectors.\n13. The computer storage media of claim 11, wherein the labelling data is generated by an intent recognition system processing the speech input of the training examples.\n14. The computer storage media of claim 11, further comprising instructions that, when executed by the at least one processor, cause the at least one processor to generate the labelling data for the speech input.\n15. The computer storage media of claim 11, wherein the labelling data further comprises audio feature annotations for the speech input.\n16. The computer storage media of claim 11, wherein the emotion-adapted voice font model is usable by an emotional text-to-speech system to generate synthesized speech that conveys a particular emotion."
}