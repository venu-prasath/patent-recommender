{
    "patent_link": "https://patents.google.com/patent/US20080317331A1/en",
    "patent_id": "US20080317331A1",
    "title": "Recognizing Hand Poses and/or Object Classes",
    "abstract": "There is a need to provide simple, accurate, fast and computationally inexpensive methods of object and hand pose recognition for many applications. For example, to enable a user to make use of his or her hands to drive an application either displayed on a tablet screen or projected onto a table top. There is also a need to be able to discriminate accurately between events when a user's hand or digit touches such a display from events when a user's hand or digit hovers just above that display. A random decision forest is trained to enable recognition of hand poses and objects and optionally also whether those hand poses are touching or not touching a display surface. The random decision forest uses image features such as appearance, shape and optionally stereo image features. In some cases, the training process is cost aware. The resulting recognition system is operable in real-time.",
    "inventors": [
        "John Winn",
        "Antonio Criminisi",
        "Ankur Agarwal",
        "Thomas Deselaers"
    ],
    "assignee": "Microsoft Technology Licensing LLC",
    "classifications": [
        "G06F3/017",
        "G06F18/24323",
        "G06F3/0425",
        "G06V10/764",
        "G06V40/28"
    ],
    "claims": "\n1. A method comprising:\nreceiving at least one image of an item to be classified as one of a plurality of specified classes;\naccessing a plurality of decision trees which have been formed in a training process using information about classification accuracy and information about computational cost;\nclassifying the image into one of the classes at least by applying the plurality of decision trees to at least part of the image; and\nstoring the classified image.\n2. A method as claimed in claim 1 which further comprises segmenting the at least one image to identify a foreground region and applying the plurality of decision trees to that foreground region.\n3. A method as claimed in claim 1 wherein the process of receiving at least one image comprises receiving a stereo image pair.\n4. A method as claimed in claim 1 wherein the process of receiving at least one image comprises receiving a depth map.\n5. A method as claimed in claim 1 which further comprises receiving a touch map and wherein the process of classifying the image comprises using information from the touch map.\n6. A method as claimed in claim 1 wherein the plurality of specified classes comprises classes in which items are touching a display surface and classes in which items are not touching a display surface.\n7. A method as claimed in claim 1 wherein the plurality of specified classes comprise a plurality of classes of hand poses and a plurality of classes of objects.\n8. A method as claimed in claim 1 which further comprises inputting information about the classified image into a user interface.\n9. A method as claimed in claim 1 wherein the step of accessing the decision trees comprises accessing decision trees comprising tests using any of appearance and shape image features.\n10. A method as claimed in claim 3 wherein the step of accessing the decision trees comprises accessing decision trees comprising tests using appearance image features, shape image features and stereo image features.\n11. A method as claimed in claim 4 wherein the step of accessing the decision trees comprises accessing decision trees comprising tests using depth map features.\n12. A method as claimed in claim 1 wherein the step of classifying the image comprises, for each decision tree, computing a histogram using results of applying that decision tree to at least part of the image.\n13. A method as claimed in claim 12 wherein the step of classifying the image further comprises, concatenating the histograms and inputting the concatenated histogram to a multi-class classifier.\n14. A method comprising:\nreceiving at least one image of an item to be classified as one of a plurality of specified classes, those classes comprising hand pose classes and object classes;\naccessing a plurality of decision trees which have been formed in a training process using information at least about classification accuracy;\nclassifying the image into one of the classes using a unified recognition process at least by applying the plurality of decision trees to at least part of the image; and\nstoring the classified image.\n15. A method as claimed in claim 14 wherein the process of accessing the plurality of decision trees comprises accessing decision trees which have been formed in a training process using information about classification accuracy and information about computational cost.\n16. A method as claimed in claim 14 wherein the process of receiving at least one image comprises receiving a stereo image pair.\n17. A method as claimed in claim 14 which further comprises inputting information about the classified image into a user interface in order to control a display.\n18. A method as claimed in claim 17 wherein the process of receiving the image comprises receiving an image comprising one or more items against a background of the display.\n19. An apparatus comprising\nan input arranged to receive at least one image of an item to be classified as one of a plurality of specified classes;\na memory arranged to store a plurality of decision trees which have been formed in a training process using information about classification accuracy and information about computational cost; and\na processor arranged to classifying the image into one of the classes at least by applying the plurality of decision trees to at least part of the image, and to store the classified image.\n20. An apparatus as claimed in claim 19 which further comprises a user interface arranged to provide a display and wherein the processor is arranged to provide information about the classified image to the user interface in order to control the display.",
    "status": "Active",
    "citations_own": [
        "US5732227A",
        "US5937079A",
        "US6128003A",
        "US6256033B1",
        "US20020118880A1",
        "US20030113018A1",
        "US6674877B1",
        "US6788809B1",
        "US20040193413A1",
        "US20050201591A1",
        "US20060136846A1",
        "US7068842B2",
        "US20070003147A1",
        "US20080168403A1",
        "US7680314B2"
    ],
    "citations_ftf": [
        "GB2449412B",
        "US8290882B2"
    ],
    "citedby_own": [
        "US20070162164A1",
        "US20070255454A1",
        "US20090074252A1",
        "US20090118863A1",
        "US20090175540A1",
        "US20100215271A1",
        "US20100239147A1",
        "US20110044506A1",
        "KR20110020718A",
        "WO2011045786A2",
        "US20110188715A1",
        "WO2011104709A2",
        "CN102436590A",
        "US20130166481A1",
        "US20130300662A1",
        "US8619049B2",
        "US8891884B2",
        "US8942917B2",
        "US20150030233A1",
        "US20150248765A1",
        "US9153031B2",
        "US20150379376A1",
        "WO2016025713A1",
        "US9563955B1",
        "US20170115737A1",
        "US9880619B2",
        "US10048765B2",
        "US10853407B2",
        "US11215711B2",
        "US11360550B2",
        "US11710309B2"
    ],
    "citedby_ftf": [
        "US8280167B2",
        "US8503720B2",
        "US8811666B2",
        "US9262015B2",
        "CA2722460A1",
        "EP2686254B1",
        "WO2012135373A2",
        "JP2015525381A",
        "US10009579B2",
        "US9367733B2",
        "KR20140095601A",
        "US9639747B2",
        "US9558455B2",
        "US10664090B2",
        "US10168838B2",
        "US10318008B2",
        "CN108960281B",
        "CN109829471B",
        "CN109948680B"
    ]
}