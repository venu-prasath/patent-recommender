{
    "patent_id": "US-11687832-B1",
    "title": "Training a model using parameter server shards ",
    "assignee": "Google Llc",
    "publication_date": "2023-06-27",
    "patent_link": "https://patents.google.com/patent/US11687832B1/en",
    "inventors": [
        "Gregory S. Corrado",
        "Kai Chen",
        "Jeffrey A. Dean",
        "Samy Bengio",
        "Rajat Monga",
        "Matthieu Devin"
    ],
    "classifications": [
        "G06N20/00",
        "G06N3/063",
        "G06N3/08",
        "G06N7/08",
        "G06F18/214",
        "G06F18/2411",
        "G06N5/025",
        "G06N7/01"
    ],
    "abstract": "Methods, systems, and apparatus, including computer programs encoded on computer storage media, for training a model using parameter server shards. One of the methods includes receiving, at a parameter server shard configured to maintain values of a disjoint partition of the parameters of the model, a succession of respective requests for parameter values from each of a plurality of replicas of the model; in response to each request, downloading a current value of each requested parameter to the replica from which the request was received; receiving a succession of uploads, each upload including respective delta values for each of the parameters in the partition maintained by the shard; and updating values of the parameters in the partition maintained by the parameter server shard repeatedly based on the uploads of delta values to generate current parameter values.",
    "claims": "\n1. A system for training a machine learning model having parameters by determining a respective trained parameter value for each of the parameters of the machine learning model, the system comprising:\none or more server computing units; and\na parameter server executing on the one or more server computing units, wherein the parameter server is configured to maintain and asynchronously update values of each of the parameters of the machine learning model based on delta values received from a plurality of model replicas, wherein each model replica executes on a respective replica computing unit, wherein each of the plurality of model replicas is configured to maintain an identical instance of the machine learning model with possibly different parameter values for the parameters of the machine learning model and to operate independently of each other model replica, and wherein each model replica is further configured to asynchronously request parameter values from the parameter server, determine delta values for the parameters based on stochastic gradient descent, and provide the delta values to the parameter server.\n2. The system of claim 1, wherein the system comprises the plurality of model replicas and wherein each model replica is configured to perform repeatedly the following operations:\nreceive, from the parameter server, values of one or more of the plurality of parameters;\ncompute respective delta values for each of the plurality of parameters by performing one or more iterations of a machine learning training process that is based on stochastic gradient descent; and\nprovide, for each of the plurality of parameters, the delta value for the parameter to the parameter server.\n3. The system of claim 2, wherein:\nperforming one or more iterations of the machine learning training process comprises:\nobtaining a respective batch of training data; and\ncomputing the respective delta values for each of the plurality of parameters by computing a gradient of an objective function for training the machine learning model based on the received values and the batch of training data.\n4. The system of claim 3, wherein each model replica obtains a different sequence of training data.\n5. The system of claim 3, wherein each model replica obtains different training data.\n6. The system of claim 1, wherein the parameter server comprises a plurality of shards that are each configured to maintain and update values of a respective partition of the model parameters, and wherein each shard is configured to perform repeatedly the following operations asynchronously with respect to every other shard:\nreceive a succession of respective requests for parameter values from each of the plurality of replicas of the model;\nin response to each request, download a current value of each requested parameter to the replica from which the request was received;\nreceive, from each of the plurality of replicas, a succession of uploads, each upload including respective delta values for each of the parameters in the partition maintained by the shard; and\nupdate values of the parameters in the partition maintained by the parameter server shard repeatedly based on the uploads of delta values to generate current parameter values.\n7. The system of claim 6, wherein the updated value of a parameter (pu) satisfies:\n\np u =p c \u2212\u03b1\u00d7\u0394p r,\n8. The system of claim 7, wherein the learning rate is an adaptive learning rate that varies between parameters.\n9. The system of claim 7, wherein the learning rate is an adaptive learning rate that varies between iterations of the training process.\n10. One or more non-transitory computer-readable storage media storing instructions that when executed by one or more computers cause the one or more computers to implement a training system for training a machine learning model having parameters by determining a respective trained parameter value for each of the parameters of the machine learning model, the training system comprising:\na parameter server executing on the one or more server computing units, wherein the parameter server is configured to maintain and asynchronously update values of each of the parameters of the machine learning model based on delta values received from a plurality of model replicas, wherein each model replica executes on a respective replica computing unit, wherein each of the plurality of model replicas is configured to maintain an identical instance of the machine learning model with possibly different parameter values for the parameters of the machine learning model and to operate independently of each other model replica, and wherein each model replica is further configured to asynchronously request parameter values from the parameter server, determine delta values for the parameters based on stochastic gradient descent, and provide the delta values to the parameter server.\n11. The computer-readable storage media of claim 10, wherein the training system further comprises the model replicas and wherein each replica is configured to perform repeatedly the following operations:\nreceive, from the parameter server, values of one or more of the plurality of parameters;\ncompute respective delta values for each of the plurality of parameters by performing one or more iterations of a machine learning training process that is based on stochastic gradient descent; and\nprovide, for each of the plurality of parameters, the delta value for the parameter to the parameter server.\n12. The computer-readable storage media of claim 11, wherein:\nperforming one or more iterations of the machine learning training process comprises:\nobtaining a respective batch of training data; and\ncomputing the respective delta values for each of the plurality of parameters by computing a gradient of an objective function for training the machine learning model based on the received values and the batch of training data.\n13. The computer-readable storage media of claim 12, wherein each model replica obtains a different sequence of training data.\n14. The computer-readable storage media of claim 12, wherein each model replica obtains different training data.\n15. The computer-readable storage media of claim 10, wherein the parameter server comprises a plurality of shards that are each configured to maintain and update values of a respective partition of the parameters, and wherein each shard is configured to perform repeatedly the following operations asynchronously with respect to every other shard:\nreceive a succession of respective requests for parameter values from each of the plurality of replicas of the model;\nin response to each request, download a current value of each requested parameter to the replica from which the request was received;\nreceive, from each of the plurality of replicas, a succession of uploads, each upload including respective delta values for each of the parameters in the partition maintained by the shard; and\nupdate values of the parameters in the partition maintained by the parameter server shard repeatedly based on the uploads of delta values to generate current parameter values.\n16. The computer-readable storage media of claim 15, wherein the updated value of a parameter (pu) satisfies:\n\np u =p c \u2212\u03b1\u00d7\u0394p r,"
}