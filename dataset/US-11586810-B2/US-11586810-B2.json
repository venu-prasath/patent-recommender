{
    "patent_id": "US-11586810-B2",
    "title": "Generating responses in automated chatting ",
    "assignee": "Microsoft Technology Licensing, Llc",
    "publication_date": "2023-02-21",
    "patent_link": "https://patents.google.com/patent/US11586810B2/en",
    "inventors": [
        "Xianchao Wu",
        "Zhan Chen"
    ],
    "classifications": [
        "G06F40/20",
        "G06F40/30",
        "G06F40/35",
        "G06N20/10",
        "G06N20/20",
        "G06N3/006",
        "G06N3/042",
        "G06N3/044",
        "G06N3/045",
        "G06N3/0454",
        "G06N3/047",
        "G06N3/048",
        "G06N3/084",
        "G06N3/088",
        "G06N5/01",
        "G06N5/022",
        "G06N5/041",
        "H04L51/02",
        "G06N7/01"
    ],
    "abstract": "The present disclosure provides method and apparatus for generating responses in automated chatting. A message may be received in a session. An intention vector may be determined based at least on the message and the session through dynamic memory network (DMN), the intention vector indicating an attention point and an intention. A response may be generated based at least on the intention vector.",
    "claims": "\n1. A method for using a dynamic memory network (DMN) to generate responses in automated chatting, said method comprising:\nreceiving a message in a session;\ndetermining an intention vector based at least on the message and the session through a DMN, the intention vector indicating an attention point and an intention, wherein:\na fact memory vector and an emotion memory vector is generated and is used to determine the intention vector,\na recurrent convolutional neural network (RCNN) encodes both semantic and orthographic information from characters provided within the session,\nthe RCNN includes an embedding layer, the embedding layer converts the characters into a dense vector space to enable generation of at least the emotion memory vector,\nwhen the factor memory vector is generated, the fact memory vector is generated based on a candidate fact response that is based on one or more of the message, the session, a pure chat index, or a topic-topic knowledge graph, and\nwhen the emotion memory vector is generated, the emotion memory is generated based on a candidate emotion response that is based on one or more of the message, the session, the pure chat index, or an emotion-topic knowledge graph; and\n2. The method of claim 1, wherein determining the intention vector comprises:\nreasoning out the fact memory vector based at least on the message and the session;\nreasoning out the emotion memory vector based at least on the message and the session; and\ngenerating the intention vector based on the fact memory vector and the emotion memory vector.\n3. The method of claim 2, wherein reasoning out the fact memory vector comprises:\nreasoning out a first fact memory vector from fact vectors of the session; and\nreasoning out a second fact memory vector from one or more fact vectors of the candidate fact response.\n4. The method of claim 3, further comprising:\ndetermining the candidate fact response based on a combination of the message, the session, the pure chat index and the topic-topic knowledge graph,\nwherein the topic-topic knowledge graph comprises a plurality of topic phrases that are associated with each other.\n5. The method of claim 2, wherein reasoning out the emotion memory vector comprises:\nreasoning out a first emotion memory vector from emotion vectors of the session; and\nreasoning out a second emotion memory vector from one or more emotion vectors of the candidate emotion response.\n6. The method of claim 5, further comprising:\ndetermining the candidate emotion response based on a combination of the message, the session, the pure chat index, and the emotion-topic knowledge graph,\nwherein the emotion-topic knowledge graph comprises a plurality of topic phrases and a plurality of emotion phrases that are associated with each other.\n7. The method of claim 1, further comprising:\nestablishing a generative adversarial network (GAN) including a generator and a discriminator, the generator being based at least on the DMN, the discriminator being based on a deep semantic similarity model (DSSM).\n8. The method of claim 7, wherein:\nthe discriminator is trained by reference data and generated data, the generated data being generated by the generator based on samples of the reference data, and\nthe generator is trained through a gradient policy that is based on a reward provided by the discriminator.\n9. The method of claim 8, further comprising:\ndetermining a new reference data based on the response; and\nupdating the GAN based at least on the new reference data.\n10. The method of claim 5, wherein the emotion vectors are generated by a sentiment analysis classifier, a training dataset for the sentiment analysis classifier being obtained through an emotion lexicon, the emotion lexicon being established at least by performing Word2vec word extension and bilingual word alignment on seed emotional words.\n11. The method of claim 10, wherein the sentiment analysis classifier is a character-level recurrent convolutional neural network (RCNN).\n12. An apparatus for generating responses in automated chatting, where a recurrent convolutional neural network (RCNN) is configured to analyze characters provided within the automated chatting, said apparatus comprising:\na message receiving module, for receiving a message in a session;\nan intention vector determining module, for determining an intention vector based at least on the message and the session through dynamic memory network (DMN), the intention vector indicating an attention point and an intention;\nthe RCNN, which is configured to encode both semantic and orthographic information from characters provided within the session, wherein said information is used to generate an emotion memory vector, and wherein the RCNN includes an embedding layer that converts the characters into a dense vector space to enable generation of the emotion memory vector; and\na response generating module, for generating a response based at least on the intention vector and on the emotion memory vector.\n13. The apparatus of claim 12, further comprising:\na fact memory module, for reasoning out at least one fact memory vector based at least on the message and the session; and\nan emotion memory module, for reasoning out the emotion memory vector based at least on the message and the session,\nwherein the intention vector determining module is further for generating the intention vector based on the at least one fact memory vector and the emotion memory vector.\n14. The apparatus of claim 13, wherein reasoning out the at least one fact memory vector comprises:\nreasoning out a first fact memory vector from fact vectors of the session; and\nreasoning out a second fact memory vector from fact vectors of one or more candidate fact responses.\n15. The apparatus of claim 14, further comprising:\na fact response ranking model, for determining the one or more candidate fact responses based on at least one of the message, the session, a pure chat index and a topic-topic knowledge graph,\nwherein the topic-topic knowledge graph comprises a plurality of topic phrases that are associated with each other.\n16. The apparatus of claim 13, wherein reasoning out the emotion memory vector comprises:\nreasoning out a first emotion memory vector from emotion vectors of the session; and\nreasoning out a second emotion memory vector from emotion vectors of one or more candidate emotion responses.\n17. The apparatus of claim 16, further comprising:\nan emotion response ranking model, for determining the one or more candidate emotion responses based on at least one of the message, the session, a pure chat index and an emotion-topic knowledge graph,\nwherein the emotion-topic knowledge graph comprises a plurality of topic phrases and a plurality of emotion phrases that are associated with each other.\n18. The apparatus of claim 12, further comprising:\na generative adversarial network (GAN) establishing module, for establishing a GAN including a generator and a discriminator, the generator being based at least on the DMN, the discriminator being based on a deep semantic similarity model (DSSM).\n19. The apparatus of claim 18, wherein\nthe discriminator is trained by reference data and generated data, the generated data being generated by the generator based on samples of the reference data, and\nthe generator is trained through a gradient policy that is based on a reward provided by the discriminator.\n20. The apparatus of claim 18, wherein the RCNN further includes a convolutional layer, a recurrent layer, and an output layer."
}