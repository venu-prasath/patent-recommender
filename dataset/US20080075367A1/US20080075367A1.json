{
    "patent_link": "https://patents.google.com/patent/US20080075367A1/en",
    "patent_id": "US20080075367A1",
    "title": "Object Detection and Recognition System",
    "abstract": "During a training phase we learn parts of images which assist in the object detection and recognition task. A part is a densely represented area of an image of an object to which we assign a unique label. Parts contiguously cover an image of an object to give a part label map for that object. The parts do not necessarily correspond to semantic object parts. During the training phase a classifier is learnt which can be used to estimate belief distributions over parts for each image element of a test image. A conditional random field is used to force a global part labeling which is substantially layout-consistent and a part label map is inferred from this. By recognizing parts we enable object detection and recognition even for partially occluded objects, for multiple-objects of different classes in the same scene, for unstructured and structured objects and allowing for object deformation.",
    "inventors": [
        "John Winn",
        "Jamie Shotton"
    ],
    "assignee": "Zhigu Holdings Ltd",
    "classifications": [
        "G06V10/457",
        "G06V10/267"
    ],
    "claims": "\n1. A method of object detection and recognition comprising:\n(i) receiving an image for which it is required to carry out object detection and recognition; and\n(ii) generating a part label map for the received image, the part label map comprising, for each image element of the received image, a label indicating which of a plurality of parts that image element is assigned to, each part being a densely represented image area; wherein the step of generating the part label map comprises at least:\naccessing a pre-specified classifier arranged to estimate a belief distribution over parts for each image element of the received image; and using an inference algorithm to infer the part label map from a conditional random field by forcing a global part labeling which is substantially layout-consistent.\n2. A method as claimed in claim 1 wherein the inference algorithm comprises an annealed expansion move algorithm.\n3. A method as claimed in claim 1 wherein the inference algorithm comprises belief propagation.\n4. A method as claimed in claim 1 wherein the conditional random field comprises a hidden layer of part labels.\n5. A method as claimed in claim 1 which is suitable for detecting and recognizing images of partially occluded objects.\n6. A method as claimed in claim 1 which further comprises forming the classifier during a training phase using a plurality of training images together with a mask for each training image indicating which pixels in that image correspond to objects to be recognized and which correspond to background that it is not required to recognize.\n7. A method as claimed in claim 6 which further comprises, during the training phase, forming an initial part label map for a training image by dividing the image into a plurality of parts having a consistent pair-wise ordering such that the parts contiguously cover the image.\n8. A method as claimed in claim 7 wherein the step of forming an initial part label map also comprises ensuring that the parts meet constraints related to image elements that are non-immediate neighbors.\n9. A method as claimed in claim 7 which further comprises deforming the initial part label map for the training image during a learning process to form a deformed part labeling such that parts which assist in the object detection and recognition task are learnt.\n10. A method as claimed in claim 9 which further comprises using the deformed labeling to form a new initial part label map for each training image and repeating the learning process.\n11. A method of object detection and recognition comprising:\n(i) receiving an image for which it is required to carry out object detection and recognition that image being of partially occluded objects;\n(ii) accessing a pre-specified classifier arranged to estimate a belief distribution over parts for each image element of the received image;\n(iii) applying an inference process to a conditional random field model to force a global part labeling which is substantially layout-consistent and thus generating a part label map from the conditional random field model for the received image, the part label map comprising, for each image element of the received image, a label indicating which of a plurality of parts that image element is assigned to, each part being a densely represented image area.\n12. A method as claimed in claim 11 wherein the step of using a conditional random field model comprises using such a model having a hidden layer of part labels.\n13. A method as claimed in claim 11 wherein the step of using a conditional random field model comprises using a plurality of decision trees.\n14. An apparatus for object detection and recognition comprising:\n(i) an input arranged to receive an image for which it is required to carry out object detection and recognition;\n(ii) an input arranged to access a pre-specified classifier arranged to estimate a belief distribution over parts for each image element of the received image;\n(iii) a conditional random field model; and\n(iv) an inference mechanism arranged to carry out an inference process on the conditional random field model to force a global part labeling which is substantially layout-consistent and thereby generate a part label map for the received image, the part label map comprising, for each image element of the received image, a label indicating which of a plurality of parts that image element is assigned to, each part being a densely represented image area.\n15. An apparatus as claimed in claim 14 wherein the inference mechanism is arranged to carry out an annealed expansion move algorithm.\n16. An apparatus as claimed in claim 14 wherein the conditional random field model comprises a hidden layer of part labels.\n17. An apparatus as claimed in claim 14 wherein the classifier comprises a plurality of decision trees.\n18. An apparatus as claimed in claim 14 which further comprises a processor arranged to form the classifier during a training phase using a plurality of training images together with a mask for each training image indicating which pixels in that image correspond to objects to be recognized and which correspond to background that it is not required to recognize.\n19. An apparatus as claimed in claim 18 wherein the processor is further arranged to, during the training phase, form an initial part label map for a training image by dividing the image into a plurality of parts having a consistent pair-wise ordering such that the parts contiguously cover the image.\n20. An apparatus as claimed in claim 19 wherein the processor is further arranged to deform the initial part label map for the training image during a learning process to form a deformed part labeling such that parts which assist in the object detection and recognition task are learnt.",
    "status": "Active",
    "citations_own": [
        "US7027620B2",
        "US20060098871A1"
    ],
    "citations_ftf": [],
    "citedby_own": [
        "US20080304740A1",
        "US20100106671A1",
        "US20110016333A1",
        "US20110033122A1",
        "US20110052063A1",
        "US20110188715A1",
        "US20120063689A1",
        "US20120076417A1",
        "US20120134568A1",
        "US20120257831A1",
        "US8290882B2",
        "US20120269436A1",
        "CN103077384A",
        "US8483447B1",
        "US20130322739A1",
        "US8787659B2",
        "US20140306953A1",
        "US8879831B2",
        "US8891884B2",
        "US8942917B2",
        "CN104392228A",
        "US9008415B2",
        "US9070047B2",
        "US20150310624A1",
        "US20160086334A1",
        "US20160180192A1",
        "US9476705B2",
        "US9679226B1",
        "US9696782B2",
        "US9748765B2",
        "US9793570B2",
        "WO2018009552A1",
        "US9939862B2",
        "US20180189598A1",
        "US10061366B2",
        "WO2018217828A1",
        "US10158148B2",
        "US20190094875A1",
        "US10417816B2",
        "US10453150B2",
        "US10503990B2",
        "CN110705344A",
        "US10703268B2",
        "US10733460B2",
        "US10769456B2",
        "US10891501B2",
        "WO2021099938A1",
        "US11175145B2",
        "US11215711B2",
        "CN114626251A",
        "US11392131B2",
        "US11710309B2"
    ],
    "citedby_ftf": [
        "US8473430B2",
        "KR101700362B1",
        "US8824797B2",
        "US9569439B2",
        "US20130173298A1",
        "US10340034B2",
        "US10475142B2",
        "US10679309B2",
        "US10552581B2",
        "US10528913B2",
        "US10559380B2",
        "US9842390B2",
        "US9990712B2",
        "US9881399B2",
        "WO2016197303A1",
        "US9858525B2",
        "US10181208B2",
        "WO2018229548A2",
        "US11087525B2"
    ]
}