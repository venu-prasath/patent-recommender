{
    "patent_id": "US-11562246-B2",
    "title": "Asynchronous agents with learning coaches and structurally modifying deep neural networks without performance degradation ",
    "assignee": "D5Ai Llc",
    "publication_date": "2023-01-24",
    "patent_link": "https://patents.google.com/patent/US11562246B2/en",
    "inventors": [
        "James K. Baker"
    ],
    "classifications": [
        "G06N3/006",
        "G06N3/082",
        "G06N3/044",
        "G06N3/0445",
        "G06N3/045",
        "G06N3/084"
    ],
    "abstract": "Methods and computer systems improve a trained base deep neural network by structurally changing the base deep neural network to create an updated deep neural network, such that the updated deep neural network has no degradation in performance relative to the base deep neural network on the training data. The updated deep neural network is subsequently training. Also, an asynchronous agent for use in a machine learning system comprises a second machine learning system ML2 that is to be trained to perform some machine learning task. The asynchronous agent further comprises a learning coach LC and an optional data selector machine learning system DS. The purpose of the data selection machine learning system DS is to make the second stage machine learning system ML2 more efficient in its learning (by selecting a set of training data that is smaller but sufficient) and/or more effective (by selecting a set of training data that is focused on an important task). The learning coach LC is a machine learning system that assists the learning of the DS and ML2. Multiple asynchronous agents could also be in communication with each others, each trained and grown asynchronously under the guidance of their respective learning coaches to perform different tasks.",
    "claims": "\n1. A method of training a neural network, the neural network comprising one or more output nodes, wherein the one or more output nodes comprises at least a first output node, the method comprising iteratively training, by a computer system that comprises one or more programmed processing cores, the neural network with a set of training data items, wherein the iterative training comprises:\nfor each of n=1, . . . , N training iterations:\nin a forward computation phase, computing, by the computer system, in a forward computation through the neural network, activation values for nodes of the neural network for an nth training data item in the set of training data items, wherein computing the activation values comprises computing an activation value for the first output node; and\nin a back-propagation phase, computing, by the computer system, partial derivatives of a cost function for an objective for the neural network through the neural network for the nth training data item; and\nafter the forward computation phase and the back-propagation phase for the Nth training iteration, computing, by the computer system updated learned parameters for the neural network based on the computed partial derivatives for each of the n=1, . . . , N training iterations,\nwherein:\nthe first output node is connected to D1 detector nodes, wherein D1 is greater than or equal to three;\neach of the D1 detector nodes is trained to detect a category represented by the first output node; and\nthe n=1, . . . , N training iterations comprise:\nfor each of the n=1, . . . , N training iterations, determining by the computer system whether any of the D1 detector nodes are falsely activated; and\nfor each of the n=1, . . . , N training iterations for which there is a determination that two or more of the D1 detector nodes are falsely activated, including a first penalty term with the cost function for the two or more of the D1 detector nodes that are falsely activated in the computing of the partial derivatives in the back-propagation phase.\n2. The method of claim 1, wherein:\nthe first output node is connected to R1 rejecter nodes, wherein R1 is greater than or equal to three;\neach of the R1 rejecter nodes is trained to reject a category represented by the first output node; and\nthe n=1, . . . , N training iterations comprise:\nfor each of the n=1, . . . , N training iterations, determining by the computer system whether any of the R1 rejecter nodes are falsely activated; and\nfor each of the n=1, . . . , N training iterations for which there is a determination that two or more of the R1 rejecter nodes are falsely activated, including a second penalty term with the cost function for the two or more of the R1 rejecter nodes that are falsely activated in the computing of the partial derivatives in the back-propagation phase.\n3. The method of claim 1, wherein:\nthe neural network further comprises a second output node;\nthe second output node is connected to D2 detector nodes, where D2 is greater than or equal to three;\neach of the D2 detector nodes is trained to detect a category represented by the second output node; and\nthe n=1, . . . , N training iterations comprise:\nfor each of the n=1, . . . , N training iterations, determining by the computer system whether any of the D2 detector nodes are falsely activated; and\nfor each of the n=1, . . . , N training iterations for which there is a determination that two or more of the D2 detector nodes are falsely activated, including the first penalty term with the cost function for the two or more of the D2 detector nodes that are falsely activated in the computing of the partial derivatives in the back-propagation phase.\n4. A method of training a neural network, the neural network comprising one or more output nodes, wherein the one or more output nodes comprises at least a first output node, the method comprising iteratively training, by a computer system that comprises one or more programmed processing cores, the neural network with a set of training data items, wherein the iterative training comprises:\nfor each of n=1, . . . , N training iterations:\nin a forward computation phase, computing, by the computer system, in a forward computation through the neural network, activation values for nodes of the neural network for an nth training data item in the set of training data items, wherein computing the activation values comprises computing an activation value for the first output node; and\nin a back-propagation phase, computing, by the computer system, partial derivatives of a cost function for an objective for the neural network through the neural network for the nth training data item; and\nafter the forward computation phase and the back-propagation phase for the Nth training iteration, computing, by the computer system updated learned parameters for the neural network based on the computed partial derivatives for each of the n=1, . . . , N training iterations,\nwherein:\nthe first output node is connected to R1 rejecter nodes, wherein R1 is greater than or equal to three;\neach of the R1 rejecter nodes is trained to reject a category represented by the first output node; and\nthe n=1, . . . , N training iterations comprise:\nfor each of the n=1, . . . , N training iterations, determining by the computer system whether any of the R1 rejecter nodes are falsely activated; and\nfor each of the n=1, . . . , N training iterations for which there is a determination that two or more of the R1 rejecter nodes are falsely activated, including a penalty term with the cost function for the two or more of the R1 rejecter nodes that are falsely activated in the computing of the partial derivatives in the back-propagation phase.\n5. The method of claim 4, wherein:\nthe neural network further comprises a second output node;\nthe second output node is connected to R2 rejecter nodes, where R2 is greater than or equal to three;\neach of the R2 rejecter nodes is trained to detect a category represented by the second output node; and\nthe n=1, . . . , N training iterations comprise:\nfor each of the n=1, . . . , N training iterations, determining by the computer system whether any of the R2 rejecter nodes are falsely activated; and\nfor each of the n=1, . . . , N training iterations for which there is a determination that two or more of the R2 rejecter nodes are falsely activated, including the penalty term with the cost function for the two or more of the R2 rejecter nodes that are falsely activated in the computing of the partial derivatives in the back-propagation phase.\n6. A computer system comprising:\none or more processor cores; and\na memory in communication with the one or more processor cores, wherein the memory stores computer instructions that when executed by the one or more processor cores, cause the one or more processor cores to train a neural network, wherein:\nthe neural network comprising one or more output nodes;\nthe one or more output nodes comprises at least a first output node;\nthe computer instructions, when executed by the one or more processor cores, cause the one or more processor cores to train the neural network by iteratively training the neural network with a set of training data items by:\nfor each of n=1, . . . , N training iterations:\nin a forward computation phase, computing, in a forward computation through the neural network, activation values for nodes of the neural network, including an activation value for the first output node, for an nth training data item in the set of training data items; and\nin a back-propagation phase, computing, partial derivatives of a cost function for an objective for the neural network through the neural network for the nth training data item; and\nafter the forward computation phase and the back-propagation phase for the Nth training iteration, updating learned parameters for the neural network based on the computed partial derivatives for each of the n=1, . . . , N training iterations,\nwherein:\nthe first output node is connected to D1 detector nodes, wherein D1 is greater than or equal to three;\neach of the D1 detector nodes is trained to detect a category represented by the first output node;\nfor each of the n=1, . . . , N training iterations, the one or more processor cores are programmed to:\ndetermine whether any of the D1 detector nodes are falsely activated; and\nfor each of the n=1, . . . , N training iterations for which there is a determination that two or more of the D1 detector nodes are falsely activated, include a first penalty term with the cost function for the two or more of the D1 detector nodes that are falsely activated in the computing of the partial derivatives in the back-propagation phase.\n7. The computer system of claim 6, wherein:\nthe first output node is connected to R1 rejecter nodes, wherein R1 is greater than or equal to three;\neach of the R1 rejecter nodes is trained to reject a category represented by the first output node; and\nfor each of the n=1, . . . , N training iterations, the one or more processor cores are programmed to:\ndetermine whether any of the R1 rejecter nodes are falsely activated; and\nfor each of the n=1, . . . , N training iterations for which there is a determination that two or more of the R1 rejecter nodes are falsely activated, include a second penalty term with the cost function for the two or more of the R1 rejecter nodes that are falsely activated in the computing of the partial derivatives in the back-propagation phase.\n8. The computer system of claim 6, wherein:\nthe neural network further comprises a second output node;\nthe second output node is connected to D2 detector nodes, where D2 is greater than or equal to three;\neach of the D2 detector nodes is trained to detect a category represented by the second output node; and\nfor each of the n=1, . . . , N training iterations, the one or more processor cores are programmed to:\ndetermine whether any of the D2 detector nodes are falsely activated; and\nfor each of the n=1, . . . , N training iterations for which there is a determination that two or more of the D2 detector nodes are falsely activated, include the first penalty term with the cost function for the two or more of the D2 detector nodes that are falsely activated in the computing of the partial derivatives in the back-propagation phase.\n9. A computer system comprising:\none or more processor cores; and\na memory in communication with the one or more processor cores, wherein the memory stores computer instructions that when executed by the one or more processor cores, cause the one or more processor cores to train a neural network, wherein:\nthe neural network comprising one or more output nodes;\nthe one or more output nodes comprises at least a first output node;\nthe computer instructions, when executed by the one or more processor cores, cause the one or more processor cores to train the neural network by iteratively training with a set of training data items by:\nfor each of n=1, . . . , N training iterations:\nin a forward computation phase, computing, in a forward computation through the neural network, activation values for nodes of the neural network, including an activation value for the first output node, for an nth training data item in the set of training data items; and\nin a back-propagation phase, computing partial derivatives of a cost function for an objective for the neural network through the neural network for the nth training data item; and\nafter the forward computation phase and the back-propagation phase for the Nth training iteration, updating learned parameters for the neural network based on the computed partial derivatives for each of the n=1, . . . , N training iterations,\nwherein:\nthe first output node is connected to R1 rejecter nodes, wherein R1 is greater than or equal to three;\neach of the R1 rejecter nodes is trained to reject a category represented by the first output node;\nfor each of the n=1, . . . , N training iterations, the one or more processor cores are programmed to:\ndetermine whether any of the R1 rejecter nodes are falsely activated; and\nfor each of the n=1, . . . , N training iterations for which there is a determination that two or more of the R1 rejecter nodes are falsely activated, include a penalty term with the cost function for the two or more of the R1 rejecter nodes that are falsely activated in the computing of the partial derivatives in the back-propagation phase.\n10. The computer system of claim 9, wherein:\nthe neural network further comprises a second output node;\nthe second output node is connected to R2 rejecter nodes, where R2 is greater than or equal to three;\neach of the R2 rejecter nodes is trained to detect a category represented by the second output node; and\nfor each of the n=1, . . . , N training iterations, the one or more processor cores are programmed to:\nfor each of the n=1, . . . , N training iterations, determine whether any of the R2 rejecter nodes are falsely activated; and\nfor each of the n=1, . . . , N training iterations for which there is a determination that two or more of the R2 rejecter nodes are falsely activated, include the penalty term with the cost function for the two or more of the R2 rejecter nodes that are falsely activated in the computing of the partial derivatives in the back-propagation phase.",
    "status": "Active",
    "citations_own": [
        "US5214746A",
        "US20040059695A1",
        "US20070271075A1",
        "US20080077544A1",
        "US20080183685A1",
        "US20110098999A1",
        "US20110190657A1",
        "US20130238533A1",
        "US20140236578A1",
        "US20150106310A1",
        "US9015093B1",
        "WO2015162050A1",
        "US20150324689A1",
        "US20150363197A1",
        "US20160132786A1",
        "US20160155049A1",
        "US20160217367A1",
        "WO2016132145A1",
        "US20160335550A1",
        "US20160364522A1",
        "US20170068888A1",
        "WO2017062635A1",
        "US20170109628A1",
        "WO2018175098A1",
        "WO2018194960A1",
        "US20180336465A1",
        "WO2018226492A1",
        "WO2018226527A1",
        "WO2018231708A2",
        "WO2019005507A1",
        "WO2019005611A1",
        "US20190095798A1",
        "WO2019067831A1",
        "WO2019067542A1",
        "WO2019067960A1",
        "US20190108445A1",
        "US20190171947A1",
        "US20190197550A1",
        "WO2019152308A1",
        "US20200012923A1",
        "US20200184337A1"
    ],
    "citations_ftf": [
        "CN108475345A"
    ],
    "citedby_own": [
        "US20210374565A1"
    ],
    "citedby_ftf": [
        "US10241528B1",
        "US11182804B2",
        "US10746425B1",
        "US11295210B2",
        "WO2018226527A1",
        "US10885470B2",
        "WO2019005507A1",
        "WO2019067542A1",
        "US10679129B2",
        "US10770897B1",
        "EP3518153A1",
        "US11321612B2",
        "US10832137B2",
        "CN111602149A",
        "WO2020005471A1",
        "US10922587B2",
        "US11195097B2",
        "US11501164B2",
        "WO2020041026A1",
        "US11010670B2",
        "WO2020046719A1",
        "JP2021536640A",
        "CN111045422A",
        "US11341369B2",
        "US11715030B2",
        "US11657118B2",
        "US20220335296A1",
        "US11645498B2",
        "US20220383111A1",
        "CN110796497A",
        "US11507840B2",
        "US11556825B2",
        "CN113222103A",
        "CN112001502B",
        "US11355937B2",
        "US11735916B2",
        "EP3985560A1",
        "CN112734011B",
        "KR102277643B1",
        "WO2022212217A1",
        "CN115237797B"
    ]
}