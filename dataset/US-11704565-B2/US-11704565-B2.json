{
    "patent_id": "US-11704565-B2",
    "title": "Communication optimizations for distributed machine learning ",
    "assignee": "Intel Corporation",
    "publication_date": "2023-07-18",
    "patent_link": "https://patents.google.com/patent/US11704565B2/en",
    "inventors": [
        "Srinivas Sridharan",
        "Karthikeyan Vaidyanathan",
        "Dipankar Das",
        "Chandrasekaran Sakthivel",
        "Mikhail E. Smorkalov"
    ],
    "classifications": [
        "G06N3/08",
        "G06F9/50",
        "G06F9/5061",
        "G06F9/5077",
        "G06N3/04",
        "G06N3/044",
        "G06N3/045",
        "G06N3/063",
        "G06N3/084",
        "G06N3/088",
        "G06N3/048",
        "G06N7/01"
    ],
    "abstract": "Embodiments described herein provide a system to configure distributed training of a neural network, the system comprising memory to store a library to facilitate data transmission during distributed training of the neural network; a network interface to enable transmission and receipt of configuration data associated with a set of worker nodes, the worker nodes configured to perform distributed training of the neural network; and a processor to execute instructions provided by the library. The instructions cause the processor to create one or more groups of the worker nodes, the one or more groups of worker nodes to be created based on a communication pattern for messages to be transmitted between the worker nodes during distributed training of the neural network. The processor can transparently adjust communication paths between worker nodes based on the communication pattern.",
    "claims": "\n1. A system to configure distributed training of a neural network, the system comprising:\nmemory to store a library to facilitate data transmission during distributed training of the neural network;\na network interface to enable transmission and receipt of configuration data associated with a set of worker nodes, the worker nodes configured to perform distributed training of the neural network, wherein each worker node includes one or more graphics processors to perform compute operations associated with a machine learning framework workflow; and\na processor to execute instructions provided by the library, the instructions to cause the processor to create one or more groups of the worker nodes, the one or more groups of worker nodes to be created based on a communication pattern for messages to be transmitted between the worker nodes during distributed training of the neural network, and the instructions cause the processor to receive a request to transmit parameter data using a first communication pattern and, in response to the request, transparently adjust communication paths between worker nodes based on the first communication pattern.\n2. The system as in claim 1, wherein the first communication pattern is an all-reduce pattern.\n3. The system as in claim 1, wherein the instructions cause the processor to group the worker nodes into one or more groups of topologically local worker nodes and perform an all-reduce synchronization of parameter data between the worker nodes within a group.\n4. The system as in claim 3, wherein the instructions cause the processor to synchronize parameter data between groups via a parameter server.\n5. The system as in claim 4, the instructions to cause the processor to configure a worker node as a parameter server.\n6. The system as in claim 5, the instructions to cause the processor to synchronize parameter data between multiple sets of groups via multiple parameter servers.\n7. The system as in claim 5, the instructions to cause the processor to retrieve a known network topology for a network that interconnects a set of worker nodes and group the worker nodes into one or more groups of topologically local worker nodes based on the known network topology.\n8. The system as in claim 7, the instructions to cause the processor to initiate a sample training iteration on the set of worker nodes and group the worker nodes into one or more groups of worker nodes based on observed latency metrics for communication between the set of worker nodes.\n9. A method comprising:\ntransmitting configuration data associated with a set of worker nodes of a distributed training system configured to perform distributed training of a neural network, wherein each worker node includes one or more graphics processors to perform compute operations associated with a machine learning framework workflow;\ncreating one or more groups of the worker nodes based on a communication pattern for messages to be transmitted between the worker nodes during the distributed training of the neural network; and\nreceiving a request to transmit parameter data using a first communication pattern and, in response to the request, transparently adjust communication paths between the worker nodes based on the first communication pattern.\n10. The method as in claim 9, further comprising:\ngrouping the worker nodes into one or more groups of topologically local worker nodes; and\nperforming an all-reduce synchronization of parameter data between the worker nodes within a group.\n11. The method as in claim 10, further comprising:\nconfiguring a worker node as a parameter server; and\nsynchronizing parameter data between groups via the parameter server.\n12. The method as in claim 11, further comprising synchronizing parameter data between multiple sets of groups via multiple parameter servers.\n13. The method as in claim 12, further comprising:\nretrieving a known network topology for a network that interconnects a set of worker nodes; and\ngrouping the worker nodes into one or more groups of topologically local worker nodes based on the known network topology.\n14. The method as in claim 13, further comprising:\ninitiating a sample training iteration on the set of worker nodes and group the worker nodes into one or more groups of worker nodes based on observed latency metrics for communication between the set of worker nodes.\n15. A non-transitory machine-readable medium storing instructions which, when executed by one or more processors, cause the one or more processors to perform operations comprising:\ntransmitting configuration data associated with a set of worker nodes of a distributed training system configured to perform distributed training of a neural network, wherein each worker node includes one or more graphics processors to perform compute operations associated with a machine learning framework workflow;\ncreating one or more groups of the worker nodes based on a communication pattern for messages to be transmitted between the worker nodes during the distributed training of the neural network; and\nreceiving a request to transmit parameter data using a first communication pattern and, in response to the request, transparently adjust communication paths between the worker nodes based on the first communication pattern.\n16. The non-transitory machine-readable medium as in claim 15, the operations further comprising:\ngrouping the worker nodes into one or more groups of topologically local worker nodes; and\nperforming an all-reduce synchronization of parameter data between the worker nodes within a group.\n17. The non-transitory machine-readable medium as in claim 16, the operations further comprising:\nconfiguring a worker node as a parameter server; and\nsynchronizing parameter data between groups via the parameter server.\n18. The non-transitory machine-readable medium as in claim 17, further comprising synchronizing parameter data between multiple sets of groups via multiple parameter servers.\n19. The non-transitory machine-readable medium as in claim 18, the operations further comprising:\nretrieving a known network topology for a network that interconnects a set of worker nodes; and\ngrouping the worker nodes into one or more groups of topologically local worker nodes based on the known network topology.\n20. The non-transitory machine-readable medium as in claim 19, the operations further comprising:\ninitiating a sample training iteration on the set of worker nodes and group the worker nodes into one or more groups of worker nodes based on observed latency metrics for communication between the set of worker nodes."
}