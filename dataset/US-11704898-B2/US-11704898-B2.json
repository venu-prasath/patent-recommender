{
    "patent_id": "US-11704898-B2",
    "title": "Machine learning system and method for determining or inferring user action and intent based on screen image analysis ",
    "assignee": "M37 Inc.",
    "publication_date": "2023-07-18",
    "patent_link": "https://patents.google.com/patent/US11704898B2/en",
    "inventors": [
        "Ali Jelveh"
    ],
    "classifications": [
        "G06V10/7784",
        "G06F18/2178",
        "G06F18/41",
        "G06N20/00",
        "G06N3/084",
        "G06N3/006",
        "G06N7/01",
        "G06T13/40",
        "G06V2201/02"
    ],
    "abstract": "System(s) and method(s) that analyze image data associated with a computing screen operated by a user, and learns the image data (e.g., using pattern recognition, historical information analysis, user implicit and explicit training data, optical character recognition (OCR), video information, 360\u00b0/panoramic recordings, and so on) to concurrently glean information regarding multiple states of user interaction (e.g., analyzing data associated with multiple applications open on a desktop, mobile phone or tablet). A machine learning model is trained on analysis of graphical image data associated with screen display to determine or infer user intent. An input component receives image data regarding a screen display associated with user interaction with a computing device. An analysis component employs the model to determine or infer user intent based on the image data analysis; and an action component provisions services to the user as a function of the determined or inferred user intent. In an implementation, a gaming component gamifies interaction with the user in connection with explicitly training the model.",
    "claims": "\n1. A system, comprising:\na processor that executes computer executable components stored in memory, wherein the computer executable components comprise:\nan input component that receives graphical image data regarding user interaction with a computing device;\na lifegraph that stores time series data related to the user interaction with the computing device, wherein the lifegraph is an n-dimensional graph that represents a web of people, objects and relationships connected to a user, wherein n is a positive integer;\na model generation component that learns user behavior and builds a model that facilitates providing recommendations to optimize future computing device interaction, wherein the model employs a recursive learning algorithm and wherein the model learns impact of respective images via focus objects that indicate relevant portions of an on-screen display;\nan analysis component that employs the model to determine or infer user intent based on image analysis; and\nan action component that provisions services to the user as a function of determined or inferred user intent.\n2. The system of claim 1, further comprising a gaming component that gamifies interaction with the computing device in connection with explicitly training the model to create an incentivized environment to encourage the user and one or more additional users to continue training respective models for new functionalities.\n3. The system of claim 1, further comprising a reward component that exposes functionality associated with the model as a function of user training efforts.\n4. The system of claim 1, further comprising an interactive component that shares status of training the model with other entities.\n5. The system of claim 1, further comprising an avatar component that generates an avatar that interacts with the user based in part on the model and the action component.\n6. The system of claim 1, wherein the model further employs a backward propagation of learning across other models or a continuous learning algorithm.\n7. The system of claim 1, wherein the model directs the action component to revise actions as a function of learned impact, and wherein the focus objects are implemented via dirty rectangle algorithms.\n8. The system of claim 1, further comprising an optimization component that generates inferences, based on the model, regarding potential points of failure, weakness or bottlenecks in connection with taking automated action.\n9. The system of claim 1, comprising a scheduling component that employs the model to schedule activities for the user.\n10. The system of claim 2, wherein the gaming component displays progress of training of the model.\n11. The system of claim 10, wherein the gaming component displays exposed functionalities of the model.\n12. The system of claim 1, further comprising a training component that generates an interface for explicitly training the model.\n13. The system of claim 12, wherein the training component generates questions regarding accuracy of determinations or inferences generated by the model.\n14. The system of claim 1, wherein the model identifies two or more applications running concurrently on a screen and determines or infers the user intent regarding the respective applications.\n15. The system of claim 1, wherein the model analyzes multiple sets of screen image data in a time series manner.\n16. A method, comprising:\nexecuting, by a system operatively coupled to a processor, computer implemented components stored in memory to perform the following acts:\nreceiving, by the system operatively coupled to the processor, graphical image data regarding user interaction with a computing device;\nstoring, by the system operatively coupled to the processor, in a lifegraph, time-series data related to the user interaction with the computing device, wherein the lifegraph is an n-dimensional graph that represents a web of people, objects and relationships connected to a user, wherein n is a positive integer;\nemploying, by the system operatively coupled to the processor, a model to determine or infer user intent based on graphical image analysis, wherein the model employs a recursive learning algorithm and wherein the model learns impact of respective images via focus objects that indicate relevant portions of an on-screen display;\nproviding, by the system operatively coupled to the processor, recommendations associated with the user intent to optimize future computing device interaction; and\nprovisioning, by the system operatively coupled to the processor, services as a function of determined or inferred user intent.\n17. The method of claim 16, further comprising employing backward propagation of learning across other models or continuous learning algorithm.\n18. The method of claim 16, further comprising using a gaming component to gamify interaction with the computing device in connection with explicitly training the model to create an incentivized environment to encourage the user and one or more additional users to continue training respective models for new functionalities.\n19. The method of claim 16, further comprising directing an action component to revise actions as a function of learned impact, wherein the focus objects are implemented via dirty rectangle algorithms.\n20. A computer readable storage device comprising instructions that, in response to execution, cause a system comprising a processor to perform operations, comprising:\nreceiving graphical image data regarding a screen display associated with user interaction with a computing device;\nstoring, in a lifegraph, time-series data related to the user interaction with the computing device, wherein the lifegraph is an n-dimensional graph that represents a web of people, objects and relationships connected to a user, wherein n is a positive integer;\nemploying a model to determine or infer user intent based on analysis of the graphical image data, wherein the model employs a recursive learning algorithm and wherein the model learns impact of respective images via focus objects that indicate relevant portions of the screen display;\nproviding recommendations associated with the user intent to optimize future computing device interaction; and\nprovisioning services to the user as a function of determined or inferred user intent.",
    "status": "Active",
    "citations_own": [
        "US20090119234A1",
        "US20100169792A1",
        "US20140096077A1",
        "US8706754B2",
        "US8867807B1",
        "US20160041733A1",
        "US20170031711A1",
        "US20170132019A1",
        "US20170178007A1",
        "US20180131645A1",
        "US20180165596A1",
        "US20180188938A1",
        "US20180336411A1",
        "US20190042951A1",
        "US20190252061A1"
    ],
    "citations_ftf": [],
    "citedby_own": [],
    "citedby_ftf": [
        "US11465640B2",
        "US10836403B2",
        "US10867218B2",
        "US11188930B2",
        "US11645467B2",
        "US10931659B2",
        "US11151603B2",
        "US20200242540A1",
        "US11710034B2",
        "US11270241B2",
        "US20220229764A1",
        "US11524691B2",
        "US11481420B2",
        "US11347756B2",
        "US11127073B2",
        "GB2590542B",
        "CN115004190A",
        "US11074164B1",
        "CN111369154A",
        "EP3885917A1",
        "US11575626B2",
        "US11356389B2",
        "US20220043964A1",
        "US11558335B2",
        "US11782733B2",
        "KR20220079336A",
        "FR3117240A1",
        "US11526570B2",
        "TWI774258B",
        "US11763228B2",
        "CN115098782B",
        "CN116823598A"
    ]
}