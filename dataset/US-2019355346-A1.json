{
    "patent_id": "US-2019355346-A1",
    "title": "Global semantic word embeddings using bi-directional recurrent neural networks ",
    "assignee": "Apple Inc.",
    "publication_date": "2019-11-21",
    "patent_link": "https://patents.google.com/patent/US20190355346A1/en",
    "inventors": [
        "Jerome R. Bellegarda"
    ],
    "classifications": [
        "G10L15/063",
        "G06F40/20",
        "G06F40/216",
        "G06F40/284",
        "G06F40/289",
        "G06F40/30",
        "G06N20/00",
        "G06N3/042",
        "G06N3/044",
        "G06N3/08",
        "G06N5/022",
        "G06N7/01",
        "G10L15/16",
        "G10L15/1815",
        "G10L15/30",
        "G06N5/041",
        "G06N5/043"
    ],
    "abstract": "Systems and processes for operating a digital assistant are provided. In accordance with one or more examples, a method includes, receiving training data for a data-driven learning network. The training data include a plurality of word sequences. The method further includes obtaining representations of an initial set of semantic categories associated with the words included in the training data; and training the data-driven learning network based on the plurality of word sequences included in the training data and based on the representations of the initial set of semantic categories. The training is performed using the word sequences in their entirety. The method further includes obtaining, based on the trained data-driven learning network, representations of a set of semantic embeddings of the words included in the training data; and providing the representations of the set of semantic embeddings to at least one of a plurality of different natural language processing tasks.",
    "claims": "\n1. An electronic device, comprising:\none or more processors;\na microphone; and\nmemory storing one or more programs configured to be executed by the one or more processors, the one or more programs including instructions for:\nreceiving training data for a data-driven learning network, wherein the training data include a plurality of word sequences;\nobtaining representations of an initial set of semantic categories associated with the words included in the training data;\ntraining the data-driven learning network based on the plurality of word sequences included in the training data and based on the representations of the initial set of semantic categories, wherein the training is performed using the word sequences in their entirety;\nobtaining, based on the trained data-driven learning network, representations of a set of semantic embeddings of the words included in the training data; and\nproviding the representations of the set of semantic embeddings to at least one of a plurality of different natural language processing tasks, wherein one or more of the different natural language processing tasks are performed using the representation of semantic embeddings.\n2. The electronic device of claim 1, wherein each of the plurality of word sequences represents a sentence, a paragraph, or a document.\n3. The electronic device of claim 1, wherein obtaining representations of the initial set of semantic categories comprises:\nperforming a latent semantic mapping (LSM) of the words included in the training data; and\ndetermining representations of the initial set of semantic categories based on the results of performing the latent semantic mapping, wherein one or more semantic categories of the initial set of semantic categories are associated with the corresponding word sequences in their entirety.\n4. The electronic device of claim 1, wherein training the data-driven learning network based on the plurality of word sequences included in the training data and based on the representations of the initial set of semantic categories comprises:\ngenerating a plurality of input token sequences corresponding to the plurality of word sequences included in the training data; and\ntraining the data-driven learning network using the plurality of input token sequences.\n5. The electronic device of claim 4, wherein generating the plurality of input token sequences corresponding to the plurality of word sequences included in the training data comprises:\nencoding one or more words included in a first word sequence of the plurality of word sequences; and\ngenerating, based on the encoded words, a first input token sequence corresponding to the first word sequence of the plurality of word sequences.\n6. The electronic device of claim 5, wherein encoding one or more words included in the first word sequence of the plurality of word sequences comprises:\ndetermining, for each word included in the first word sequence of the plurality of word sequences, whether the word corresponds to a word in a vocabulary;\nin accordance with a determination that the word corresponds to a word in the vocabulary, encoding the word using 1-of-N encoding, wherein N is the number of words in the vocabulary; and\nin accordance with a determination that the word included in the word sequence of the plurality of word sequences does not correspond to a word in the vocabulary,\nincreasing the dimension of the encoding, and\nencoding the word using the increased-dimension encoding.\n7. The electronic device of claim 5, further comprising:\nencoding one or more words included in additional word sequences of the plurality of word sequences; and\ngenerating, based on the encoded words, additional input token sequences corresponding to the additional word sequences of the plurality of word sequences.\n8. The electronic device of claim 4, wherein training the data-driven learning network using the plurality of input token sequences comprises:\nproviding a first input token sequence of the plurality of input token sequences to a bi-directional recurrent neural network; and\ndetermining, for each token included in the first input token sequence, a continuous-space representation of the token included in the first input token sequence.\n9. The electronic device of claim 8, wherein the bi-directional recurrent neural network is a bi-directional long short-term memory (LSTM) recurrent neural network.\n10. The electronic device of claim 8, wherein determining, for each token included in the first input token sequence, a continuous-space representation of the token included in the first input token sequence comprises:\ndetermining, for a current input unit representing a current token, a current first-context unit;\ndetermining, for a current input unit representing current token, a current second-context unit; and\ndetermining the continuous-space representation of the current token based on the current first-context unit and the current second-context unit.\n11. The electronic device of claim 8, further comprising:\npredicting, for each token included in the first input token sequence, a representation of a semantic category based on the continuous-space representation of the token included in the first input token sequence.\n12. The electronic device of claim 11, further comprising:\npredicting, for each token included in the first input token sequence, an expected following token based on the continuous-space representation of the token included in the first input token sequence.\n13. The electronic device of claim 12, further comprising:\nupdating one or more parameters of the data-driven learning network based on at least one of the predicted representations of semantic categories and based on at least one of the expected following tokens.\n14. The electronic device of claim 8, further comprising:\nobtaining representations of an interim set of semantic embeddings based on the continuous-space representations of the tokens included in the first input token sequence; and\nupdating the representations of the initial set of semantic categories based on the representations of the interim set of semantic embeddings.\n15. The electronic device of claim 8, further comprising:\nproviding additional input token sequences of the plurality of input token sequences to the bi-directional recurrent neural network; and\ndetermining additional continuous-space representations of the tokens included in the corresponding additional input token sequences.\n16. The electronic device of claim 15, further comprising:\npredicting additional representations of semantic categories based on the additional continuous-space representations of the tokens included in the corresponding additional input token sequences;\npredicting additional expected following tokens based on the additional continuous-space representations of the corresponding tokens included in the corresponding additional input token sequences; and\nupdating one or more parameters of the data-driven learning network based on at least one of the additional predicted representations of semantic categories and based on at least one of the additional expected following tokens.\n17. The electronic device of claim 15, further comprising:\nobtaining representations of additional sets of interim semantic embeddings based on the additional continuous-space representations of the tokens included in the additional input token sequences; and\nupdating representations of previously-updated sets of semantic categories based on the representations of the corresponding additional sets of interim semantic embeddings.\n18. The electronic device of claim 1, wherein the at least one of the plurality of different natural language processing tasks include one or more of: predictive typing, document classification, sentiment classification, question answering, and speech tagging.\n19. The electronic device of claim 1, wherein the one or more of the different natural language processing tasks are performed using the representations of set of semantic embeddings without substantial adaptation of the semantic embeddings.\n20. A non-transitory computer-readable storage medium storing one or more programs configured to be executed by one or more processors of an electronic device, the one or more programs including instructions for:\nreceiving training data for a data-driven learning network, wherein the training data include a plurality of word sequences;\nobtaining representations of an initial set of semantic categories associated with the words included in the training data;\ntraining the data-driven learning network based on the plurality of word sequences included in the training data and based on the representations of the initial set of semantic categories, wherein the training is performed using the word sequences in their entirety;\nobtaining, based on the trained data-driven learning network, representations of a set of semantic embeddings of the words included in the training data; and\nproviding the representations of the set of semantic embeddings to at least one of a plurality of different natural language processing tasks, wherein one or more of the different natural language processing tasks are performed using the representation of semantic embeddings.\n21. The computer-readable storage medium of claim 20, wherein training the data-driven learning network based on the plurality of word sequences included in the training data and based on the representations of the initial set of semantic categories comprises:\ngenerating a plurality of input token sequences corresponding to the plurality of word sequences included in the training data; and\ntraining the data-driven learning network using the plurality of input token sequences.\n22. The computer-readable storage medium of claim 21, wherein generating the plurality of input token sequences corresponding to the plurality of word sequences included in the training data comprises:\nencoding one or more words included in a first word sequence of the plurality of word sequences; and\ngenerating, based on the encoded words, a first input token sequence corresponding to the first word sequence of the plurality of word sequences.\n23. The computer-readable storage medium of claim 22, wherein encoding one or more words included in the first word sequence of the plurality of word sequences comprises:\ndetermining, for each word included in the first word sequence of the plurality of word sequences, whether the word corresponds to a word in a vocabulary;\nin accordance with a determination that the word corresponds to a word in the vocabulary, encoding the word using 1-of-N encoding, wherein N is the number of words in the vocabulary; and\nin accordance with a determination that the word included in the word sequence of the plurality of word sequences does not correspond to a word in the vocabulary,\nincreasing the dimension of the encoding, and\nencoding the word using the increased-dimension encoding.\n24. The computer-readable storage medium of claim 22, further comprising:\nencoding one or more words included in additional word sequences of the plurality of word sequences; and\ngenerating, based on the encoded words, additional input token sequences corresponding to the additional word sequences of the plurality of word sequences.\n25. The computer-readable storage medium of claim 21, wherein training the data-driven learning network using the plurality of input token sequences comprises:\nproviding a first input token sequence of the plurality of input token sequences to a bi-directional recurrent neural network; and\ndetermining, for each token included in the first input token sequence, a continuous-space representation of the token included in the first input token sequence.\n26. The computer-readable storage medium of claim 25, further comprising:\npredicting, for each token included in the first input token sequence, a representation of a semantic category based on the continuous-space representation of the token included in the first input token sequence.\n27. The computer-readable storage medium of claim 26, further comprising:\npredicting, for each token included in the first input token sequence, an expected following token based on the continuous-space representation of the token included in the first input token sequence.\n28. The computer-readable storage medium of claim 27, further comprising:\nupdating one or more parameters of the data-driven learning network based on at least one of the predicted representations of semantic categories and based on at least one of the expected following tokens.\n29. The computer-readable storage medium of claim 25, further comprising:\nobtaining representations of an interim set of semantic embeddings based on the continuous-space representations of the tokens included in the first input token sequence; and\nupdating the representations of the initial set of semantic categories based on the representations of the interim set of semantic embeddings.\n30. The computer-readable storage medium of claim 20, wherein the one or more of the different natural language processing tasks are performed using the representations of set of semantic embeddings without substantial adaptation of the semantic embeddings.\n31. A method for generating semantic embeddings, comprising:\nat an electronic device including at least one processor:\nreceiving training data for a data-driven learning network, wherein the training data include a plurality of word sequences;\nobtaining representations of an initial set of semantic categories associated with the words included in the training data;\ntraining the data-driven learning network based on the plurality of word sequences included in the training data and based on the representations of the initial set of semantic categories, wherein the training is performed using the word sequences in their entirety;\nobtaining, based on the trained data-driven learning network, representations of a set of semantic embeddings of the words included in the training data; and\nproviding the representations of the set of semantic embeddings to at least one of a plurality of different natural language processing tasks, wherein one or more of the different natural language processing tasks are performed using the representation of semantic embeddings.\n32. The method of claim 31, wherein training the data-driven learning network based on the plurality of word sequences included in the training data and based on the representations of the initial set of semantic categories comprises:\ngenerating a plurality of input token sequences corresponding to the plurality of word sequences included in the training data; and\ntraining the data-driven learning network using the plurality of input token sequences.\n33. The method of claim 32, wherein generating the plurality of input token sequences corresponding to the plurality of word sequences included in the training data comprises:\nencoding one or more words included in a first word sequence of the plurality of word sequences; and\ngenerating, based on the encoded words, a first input token sequence corresponding to the first word sequence of the plurality of word sequences.\n34. The method of claim 33, wherein encoding one or more words included in the first word sequence of the plurality of word sequences comprises:\ndetermining, for each word included in the first word sequence of the plurality of word sequences, whether the word corresponds to a word in a vocabulary,\nin accordance with a determination that the word corresponds to a word in the vocabulary, encoding the word using 1-of-N encoding, wherein N is the number of words in the vocabulary; and\nin accordance with a determination that the word included in the word sequence of the plurality of word sequences does not correspond to a word in the vocabulary,\nincreasing the dimension of the encoding, and\nencoding the word using the increased-dimension encoding.\n35. The method of claim 33, further comprising:\nencoding one or more words included in additional word sequences of the plurality of word sequences; and\ngenerating, based on the encoded words, additional input token sequences corresponding to the additional word sequences of the plurality of word sequences.\n36. The method of claim 32, wherein training the data-driven learning network using the plurality of input token sequences comprises:\nproviding a first input token sequence of the plurality of input token sequences to a bi-directional recurrent neural network; and\ndetermining, for each token included in the first input token sequence, a continuous-space representation of the token included in the first input token sequence.\n37. The method of claim 36, further comprising:\npredicting, for each token included in the first input token sequence, a representation of a semantic category based on the continuous-space representation of the token included in the first input token sequence.\n38. The method of claim 37, further comprising:\npredicting, for each token included in the first input token sequence, an expected following token based on the continuous-space representation of the token included in the first input token sequence.\n39. The method of claim 38, further comprising:\nupdating one or more parameters of the data-driven learning network based on at least one of the predicted representations of semantic categories and based on at least one of the expected following tokens.\n40. The method of claim 36, further comprising:\nobtaining representations of an interim set of semantic embeddings based on the continuous-space representations of the tokens included in the first input token sequence; and\nupdating the representations of the initial set of semantic categories based on the representations of the interim set of semantic embeddings.\n41. The method of claim 31, wherein the one or more of the different natural language processing tasks are performed using the representations of set of semantic embeddings without substantial adaptation of the semantic embeddings."
}