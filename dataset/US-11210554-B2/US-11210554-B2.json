{
    "patent_id": "US-11210554-B2",
    "title": "Artificial intelligence-based generation of sequencing metadata ",
    "assignee": "Illumina, Inc.",
    "publication_date": "2021-12-28",
    "patent_link": "https://patents.google.com/patent/US11210554B2/en",
    "inventors": [
        "Anindita DUTTA",
        "Dorna KASHEFHAGHIGHI",
        "Amirali KIA"
    ],
    "classifications": [
        "G06K9/6218",
        "G16B40/20",
        "C12Q1/6874",
        "G06F16/58",
        "G06F16/907",
        "G06F18/213",
        "G06F18/214",
        "G06F18/217",
        "G06F18/23",
        "G06F18/23211",
        "G06F18/24",
        "G06F18/241",
        "G06F18/2415",
        "G06F18/2431",
        "G06K9/6202",
        "G06K9/6222",
        "G06K9/6232",
        "G06K9/6256",
        "G06K9/6262",
        "G06K9/6267",
        "G06K9/6277",
        "G06K9/628",
        "G06N3/04",
        "G06N3/042",
        "G06N3/045",
        "G06N3/08",
        "G06N3/084",
        "G06N7/005",
        "G06N7/01",
        "G06V10/267",
        "G06V10/454",
        "G06V10/751",
        "G06V10/763",
        "G06V10/764",
        "G06V10/7715",
        "G06V10/7784",
        "G06V10/82",
        "G06V10/993",
        "G06V20/69",
        "G16B30/00",
        "G16B30/10",
        "G16B30/20",
        "G16B40/00",
        "G16B40/10",
        "G06N3/044",
        "G06N3/048",
        "G06N5/046",
        "G06V20/47",
        "G06V2201/03"
    ],
    "abstract": "The technology disclosed uses neural networks to determine analyte metadata by (i) processing input image data derived from a sequence of image sets through a neural network and generating an alternative representation of the input image data, the input image data has an array of units that depicts analytes and their surrounding background, (ii) processing the alternative representation through an output layer and generating an output value for each unit in the array, (iii) thresholding output values of the units and classifying a first subset of the units as background units depicting the surrounding background, and (iv) locating peaks in the output values of the units and classifying a second subset of the units as center units containing centers of the analytes.",
    "claims": "\n1. A neural network-implemented method of determining cluster metadata from image data generated based upon one or more clusters, the method including:\nreceiving input image data, the input image data derived from a sequence of images,\nwherein each image in the sequence of images represents an imaged region and depicts intensity emissions of the one or more clusters and their surrounding background at a respective one of a plurality of sequencing cycles of a sequencing run, and\nwherein the input image data comprises image patches extracted from each image in the sequence of images;\nprocessing the input image data through a neural network to generate an alternative representation of the input image data, wherein the neural network is trained for cluster metadata determination tasks, including determining cluster background, cluster centers, and cluster shapes;\nprocessing the alternative representation through an output layer to generate an output indicating properties of respective portions of the imaged region;\nthresholding output values of the output and classifying a first subset of the respective portions of the imaged region as background portions depicting the surrounding background;\nlocating peaks in the output values of the output and classifying a second subset of the respective portions of the imaged region as center portions containing centers of the clusters; and\napplying a segmenter to the output values of the output and determining shapes of the clusters as non-overlapping regions of contiguous portions of the imaged region separated by the background portions and centered at the center portions.\n2. The neural network-implemented method of claim 1, wherein the properties include\nwhether a portion represents background or cluster, and\nwhether a portion represents a center of a plurality of contiguous image portions each representing a same cluster.\n3. The neural network-implemented method of claim 1, wherein the output identifies\nthe one or more clusters, whose intensity emissions are depicted by the input image data, as disjoint regions of adjoining units,\ncenters of the one or more clusters as center units at centers of mass of the respective ones of the disjoint regions, and\ntheir surrounding background as background units not belonging to any of the disjoint regions.\n4. The neural network-implemented method of claim 3, wherein the adjoining units in the respective ones of the disjoint regions have intensity values weighted according to distance of an adjoining unit from a center unit in a disjoint region to which the adjoining unit belongs.\n5. The neural network-implemented method of claim 1, wherein the output is a binary map which classifies each portion as cluster or background.\n6. The neural network-implemented method of claim 1, wherein the output is a ternary map which classifies each portion as cluster, background, or center.\n7. The neural network-implemented method of claim 1, further including:\napplying a peak locator to the output to find peak intensities in the output;\ndetermining location coordinates of the centers of the clusters based on the peak intensities;\ndownscaling the location coordinates by an upsampling factor used to prepare the input image data; and\nstoring the downscaled location coordinates in memory for use in base calling the clusters.\n8. The neural network-implemented method of claim 7, further including:\ncategorizing the adjoining units in the respective ones of the disjoint regions as cluster interior units belonging to a same cluster; and\nstoring the categorization and downscaled location coordinates of the cluster interior units in the memory on an cluster-by-cluster basis for use in base calling the clusters.\n9. The neural network-implemented method of claim 1, further including:\nobtaining training data for training the neural network,\nwherein the training data includes a plurality of training examples and corresponding ground truth data,\nwherein each training example includes image data from a sequence of image sets,\nwherein each image in the sequence of image sets represents a tile of a flow cell and depicts intensity emissions of clusters on the tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on the flow cell, and\nwherein each ground truth data identifies properties of respective portions of the training examples; and\nusing a gradient descent training technique to train the neural network and generating outputs for the training examples that progressively match the ground truth data, including iteratively\noptimizing a loss function that minimizes error between the outputs and the ground truth data, and\nupdating parameters of the neural network based on the error.\n10. The neural network-implemented method of claim 9, wherein the properties comprise\nclusters, whose intensity emissions are depicted by the image data of a corresponding training example, as disjoint regions of adjoining units,\ncenters of the clusters as center units at centers of mass of the respective ones of the disjoint regions, and\ntheir surrounding background as background units not belonging to any of the disjoint regions.\n11. The neural network-implemented method of claim 10, wherein the properties comprise identifying whether a unit is a center or a non-center.\n12. The neural network-implemented method of claim 11, further including:\nupon error convergence after a final iteration, storing the updated parameters of the neural network in memory to be applied to further neural network-based template generation and base calling.\n13. The neural network-implemented method of claim 12, wherein, in the ground truth data, the adjoining units in the respective ones of the disjoint regions have intensity values weighted according to distance of an adjoining unit from a center unit in a disjoint region to which the adjoining unit belongs.\n14. The neural network-implemented method of claim 13, wherein, in the ground truth data, the center units have highest intensity values within the respective ones of the disjoint regions.\n15. The neural network-implemented method of claim 14, wherein the loss function is mean squared error and the error is minimized on a unit-basis between the normalized intensity values of corresponding units in the outputs and the ground truth data.\n16. The neural network-implemented method of claim 15, wherein, in the training data, multiple training examples respectively include as image data different image patches from each image in a sequence of image sets of a same tile, and\nwherein at least some of the different image patches overlap with each other.\n17. The neural network-implemented method of claim 16, wherein, in the ground truth data,\nunits classified as cluster centers are all assigned a same first predetermined class score, and\nunits classified as non-centers are all assigned a same second predetermined class score.\n18. The neural network-implemented method of claim 17, wherein the loss function is custom weighted binary cross entropy loss and the error is minimized on a unit-basis between the prediction scores and the class scores of corresponding units in the outputs and the ground truth data.\n19. The neural network-implemented method of claim 18, wherein, in the ground truth data,\nunits classified as background are all assigned a same first predetermined class score,\nunits classified as cluster centers are all assigned a same second predetermined class score, and\nunits classified as cluster interior are all assigned a same third predetermined class score.\n20. The neural network-implemented method of claim 19, further including:\nthresholding output values of the units and classifying a first subset of the units as background units depicting the surrounding background;\nlocating peaks in the output values of the units and classifying a second subset of the units as center units containing centers of the clusters; and\napplying a segmenter to the output values of the units and determining shapes of the clusters as non-overlapping regions of contiguous units separated by the background units and centered at the center units, wherein the segmenter begins with the center units and determines, for each center unit, a group of successively contiguous units that depict a same cluster whose center is contained in the center unit.\n21. The neural network-implemented method of claim 20, wherein the non-overlapping regions have irregular contours and the units are units, further including:\ndetermining cluster intensity of a given cluster by:\nidentifying units that contribute to the cluster intensity of the given cluster based on a corresponding non-overlapping region of contiguous units that identifies a shape of the given cluster;\nlocating the identified units in one or more optical, pixel resolution images generated for one or more image channels at a current sequencing cycle;\nin each of the images, interpolating intensities of the identified units, combining the interpolated intensities, and normalizing the combined interpolated intensities to produce a per-image cluster intensity for the given cluster in each of the images; and\ncombining the per-image cluster intensity for each of the images to determine the cluster intensity of the given cluster at the current sequencing cycle.\n22. The neural network-implemented method of claim 21, wherein the non-overlapping regions have irregular contours and the units are units, further including:\ndetermining cluster intensity of a given cluster by:\nidentifying units that contribute to the cluster intensity of the given cluster based on a corresponding non-overlapping region of contiguous units that identifies a shape of the given cluster;\nlocating the identified units in one or more unit resolution images upsampled from corresponding optical, pixel resolution images generated for one or more image channels at a current sequencing cycle;\nin each of the upsampled images, combining intensities of the identified units and normalizing the combined intensities to produce a per-image cluster intensity for the given cluster in each of the upsampled images; and\ncombining the per-image cluster intensity for each of the upsampled images to determine the cluster intensity of the given cluster at the current sequencing cycle.\n23. The neural network-implemented method of claim 22, wherein the normalizing is based on a normalization factor, and\nwherein the normalization factor is a number of the identified units.\n24. The neural network-implemented method of claim 23, further including:\nbase calling the given cluster based on the cluster intensity at the current sequencing cycle.",
    "status": "Active",
    "citations_own": [
        "US20060064248A1",
        "US20060269130A1",
        "JP2007199397A",
        "WO2008154317A1",
        "US20090081775A1",
        "US20100046830A1",
        "US20100111370A1",
        "US20100157086A1",
        "US20110286628A1",
        "US20120020537A1",
        "US8241573B2",
        "US8392126B2",
        "US8401258B2",
        "US20130079232A1",
        "US20130188866A1",
        "US20130250407A1",
        "US8594439B2",
        "US8725425B2",
        "US8795971B2",
        "WO2014142921A1",
        "US20150079596A1",
        "US20150117784A1",
        "WO2015084985A2",
        "US20150169824A1",
        "US20160042511A1",
        "US20160078272A1",
        "US20160110498A1",
        "US20160196479A1",
        "WO2016145516A1",
        "US20160357903A1",
        "US20160356715A1",
        "CA2894317A1",
        "WO2016201564A1",
        "US20160371431A1",
        "EP3130681A1",
        "US20170169313A1",
        "US9708656B2",
        "US20170249744A1",
        "US20170249421A1",
        "WO2017184997A1",
        "US20180075279A1",
        "US20180107927A1",
        "US20180114337A1",
        "US20180189613A1",
        "WO2018129314A1",
        "US10023911B2",
        "US20180201992A1",
        "US10068054B2",
        "EP3373238A1",
        "WO2018165099A1",
        "US20180305751A1",
        "WO2018203084A1",
        "US20180322327A1",
        "US20180330824A1",
        "US20180334712A1",
        "US20180334711A1",
        "WO2019027767A1",
        "WO2019028047A1",
        "US20190080450A1",
        "WO2019055856A1",
        "US10241075B2",
        "US20190107642A1",
        "US20190114544A1",
        "WO2019079202A1",
        "WO2019090251A2",
        "US20190156915A1",
        "US20190164010A1",
        "US20190170680A1",
        "WO2019136388A1",
        "WO2019136284A1",
        "WO2019140402A1",
        "WO2019147904A1",
        "US20190237163A1",
        "US20190272638A1",
        "US20190332118A1",
        "US20190392578A1",
        "WO2020014280A1",
        "US10540591B2",
        "US20200027002A1",
        "US10648027B2",
        "US20200176082A1",
        "WO2020123552A1",
        "US10711299B2",
        "US20200302603A1",
        "US20200320294A1",
        "US20200388029A1",
        "US20210027462A1",
        "US20210056287A1",
        "US20210072391A1",
        "US20210089827A1"
    ],
    "citations_ftf": [
        "SE0301945D0",
        "US8300971B2",
        "US10061972B2",
        "AU2016313775A1",
        "WO2017153848A1",
        "CN109072304A",
        "JP6915349B2",
        "FR3109635B1"
    ],
    "citedby_own": [],
    "citedby_ftf": [
        "US11615285B2",
        "US11663478B2",
        "EP3617947A1",
        "US11652603B2",
        "US11569978B2",
        "US11783917B2",
        "CN110084309B",
        "CN113767416A",
        "US11593649B2",
        "US11797827B2",
        "US11651210B2",
        "US11580401B2",
        "US20210181930A1",
        "JP2023515108A",
        "US11188778B1",
        "US20220114259A1",
        "US11800258B2",
        "US20220180630A1",
        "CN112949499A",
        "JP2022147328A",
        "CN113052189B",
        "CA3183578A1",
        "US20220366043A1",
        "US11693570B2",
        "CN113361683B",
        "CN113095304B",
        "WO2022271983A1",
        "US20220415442A1",
        "WO2023283411A2",
        "CN113343937B",
        "WO2023003757A1",
        "CN113552855B",
        "CN113780450B",
        "CN113963199B",
        "WO2023097685A1",
        "CN114200548B",
        "CN114445456B",
        "WO2023115550A1",
        "CN114706798B",
        "CN115078430B",
        "CN115409174B",
        "CN116363403B"
    ]
}