{
    "patent_id": "US-2022172050-A1",
    "title": "Method for an explainable autoencoder and an explainable generative adversarial network ",
    "assignee": "UMNAI Limited",
    "publication_date": "2022-06-02",
    "patent_link": "https://patents.google.com/patent/US20220172050A1/en",
    "inventors": [
        "Angelo Dalli",
        "Mauro PIRRONE",
        "Matthew GRECH"
    ],
    "classifications": [
        "G06N3/08",
        "G06N3/045",
        "G06N3/047",
        "G06N5/022",
        "G06N5/045"
    ],
    "abstract": "An exemplary embodiment provides an autoencoder which is explainable. An exemplary autoencoder may explain the degree to which each feature of the input attributed to the output of the system, which may be a compressed data representation. An exemplary embodiment may be used for classification, such as anomaly detection, as well as other scenarios where an autoencoder is input to another machine learning system or when an autoencoder is a component in an end-to-end deep learning architecture. An exemplary embodiment provides an explainable generative adversarial network that adds explainable generation, simulation and discrimination capabilities. The underlying architecture of an exemplary embodiment may be based on an explainable or interpretable neural network, allowing the underlying architecture to be a fully explainable white-box machine learning system.",
    "claims": "\n1. A method for providing an explainable model, the explainable model comprising one or more of an explainable autoencoder and an explainable generative adversarial network, comprising:\nforming a first model and a second model; wherein one or more of the first model and second model is a white-box explainable model;\ngenerating synthetic data from the first model and inputting the synthetic data and/or an input dataset to the second model, and forming an explainable model, the explainable model comprising one or more of an explainable autoencoder and an explainable generative adversarial network based on an output of the second model associated with the synthetic data and/or an input dataset, wherein the training comprises identifying a plurality of partitions associated with the synthetic data and the input dataset, wherein the partitions are arranged in a hierarchy;\nextracting partition information from the explainable model and identifying a plurality of feature attributions associated with a plurality of features of the input dataset, and generating one or more explanations based on the partition information and the feature attributions.\n2. The method of claim 1, wherein the white-box explainable model comprises a recurrent neural network architecture configured to utilize sequentially ordered data, wherein the recurrent neural network comprises one or more hidden states, input vector, weights, and output at each of a plurality of hidden layers.\n3. The method of claim 1, further comprising a class selection input parameter, wherein the explanation corresponds to an output class indicated by the class selection input parameter.\n4. The method of claim 1, further comprising forming a behavioral model framework comprising a hierarchy of behavioral models, wherein each behavioral model comprises a plurality of conditional constraints configured to trigger a plurality of actions, and further comprising verifying the at least one of the explainable autoencoder, the first model, and the second model, wherein verification is based on at least one of: a Temporal Logic of Actions, Abstract Machine Notation, Petri Nets, Computation Tree Logic Kripke semantics, or Alexandrov topologies.\n5. The method of claim 1, wherein the first model is a generator and the second model is a discriminator, wherein one of the generator and discriminator comprises an explainable architecture, and wherein the method further comprises training an occupancy measure of a policy by using explanations from the discriminator to train the first model.\n6. The method of claim 1, further comprising forming an audit log comprising a plurality of decisions and path traces associated with a behavior of the explainable at least one of the explainable autoencoder, first model, and second model, wherein the audit log is stored in a system of record, a distributed ledger, a non-fungible token, a blockchain, or a database.\n7. The method of claim 1, further comprising applying a combination of a quantization, pruning, and instability reduction methods to at least one of the model, first model, and second model.\n8. The method of claim 1, wherein the explanation is based on one or more scenarios selected from a set of: what-if, what-if-not, counterfactual, but-for, and conditional scenarios, wherein each explanation identifies one or more cost metrics associated with a plurality of variables associated with the input.\n9. The method of claim 1, further comprising identifying a symbolic model comprising a set of rules, a formal language, a neuro-symbolic model, or a logical model to generate synthetic data, and identifying a constraint model comprising one or more of a statistical dataset, causal dataset, and symbolic dataset, wherein the symbolic model comprises one or more actions, metrics, conditions, constraints, actions, triggers, and events implemented as one or more of abductive logic, inductive logic, deductive logic, and causal logic.\n10. The method of claim 1, wherein the explainable model, first model, and second model are implemented on at least one of a digital electronic circuit, analog circuit, a digital-analog hybrid, integrated circuit, application specific integrated circuit (ASIC), field programmable gate array (FPGA), neuromorphic circuit, optical circuit, optical-electronic hybrid, spiking neurons, spintronics, memristors, a distributed architecture, and quantum computing hardware;\nwherein the one or more explainable models are implemented as one or more distributed explainable models arranged to be processed in parallel; and wherein each distributed explainable model is configured to split a dataset into multiple subsets of data for training the multiple explainable models,\nwherein the one or more explainable models comprise hybrid models comprising one or more of: an explainable artificial intelligence model, an explainable neural network, an explainable transducer transformer, an explainable reinforcement learning model, an explainable spiking neural network, explainable memory network, and/or an interpretable neural network, wherein one data part is configured to implement one model, while another data part is configured to implement another model; and wherein the one model and another model are combined to form an aggregate model.\n11. The method of claim 1, wherein the explainable autoencoder, first model, and second model are implemented as a quantum processing system configured to model quantum decoherence effects and one or more of a qubit state, qubit basis state, mixed states, Ancilla bit, and one or more quantum logic operators, wherein the quantum logic operators comprise one or more of a controlled-NOT, a controlled-swap, an Isling coupling gate, a Pauli gate, a Hadamard gate, or a Toffoli gate, and further comprising identifying one or more quantum annealing effects, wherein one or more of a Bernstein-Vazirani, Simon's algorithm, Deutsch-Jozsa algorithm, Shor's algorithm, Quantum Phase estimation algorithm, Grover's algorithm, Quantum Counting, Quantum Hamiltonian NAND trees, HHL algorithm, QAOA algorithm, VQE eingensolver, CQE eingensolver, and/or quantum matrix inversion algorithm is applied to one or more conditions, events, and/or triggers of the quantum behavioral model and one or more quantum transforms or quantum algorithm configured to predict and refine one or more boundary conditions of the behavioral model.\n12. The method of claim 1, further comprising one or more neuro-symbolic conditional constraints within the white-box explainable model, wherein the neuro-symbolic constraints are linked with a previous historic rate of activation and are implemented as one or more symbolic rules or system of symbolic expressions, polynomial expressions, Boolean logic rules, first order logic, second order logic, propositional logic, predicate logic, modal logic, probabilistic logic, many-valued logic, fuzzy logic, intuitionistic logic, non-monotonic logic, non-reflexive logic, quantum logic, paraconsistent logic, conditional and non-conditional probability distributions, joint probability distributions, state-space and phase-space transforms, integer/real/complex/quaternion/octonion transforms, Fourier transforms, Walsh functions, Haar and non-Haar wavelets, generalized L2 functions, fractal-based transforms, Hadamard transforms, Type 1 and Type 2 fuzzy logic, difference analysis, data structure that references features and variables accessible to the one or more white-box explainable models and one or more associated taxonomies, ontologies, and causal models, and/or knowledge graph networks.\n13. The method of claim 1, further comprising providing feedback from the explainable model to an explainable generative adversarial network, and inputting a simulator model and an agent environment to create an experience replay learning environment,\nwherein the first model is a generator and the second model is an explainable discriminator, wherein the explainable discriminator is configured to provide feedback to the generator,\nwherein the generator comprises an explainable simulator model, and a driver function configured to drive the explainable simulator model, wherein: the driver function is a differentiable function when the explainable simulator model is a neural network or gradient-descent based model, or the driver function is differentiable or non-differentiable when the explainable simulator model is a symbolic model, or\nwherein the generator comprises a non-explainable simulator model, and the driver function is configured to drive the non-explainable simulator model, wherein: the driver function is differentiable when the non-explainable simulator model is a neural network or gradient-descent based model; or the driver function is differentiable or non-differentiable when the non-explainable simulator model is a symbolic model.\n14. The method of claim 1, further comprising sending the explanation to a workflow system, and receiving feedback data from the workflow system and updating the explainable autoencoder model based on the feedback data, wherein the workflow is integrated with one or more of a robotic process automation system, decision support system, or a data lake system, wherein the explainable model is configured to be used as the basis or part of a practical data privacy preserving AI system implementation, wherein primary data privacy preserving solutions for AI is classified under four categories: differential privacy, secure multi-party computation, federated learning, and homomorphic encryption.\n15. The method of claim 1, wherein the white-box explainable model is an explainable structural causal model configured to detect bias between one or more causal variables, and further comprises an Explanation Structure Model (ESM) comprising: an explainable model, a Statistical Structural Model for modelling statistical relationships, a Causal Structural Model for modelling causal relationships, and a Symbolic Structural Model for modelling symbolic and logical relationships; or the neural network is configured to be used to input, output and process Explanation Structure Models together with an Explanation Output Template (EOT) to create Machine and Human Readable Explanations.\n16. The method of claim 1, wherein an AutoXAI system is configured to adapt the explainable model to one or more specific application domains or tasks, and\nwherein the AutoXAI system is adapted to provide a practical solution to the incorporation of meta-learning systems within the explainable model, and\nwherein the explainable model is further configured to be trained to learn one or more suggested actions for a given user with a specific context which lead to a change in outcome while minimizing a total cost of actions, wherein the suggested actions comprise a sequence that leads to a desired goal, and wherein the explainable model is configured to utilize a nearest-neighbor method for providing a plurality of actual examples of applications that have had the desired goal or outcome or a hypothetical average of the applications, wherein the nearest-neighbor methods utilize one or more partition hierarchy structures within the explainable model for generating examples from a same partition, or from a nearby partition or from a partition that is further away from a current partition, and wherein the explainable model is further configured to rank and prioritize possible changes in variables and present to a user or to an automated process in the form of an Identify-Assess-Recommend-Resolve framework, and wherein the explainable model is further configured to utilize one or more optimization methods for generating scenario-based explanations, wherein the optimization methods comprise Multiple Objective Optimization (MOO), Pareto Front Methods, Particle Swarm Optimization, Genetic Algorithms, Bayesian Optimization, Evolutionary Strategies, Gradient Descent techniques and/or Monte Carlo Simulation, and wherein encoding-decoding information for the explainable model are fully or partially linked to attribution values and data stored in an Explanation Structure Model.\n17. The method of claim 1, further comprising applying a linear or non-linear transformation from one or more of: a polynomial expansion, Fourier transform, continuous data bucketization, Walsh functions, state-space and phase-space transforms, Haar and non-Haar wavelets, generalized L2 functions, fractal-based transforms, Hadamard transforms, Type 1 and Type 2 fuzzy logic, knowledge graph networks, categorical encoding, topological transforms of Kolmogorov/Frechet/Hausdorff/Tychonoff spaces, difference analysis, normalization, and standardization.\n18. The method of claim 1, wherein each partition comprises a local model, and wherein one or more coefficients from the local model provide a local explanation and/or a global explanation, wherein each partition represents a concept or a single category of data, and wherein each partition comprises at least one of boundary information, observed and expected data distributions; meta-data, and constraints, and wherein each partition comprises a named reference label,\nwherein each named reference label comprises descriptive information and/or additional meta-data and links to external taxonomies, ontologies, and models,\nwherein the named reference label further comprises symbolic expressions and/or formulas of the form of: Conjunctive Normal Form (CNF) rules Boolean logic, first order logic, second order logic, propositional logic, predicate logic, modal logic, probabilistic logic, many-valued logic, fuzzy logic, intuitionistic logic, non-monotonic logic, non-reflexive logic, quantum logic, or paraconsistent logic, for providing an explanation of a set or sequence of decisions that resulted in an execution of a current component in the explainable model; and\nwherein the explainable model is configured to utilize an invariance of the named reference labels under dynamical processes to generate stable, long-term explanations of one or more dynamics occurring within the explainable model without recoding knowledge or retraining explanatory methods.\n19. The method of claim 1, further comprising generating a set of synthetic training data from the input dataset, and inputting the synthetic training data to a black-box model and partitioning an output of the black-box model to form an explainable generator or explainable discriminator model and wherein the explainable generator uses a noise variable to sample from a desired distribution and further comprising one or more of:\nusing causal generative adversarial network based generation,\nproviding a simulation using the explainable architecture and a hierarchical partition based model within the explainable architecture, or\nproviding a simulation using explainable generative adversarial imitation learning.\n20. The method of claim 1, further comprising encoding, decoding, modeling, reproducing, and/or generating at least one data distribution, the at least one data distribution comprising one or more of Normal, Binomial, Bernoulli, Hypergeometric, Beta-Binomial, Discrete Uniform, Poisson, Negative Binomial, Geometric, Lognormal, Beta, Gamma, Uniform, Exponential, Weibull, Double Exponential, Chi-Squared, Cauchy, Fisher-Snedecor, and Student T distributions.\n21. The method of claim 1, further comprising splitting and/or merging a plurality of overlapping and/or non-overlapping partitions from the hierarchical partition structure, wherein the partitions are split and/or merged based on a level of detail or a scale factor, and wherein the hierarchical partition structure comprises a graph-based method comprising one or more of: trees, graphs, Resource Description Framework (RDF) trees, RDF graphs, hypergraphs, and simplicial complexes;\nwherein the graph-based methods comprise: a graph-regularized auto-encoder; an iterative generative graph modeling method; a Deep Neural Network for Graph Representation; a Structural Deep Network Embedding, and/or a spectral autoencoder method, or wherein the partitions are computed dynamically within the one or more explainable autoencoders.\n22. The method of claim 1, further comprising providing the explanations to one or more sub-components of the first model or the second model to provide feedback or to update at least one internal state, and wherein the first model or second model further comprise a model interpretation comprising at least one of a basic interpretation, an explanatory interpretation, or a meta-explanatory interpretation.\n23. The method of claim 1, wherein forming the white-box autoencoder further comprises an external partitioning method or querying a black-box autoencoder with the training dataset and partitioning the output of the queried black-box autoencoder to identify a plurality of rules, coefficients, and/or constraints of the white-box autoencoder, wherein the black-box autoencoder comprises at least one of a black-box encoder and a black-box decoder; and further comprising measuring a reconstruction loss between the white-box autoencoder and black-box autoencoder, and transforming one or more features identified from the training dataset using at least one of polynomial expansions, rotations, dimensional and dimensionless scaling, Fourier transforms, Walsh functions, state-space and phase-space transforms, Haar and non-Haar wavelets, generalized L2 functions, fractal-based transforms, Hadamard transforms, Type 1 and Type 2 fuzzy logic, knowledge graph networks, categorical encoding, topological transforms of Kolmogorov/Frechet/Hausdorff/Tychonoff spaces, difference analysis, and normalization/standardization/binning/bucketization.\n24. The method of claim 1, further comprising forming an explainable first model using a black-box first model; forming a discriminator using the second model, and measuring a loss function between the second model and the explainable model, and wherein the explanation comprises an identification of a potential bias, performance effects, causal effects and risk effects or anomaly, wherein the second model is configured to combine the training dataset with a plurality of samples generated from the first model and generate a probability to indicate if the samples were retrieved from the training dataset, and\nconstantly monitoring one or more of the explainable model, first model, or second model to detect one or more of: anomalous behavior, instances of data drift and OOD instances, abnormal deviations from nominal operational cycles, or to analyze and assess behavior under OOD and anomalous instances, variation, deviation, performance and resource usage monitoring, or a phase-space.\n25. The method of claim 1, further comprising generating a probability associated with a plurality of input samples using a discriminator, wherein the input samples correspond to one or more tasks that are associated with one or more input features and comprise a plurality of outputs corresponding to outputs for the corresponding tasks, and\nfurther comprising sequence data comprising a plurality of data points containing feature data in one or more sequential formats comprising one or more of: 2D data, 3D data, multi-dimensional data arrays, transactional data, time series, digitized samples, sensor data, image data, hyper-spectral data, natural language text, video data, audio data, haptic data, LIDAR data, RADAR data, and SONAR data, wherein the data points comprise one or more associated labels indicating an output value or classification for a specific data point or a continuous or non-continuous interval of data points, and wherein a plurality of data point sequences are identified from an internal and/or external process configured to output one or more of synthetic data points, perturbed data, sampled data, and transformed data.\n26. The method of claim 1, further comprising injecting human knowledge into the explainable model, wherein the human knowledge creates or alters one or more rules or partitions, and wherein the created or altered rule or partition is static and unchangeable by the explainable model, and wherein the human knowledge is expressed as a knowledge graph comprising nodes that describe human defined entities, and edges that describe a relationship between nodes; and wherein each node in the knowledge graph comprises one or more taxonomy identifiers that contribute to a classification of knowledge in the knowledge graph, wherein the knowledge graph comprises multiple taxonomies; and wherein one or more taxonomy identifiers are configured to fuse a plurality of knowledge graphs together.\n27. The method of claim 1, wherein the first model and second model are black-box models, and further comprising forming an explainable second model using the black-box second model, forming an explainable first model using the black-box first model, and measuring a loss between the black-box second model and the explainable second model.\n28. The method of claim 1, further comprising forming a backmap by tracing the feature attributions back to the input dataset or synthetic data to identify a relationship between the feature attributions and an output.\n29. The method of claim 1, further comprising forming an explanation output template (EOT) or an explanation structure model (ESM) within an explanation and interpretation generation system (EIGS) and outputting the explanation based on the partitioning information or one or more internal coefficients of the explainable model, and\nfurther comprising forming an explanation scaffolding from an output produced by the first model and/or second model; and\ntransmitting the explanation scaffolding to an interpreter, wherein the interpreter is an end-user or a component within one or more explainable models.\n30. The method of claim 1, further comprising implementing a graph-based method comprising at least one of: a graph-regularized auto-encoder (GAE), iterative generative graph modeling method, the Graphite method, Deep Neural Networks for Graph Representations (DNGR), Structural Deep Network Embedding (SDNE), and spectral autoencoder methods configured to integrate and interface the explainable auto-encoder with one or more Explainable Machine Learning systems that utilize graphs, Resource Description Framework (RDF) tree, RDF graph, hypergraphs or simplicial complexes, wherein the training dataset is formed using at least one of:\na causal generative adversarial network;\na hierarchical partition-based model; and\nan explainable generative Adversarial Imitation Learning model."
}