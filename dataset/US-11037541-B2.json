{
    "patent_id": "US-11037541-B2",
    "title": "Method of composing a piece of digital music using musical experience descriptors to indicate what, when and how musical events should appear in the piece of digital music automatically composed and generated by an automated music composition and generation system ",
    "assignee": "Shutterstock, Inc.",
    "publication_date": "2021-06-15",
    "patent_link": "https://patents.google.com/patent/US11037541B2/en",
    "inventors": [
        "Andrew H. Silverstein"
    ],
    "classifications": [
        "G10H1/0025",
        "G06N20/00",
        "G06N7/005",
        "G06N7/01",
        "G10H1/00",
        "G10H1/368",
        "G10L25/15",
        "G10H2210/021",
        "G10H2210/066",
        "G10H2210/105",
        "G10H2210/111",
        "G10H2210/115",
        "G10H2210/341",
        "G10H2220/101",
        "G10H2240/081",
        "G10H2240/085",
        "G10H2240/131",
        "G10H2240/305",
        "G10H2250/311"
    ],
    "abstract": "An automated music composition and generation system having a system user interface operably connected to an automated music composition and generation engine, and supporting a method of composing a piece of digital music using musical experience descriptors as to indicate what, when and how particular musical events should occur in the piece of digital music to be automatically composed and generated. The method uses the system user interface to select one or more musical experience descriptors and applying the musical experience descriptors along a timeline representation of a piece of digital music to be automatically composed and generated by the automated music composition and generation engine.",
    "claims": "\n1. A method of composing a piece of digital music using an automated music composition and generation system being supplied with musical experience descriptors to characterize the piece of digital music to be automatically composed and generated by said automated music composition and generation system, said method comprising the steps of:\n(a) creating a project to automatically compose and generate the piece of digital music using the automated music composition and generation system having a system user interface operably connected to an automated music composition and generation engine;\n(b) using said system user interface to select one or more musical experience descriptors and to apply said musical experience descriptors along a timeline representation of the piece of digital music to be automatically composed and generated, so as to indicate what, when and how particular musical events should occur in the piece of digital music to be automatically composed and generated by said automated music composition and generation system,\nwherein said particular musical events are selected from a group consisting of a music start, music stop, descriptor change, style change, volume change, structural change, and instrumentation change;\n(c) providing said musical experience descriptors to said automated music composition and generation engine;\n(d) initiating said automated music composition and generation engine so as to automatically compose and generate said piece of digital music;\n(e) formatting said piece of digital music and creating a digital file representing said piece of digital music for display and review by a system user using said system user interface;\n(f) reviewing and assessing said digital file and making modifications to one or more selected musical experience descriptors;\n(g) providing the musical experience descriptors to said automated music composition and generation engine; and\n(h) initiating said automated music composition and generation engine to compose and generate a new digital file for display and review;\nwherein each composed and generated piece of digital music contains a set of musical notes arranged and performed in said piece of digital music, and having characteristics expressed throughout said piece of digital music and represented by said musical experience descriptors.\n2. The method of claim 1, wherein step (c) comprises selecting said musical experience descriptors from a group consisting of emotion-type musical experience descriptors, style-type musical experience descriptors, timing-type musical experience descriptors, and accent-type musical experience descriptors.\n3. The method of claim 2, wherein said timing-type musical experience descriptors and said accent-type musical experience descriptors indicate when musical events occur in said piece of digital music, and include one or more parameters, commands and/or markers selected from a group consisting of: (i) a parameter indicating a length of the piece of digital music, (ii) a marker indicating a timing location of a start in the piece of digital music, (iii) a marker indicating the timing location of a stop in the piece of digital music, (iv) a marker indicating the timing location of an instrument hit in the piece of digital music, (v) a marker indicating the timing location of a fade-in in the piece of digital music, (vi) a marker indicating the timing location of a fade-out in the piece of digital music, (vii) a marker indicating the timing location of a modulation in the piece of digital music, (viii) a marker indicating the timing location of an increase in volume in the piece of digital music, (ix) a marker indicating the timing location of a particular accent in the piece of digital music, (x) a marker indicating the timing location of a new emotion or mood to be conveyed by the piece of digital music, (xi) a marker indicating the timing location of a change in style in the piece of digital music, (xii) a marker indicating the timing location of a change in instrumentation in the piece of digital music, and (xiii) a marker indicating the timing location of the structural change of the piece of digital music.\n4. The method of claim 1, wherein said musical experience descriptors have a graphical-icon and/or linguistic format.\n5. The method of claim 1, wherein said system user interface is an interface selected from a group consisting of a text keyboard, a manual data entry device, a speech recognition interface, a graphical user interface (GUI), and a touch-screen graphical user interface (GUI).\n6. The method of claim 1, wherein the set of musical notes contained in each composed and generated piece of digital music are produced using sound/note sampling techniques and/or sound/note synthesis techniques.\n7. A method of composing a piece of digital music using an automated music composition and generation system being supplied with musical experience descriptors to characterize the piece of digital music to be automatically composed and generated by said automated music composition and generation system, said method comprising the steps of;\n(a) creating a project to automatically compose and generate the piece of digital music using the automated music composition and generation system having a system user interface operably connected to an automated music composition and generation engine;\n(b) using said system user interface to select one or more musical experience descriptors and apply said music experience descriptors along a timeline representation of the piece of digital music to be automatically composed and generated, so as to specify which musical events should occur in the piece of digital music to be automatically composed and generated by said automated music composition and generation system,\nwherein said musical events are selected from a group consisting of music start, music stop, descriptor change, style change, volume change, structural change, and instrumentation change;\n(c) providing said musical experience descriptors to said automated music composition and generation engine;\n(d) initiating said automated music composition and generation engine so as to automatically compose and generate said piece of digital music;\n(e) formatting said piece of digital music and creating a digital file representing said piece of digital music for display and review by a system user using said system user interface;\n(f) reviewing and assessing said digital file and making modifications to one or more selected musical experience descriptors;\n(g) providing the musical experience descriptors to said automated music composition and generation engine; and\n(h) initiating said automated music composition and generation engine to compose and generate a new digital file for display and review;\nwherein each composed and generated piece of digital music contains a set of musical notes arranged and performed in said piece of digital music, and having characteristics expressed throughout said piece of digital music and represented by said musical experience descriptors.\n8. The method of claim 7, wherein step (b) comprises selecting said musical experience descriptors from a group consisting of emotion-type musical experience descriptors, style-type musical experience descriptors, timing-type musical experience descriptors, and accent-type musical experience descriptors.\n9. The method of claim 8, wherein said timing-type musical experience descriptors and said accent-type musical experience descriptors indicate when the musical events occur in said piece of digital music, and include one or more parameters, commands and/or markers selected from a group consisting of: (i) a parameter indicating a length of the piece of digital music, (ii) a marker indicating a timing location of a start in the piece of digital music, (iii) a marker indicating the timing location of a stop in the piece of digital music, (iv) a marker indicating the timing location of an instrument hit in the piece of digital music, (v) a marker indicating the timing location of a fade-in in the piece of digital music, (vi) a marker indicating the timing location of a fade-out in the piece of digital music, (vii) a marker indicating the timing location of a modulation in the piece of digital music, (viii) a marker indicating the timing location of an increase in volume in the piece of digital music, (ix) a marker indicating the timing location of a particular accent in the piece of digital music, (x) a marker indicating the timing location of a new emotion or mood to be conveyed by the piece of digital music, (xi) a marker indicating the timing location of a change in style in the piece of digital music, (xii) a marker indicating the timing location of a change in instrumentation in the piece of digital music, and (xiii) a marker indicating the timing location of the structural change of the piece of digital music.\n10. The method of claim 7, wherein said musical experience descriptors have a graphical-icon and/or linguistic format.\n11. The method of claim 7, wherein said system user interface is an interface selected from a group consisting of a text keyboard, a manual data entry device, a speech recognition interface, a graphical user interface (GUI), and a touch-screen graphical user interface (GUI).\n12. The method of claim 7, wherein the set of musical notes contained in each piece of digital music are produced using sound/note sampling techniques and/or sound/note synthesis techniques."
}